// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// ra25 runs a simple random-associator four-layer axon network
// that uses the standard supervised learning paradigm to learn
// mappings between 25 random input / output patterns
// defined over 5x5 input / output layers (i.e., 25 units)
package main

//go:generate core generate -add-types

import (
	"log"
	"os"

	"cogentcore.org/core/base/metadata"
	"cogentcore.org/core/base/mpi"
	"cogentcore.org/core/base/randx"
	"cogentcore.org/core/core"
	"cogentcore.org/core/enums"
	"cogentcore.org/core/icons"
	"cogentcore.org/core/math32"
	"cogentcore.org/core/math32/vecint"
	"cogentcore.org/core/plot"
	"cogentcore.org/core/tensor"
	"cogentcore.org/core/tensor/datafs"
	"cogentcore.org/core/tensor/stats/stats"
	"cogentcore.org/core/tensor/table"
	"cogentcore.org/core/tree"
	"github.com/emer/axon/v2/axon"
	"github.com/emer/emergent/v2/econfig"
	"github.com/emer/emergent/v2/egui"
	"github.com/emer/emergent/v2/emer"
	"github.com/emer/emergent/v2/env"
	"github.com/emer/emergent/v2/looper"
	"github.com/emer/emergent/v2/paths"
)

func main() {
	sim := &Sim{}
	sim.New()
	sim.ConfigAll()
	if sim.Config.GUI {
		sim.RunGUI()
	} else {
		sim.RunNoGUI()
	}
}

// Modes are the looping modes (Stacks) for running and statistics.
type Modes int32 //enums:enum
const (
	Train Modes = iota
	Test
)

// Levels are the looping levels for running and statistics.
type Levels int32 //enums:enum
const (
	Cycle Levels = iota
	Trial
	Epoch
	Run
)

// StatsPhase is the phase of stats processing for given mode, level.
// Accumulated values are reset at Start, added each Step.
type StatsPhase int32 //enums:enum
const (
	Start StatsPhase = iota
	Step
)

// see params.go for params

// ParamConfig has config parameters related to sim params
type ParamConfig struct {

	// network parameters
	Network map[string]any

	// size of hidden layer -- can use emer.LaySize for 4D layers
	Hidden1Size vecint.Vector2i `default:"{'X':10,'Y':10}" nest:"+"`

	// size of hidden layer -- can use emer.LaySize for 4D layers
	Hidden2Size vecint.Vector2i `default:"{'X':10,'Y':10}" nest:"+"`

	// Extra Param Sheet name(s) to use (space separated if multiple) -- must be valid name as listed in compiled-in params or loaded params
	Sheet string

	// extra tag to add to file names and logs saved from this run
	Tag string

	// user note -- describe the run params etc -- like a git commit message for the run
	Note string

	// Name of the JSON file to input saved parameters from.
	File string `nest:"+"`

	// Save a snapshot of all current param and config settings in a directory named params_<datestamp> (or _good if Good is true), then quit -- useful for comparing to later changes and seeing multiple views of current params
	SaveAll bool `nest:"+"`

	// for SaveAll, save to params_good for a known good params state.  This can be done prior to making a new release after all tests are passing -- add results to git to provide a full diff record of all params over level.
	Good bool `nest:"+"`
}

// RunConfig has config parameters related to running the sim
type RunConfig struct {

	// use the GPU for computation -- generally faster even for small models if NData ~16
	GPU bool `default:"false"`

	// number of data-parallel items to process in parallel per trial -- works (and is significantly faster) for both CPU and GPU.  Results in an effective mini-batch of learning.
	NData int `default:"2" min:"1"`

	// number of parallel threads for CPU computation -- 0 = use default
	NThreads int `default:"0"`

	// starting run number -- determines the random seed -- runs counts from there -- can do all runs in parallel by launching separate jobs with each run, runs = 1
	Run int `default:"0"`

	// total number of runs to do when running Train
	NRuns int `default:"5" min:"1"`

	// total number of epochs per run
	NEpochs int `default:"100"`

	// stop run after this number of perfect, zero-error epochs
	NZero int `default:"2"`

	// total number of trials per epoch.  Should be an even multiple of NData.
	NTrials int `default:"32"`

	// how often to run through all the test patterns, in terms of training epochs -- can use 0 or -1 for no testing
	TestInterval int `default:"5"`

	// how frequently (in epochs) to compute PCA on hidden representations to measure variance?
	PCAInterval int `default:"5"`

	// if non-empty, is the name of weights file to load at start of first run -- for testing
	StartWts string
}

// LogConfig has config parameters related to logging data
type LogConfig struct {

	// if true, save final weights after each run
	SaveWeights bool

	// if true, save train epoch log to file, as .epc.tsv typically
	Epoch bool `default:"true" nest:"+"`

	// if true, save run log to file, as .run.tsv typically
	Run bool `default:"true" nest:"+"`

	// if true, save train trial log to file, as .trl.tsv typically. May be large.
	Trial bool `default:"false" nest:"+"`

	// if true, save testing epoch log to file, as .tst_epc.tsv typically.  In general it is better to copy testing items over to the training epoch log and record there.
	TestEpoch bool `default:"false" nest:"+"`

	// if true, save testing trial log to file, as .tst_trl.tsv typically. May be large.
	TestTrial bool `default:"false" nest:"+"`

	// if true, save network activation etc data from testing trials, for later viewing in netview
	NetData bool
}

// Config is a standard Sim config -- use as a starting point.
type Config struct {

	// specify include files here, and after configuration, it contains list of include files added
	Includes []string

	// open the GUI -- does not automatically run -- if false, then runs automatically and quits
	GUI bool `default:"true"`

	// log debugging information
	Debug bool

	// parameter related configuration options
	Params ParamConfig `display:"add-fields"`

	// sim running related configuration options
	Run RunConfig `display:"add-fields"`

	// data logging related configuration options
	Log LogConfig `display:"add-fields"`
}

func (cfg *Config) IncludesPtr() *[]string { return &cfg.Includes }

// Sim encapsulates the entire simulation model, and we define all the
// functionality as methods on this struct.  This structure keeps all relevant
// state information organized and available without having to pass everything around
// as arguments to methods, and provides the core GUI interface (note the view tags
// for the fields which provide hints to how things should be displayed).
type Sim struct {

	// simulation configuration parameters -- set by .toml config file and / or args
	Config Config `new-window:"+"`

	// the network -- click to view / edit parameters for layers, paths, etc
	Net *axon.Network `new-window:"+" display:"no-inline"`

	// network parameter management
	Params emer.NetParams `display:"add-fields"`

	// contains looper control loops for running sim
	Loops *looper.Stacks `new-window:"+" display:"no-inline"`

	// the training patterns to use
	Pats *table.Table `display:"no-inline"`

	// Environments
	Envs env.Envs `new-window:"+" display:"no-inline"`

	// train mode netview update parameters
	TrainUpdate axon.NetViewUpdate `display:"inline"`

	// test mode netview update parameters
	TestUpdate axon.NetViewUpdate `display:"inline"`

	// Root is the root data dir.
	Root *datafs.Data `display:"-"`

	// StatFuncs are statistics functions, per stat, handles everything.
	StatFuncs []func(mode Modes, level Levels, phase StatsPhase) `display:"-"`

	// Stats has the stats dir.
	Stats *datafs.Data `display:"-"`

	// Current has the current stats values.
	Current *datafs.Data `display:"-"`

	// manages all the gui elements
	GUI egui.GUI `display:"-"`

	// a list of random seeds to use for each run
	RandSeeds randx.Seeds `display:"-"`
}

// New creates new blank elements and initializes defaults
func (ss *Sim) New() {
	econfig.Config(&ss.Config, "config.toml")
	ss.Root, _ = datafs.NewDir("Root")
	ss.Net = axon.NewNetwork("RA25")
	ss.Params.Config(ParamSets, ss.Config.Params.Sheet, ss.Config.Params.Tag, ss.Net)
	ss.Pats = table.New()
	ss.RandSeeds.Init(100) // max 100 runs
	ss.InitRandSeed(0)
}

////////////////////////////////////////////////////////////////////////////////////////////
// 		Configs

// ConfigAll configures all the elements using the standard functions
func (ss *Sim) ConfigAll() {
	// ss.ConfigPats()
	ss.OpenPats()
	ss.ConfigEnv()
	ss.ConfigNet(ss.Net)
	ss.ConfigLoops()
	ss.ConfigStats()
	if ss.Config.Params.SaveAll {
		ss.Config.Params.SaveAll = false
		ss.Net.SaveParamsSnapshot(&ss.Params.Params, &ss.Config, ss.Config.Params.Good)
		os.Exit(0)
	}
}

func (ss *Sim) ConfigEnv() {
	// Can be called multiple times -- don't re-create
	var trn, tst *env.FixedTable
	if len(ss.Envs) == 0 {
		trn = &env.FixedTable{}
		tst = &env.FixedTable{}
	} else {
		trn = ss.Envs.ByMode(Train).(*env.FixedTable)
		tst = ss.Envs.ByMode(Test).(*env.FixedTable)
	}

	// note: names must be standard here!
	trn.Name = Train.String()
	trn.Config(table.NewView(ss.Pats))
	trn.Validate()

	tst.Name = Test.String()
	tst.Config(table.NewView(ss.Pats))
	tst.Sequential = true
	tst.Validate()

	// note: to create a train / test split of pats, do this:
	// all := table.NewIndexView(ss.Pats)
	// splits, _ := split.Permuted(all, []float64{.8, .2}, []string{"Train", "Test"})
	// trn.Table = splits.Splits[0]
	// tst.Table = splits.Splits[1]

	trn.Init(0)
	tst.Init(0)

	// note: names must be in place when adding
	ss.Envs.Add(trn, tst)
}

func (ss *Sim) ConfigNet(net *axon.Network) {
	net.SetMaxData(ss.Config.Run.NData)
	net.SetRandSeed(ss.RandSeeds[0]) // init new separate random seed, using run = 0

	inp := net.AddLayer2D("Input", axon.InputLayer, 5, 5)
	hid1 := net.AddLayer2D("Hidden1", axon.SuperLayer, ss.Config.Params.Hidden1Size.Y, ss.Config.Params.Hidden1Size.X)
	hid2 := net.AddLayer2D("Hidden2", axon.SuperLayer, ss.Config.Params.Hidden2Size.Y, ss.Config.Params.Hidden2Size.X)
	out := net.AddLayer2D("Output", axon.TargetLayer, 5, 5)

	// use this to position layers relative to each other
	// hid2.PlaceRightOf(hid1, 2)

	// note: see emergent/path module for all the options on how to connect
	// NewFull returns a new paths.Full connectivity pattern
	full := paths.NewFull()

	net.ConnectLayers(inp, hid1, full, axon.ForwardPath)
	net.BidirConnectLayers(hid1, hid2, full)
	net.BidirConnectLayers(hid2, out, full)

	// net.LateralConnectLayerPath(hid1, full, &axon.HebbPath{}).SetType(InhibPath)

	// note: if you wanted to change a layer type from e.g., Target to Compare, do this:
	// out.SetType(emer.Compare)
	// that would mean that the output layer doesn't reflect target values in plus phase
	// and thus removes error-driven learning -- but stats are still computed.

	net.Build()
	net.Defaults()
	net.SetNThreads(ss.Config.Run.NThreads)
	ss.ApplyParams()
	net.InitWeights()
}

func (ss *Sim) ApplyParams() {
	ss.Params.SetAll()
	if ss.Config.Params.Network != nil {
		ss.Params.SetNetworkMap(ss.Net, ss.Config.Params.Network)
	}
}

////////////////////////////////////////////////////////////////////////////////
// 	    Init, utils

// Init restarts the run, and initializes everything, including network weights
// and resets the epoch log table
func (ss *Sim) Init() {
	// if ss.Config.GUI {
	// 	ss.Stats.SetString("RunName", ss.Params.RunName(0)) // in case user interactively changes tag
	// }
	ss.Loops.ResetCounters()
	ss.InitRandSeed(0)
	// ss.ConfigEnv() // re-config env just in case a different set of patterns was
	// selected or patterns have been modified etc
	ss.GUI.StopNow = false
	ss.ApplyParams()
	// ss.Net.GPU.SyncParamsToGPU()
	ss.InitStats()
	ss.NewRun()
	ss.TrainUpdate.RecordSyns()
	ss.TrainUpdate.Update(ss.StatCounters(Train, Trial))
}

// InitRandSeed initializes the random seed based on current training run number
func (ss *Sim) InitRandSeed(run int) {
	ss.RandSeeds.Set(run)
	ss.RandSeeds.Set(run, &ss.Net.Rand)
}

// CurrentMode returns the current Train / Test mode from Context.
func (ss *Sim) CurrentMode() Modes {
	ctx := ss.Net.Context()
	var md Modes
	md.SetInt64(int64(ctx.Mode))
	return md
}

// ConfigLoops configures the control loops: Training, Testing
func (ss *Sim) ConfigLoops() {
	ls := looper.NewStacks()

	trls := int(math32.IntMultipleGE(float32(ss.Config.Run.NTrials), float32(ss.Config.Run.NData)))

	ls.AddStack(Train, Trial).
		AddLevel(Run, ss.Config.Run.NRuns).
		AddLevel(Epoch, ss.Config.Run.NEpochs).
		AddLevelIncr(Trial, trls, ss.Config.Run.NData).
		AddLevel(Cycle, 200)

	ls.AddStack(Test, Trial).
		AddLevel(Epoch, 1).
		AddLevelIncr(Trial, trls, ss.Config.Run.NData).
		AddLevel(Cycle, 200)

	axon.LooperStandard(ls, ss.Net, ss.GUI.NetView, 150, 199, Cycle, Trial, Train)

	ls.Stacks[Train].OnInit.Add("Init", func() { ss.Init() })

	ls.AddOnStartToLoop(Trial, "ApplyInputs", func(mode enums.Enum) {
		ss.ApplyInputs(mode.(Modes))
	})

	ls.Loop(Train, Run).OnStart.Add("NewRun", ss.NewRun)

	ls.Loop(Train, Epoch).IsDone.AddBool("NZeroStop", func() bool {
		stopNz := ss.Config.Run.NZero
		if stopNz <= 0 {
			return false
		}
		curModeDir := ss.Current.RecycleDir(Train.String())
		curNZero := int(curModeDir.Value("NZero").Float1D(-1))
		stop := curNZero >= stopNz
		return stop
		return false
	})

	// Add Testing
	trainEpoch := ls.Loop(Train, Epoch)
	trainEpoch.OnStart.Add("TestAtInterval", func() {
		if (ss.Config.Run.TestInterval > 0) && ((trainEpoch.Counter.Cur+1)%ss.Config.Run.TestInterval == 0) {
			// Note the +1 so that it doesn't occur at the 0th timestep.
			ss.TestAll()
		}
	})

	//////// Logging

	ls.AddOnStartToAll("StatsStart", ss.StatsStart)
	ls.AddOnEndToAll("StatsStep", ss.StatsStep)

	// ls.Loop(Test, Epoch).OnEnd.Add("LogTestErrors", func() {
	// 	axon.LogTestErrors(&ss.Logs)
	// })
	// ls.Loop(Train, Epoch).OnEnd.Add("PCAStats", func() {
	// 	trnEpc := ls.Stacks[Train].Loops[Epoch].Counter.Cur
	// 	if ss.Config.Run.PCAInterval > 0 && trnEpc%ss.Config.Run.PCAInterval == 0 {
	// 		axon.PCAStats(ss.Net, &ss.Logs, &ss.Stats)
	// 		ss.Logs.ResetLog(Analyze, Trial)
	// 	}
	// })
	//

	// ls.Loop(Train, Trial).OnEnd.Add("LogAnalyze", func() {
	// 	trnEpc := ls.Stacks[Train].Loops[Epoch].Counter.Cur
	// 	if (ss.Config.Run.PCAInterval > 0) && (trnEpc%ss.Config.Run.PCAInterval == 0) {
	// 		ss.Log(Analyze, Trial)
	// 	}
	// })
	//
	// ls.Loop(Train, Run).OnEnd.Add("RunStats", func() {
	// 	ss.Logs.RunStats("PctCor", "FirstZero", "LastZero")
	// })

	// Save weights to file, to look at later
	// ls.Loop(Train, Run).OnEnd.Add("SaveWeights", func() {
	// 	ctrString := ss.Stats.PrintValues([]string{"Run", "Epoch"}, []string{"%03d", "%05d"}, "_")
	// 	axon.SaveWeightsIfConfigSet(ss.Net, ss.Config.Log.SaveWeights, ctrString, ss.Stats.String("RunName"))
	// })

	//////// GUI

	if ss.Config.GUI {
		axon.LooperUpdateNetView(ls, Train, Cycle, Trial, &ss.TrainUpdate, ss.StatCounters)
		axon.LooperUpdateNetView(ls, Test, Cycle, Trial, &ss.TestUpdate, ss.StatCounters)

		ls.Stacks[Train].OnInit.Add("GUI-Init", func() { ss.GUI.UpdateWindow() })
		ls.Stacks[Test].OnInit.Add("GUI-Init", func() { ss.GUI.UpdateWindow() })
	}

	if ss.Config.Debug {
		mpi.Println(ls.DocString())
	}
	ss.Loops = ls
}

// ApplyInputs applies input patterns from given environment.
// It is good practice to have this be a separate method with appropriate
// args so that it can be used for various different contexts
// (training, testing, etc).
func (ss *Sim) ApplyInputs(mode Modes) {
	net := ss.Net
	ctx := net.Context()
	ndata := int(ctx.NData)
	curModeDir := ss.Current.RecycleDir(mode.String())
	ev := ss.Envs.ByMode(mode).(*env.FixedTable)
	lays := net.LayersByType(axon.InputLayer, axon.TargetLayer)
	net.InitExt()
	for di := range ndata {
		ev.Step()
		datafs.Value[string](curModeDir, "TrialName", ndata).SetString1D(ev.TrialName.Cur, di)
		for _, lnm := range lays {
			ly := ss.Net.LayerByName(lnm)
			pats := ev.State(ly.Name)
			if pats != nil {
				ly.ApplyExt(uint32(di), pats)
			}
		}
	}
	net.ApplyExts() // now required for GPU mode
}

// NewRun intializes a new run of the model, using the TrainEnv.Run counter
// for the new run value
func (ss *Sim) NewRun() {
	ctx := ss.Net.Context()
	ss.InitRandSeed(ss.Loops.Loop(Train, Run).Counter.Cur)
	ss.Envs.ByMode(Train).Init(0)
	ss.Envs.ByMode(Test).Init(0)
	ctx.Reset()
	ctx.Mode = int32(Train)
	ss.Net.InitWeights()
}

// TestAll runs through the full set of testing items
func (ss *Sim) TestAll() {
	ss.Envs.ByMode(Test).Init(0)
	ss.Loops.ResetAndRun(Test)
	ss.Loops.Mode = Train // Important to reset Mode back to Train because this is called from within the Train Run.
}

////////   Pats

func (ss *Sim) ConfigPats() {
	// dt := ss.Pats
	// dt.SetMetaData("name", "TrainPats")
	// dt.SetMetaData("desc", "Training patterns")
	// dt.AddStringColumn("Name")
	// dt.AddFloat32TensorColumn("Input", []int{5, 5}, "Y", "X")
	// dt.AddFloat32TensorColumn("Output", []int{5, 5}, "Y", "X")
	// dt.SetNumRows(25)
	//
	// patgen.PermutedBinaryMinDiff(dt.Columns[1].(*tensor.Float32), 6, 1, 0, 3)
	// patgen.PermutedBinaryMinDiff(dt.Columns[2].(*tensor.Float32), 6, 1, 0, 3)
	// dt.SaveCSV("random_5x5_25_gen.tsv", table.Tab, table.Headers)
}

func (ss *Sim) OpenPats() {
	dt := ss.Pats
	metadata.SetName(dt, "TrainPats")
	metadata.SetDoc(dt, "Training patterns")
	err := dt.OpenCSV("random_5x5_25.tsv", tensor.Tab)
	if err != nil {
		log.Println(err)
	}
}

//////// Stats

func (ss *Sim) AddStat(f func(mode Modes, level Levels, phase StatsPhase)) {
	ss.StatFuncs = append(ss.StatFuncs, f)
}

// StatsStart is called by Looper at the start of given level, for each iteration.
// It needs to call RunStats Start at the next level down.
// e.g., each Epoch is the start of the full set of Trial Steps.
func (ss *Sim) StatsStart(lmd, ltm enums.Enum) {
	mode := lmd.(Modes)
	level := ltm.(Levels)
	if level <= Trial {
		return
	}
	ss.RunStats(mode, level-1, Start)
}

// StatsStep is called by Looper at each step of iteration,
// where it accumulates the stat results.
func (ss *Sim) StatsStep(lmd, ltm enums.Enum) {
	mode := lmd.(Modes)
	level := ltm.(Levels)
	if level == Cycle {
		return
	}
	ss.RunStats(mode, level, Step)
	ss.StatsData(mode, level).GetDirTable(nil).WriteToLog()
}

// RunStats runs the StatFuncs for given mode, level and phase.
func (ss *Sim) RunStats(mode Modes, level Levels, phase StatsPhase) {
	for _, sf := range ss.StatFuncs {
		sf(mode, level, phase)
	}
	if phase == Step && ss.GUI.Tabs != nil {
		nm := mode.String() + "/" + level.String() + " Plot"
		ss.GUI.Tabs.GoUpdatePlot(nm)
	}
}

func (ss *Sim) StatsData(mode Modes, level Levels) *datafs.Data {
	modeDir := ss.Stats.RecycleDir(mode.String())
	return modeDir.RecycleDir(level.String())
}

func (ss *Sim) InitStats() {
	for md, st := range ss.Loops.Stacks {
		mode := md.(Modes)
		for _, lev := range st.Order {
			level := lev.(Levels)
			if level == Cycle {
				continue
			}
			ss.RunStats(mode, level, Start)
		}
	}
	if ss.GUI.Tabs != nil {
		_, idx := ss.GUI.Tabs.CurrentTab()
		ss.GUI.Tabs.PlotDataFS(ss.StatsData(Train, Epoch))
		ss.GUI.Tabs.PlotDataFS(ss.StatsData(Train, Run))
		ss.GUI.Tabs.PlotDataFS(ss.StatsData(Test, Trial))
		ss.GUI.Tabs.SelectTabIndex(idx)
	}
}

// ConfigStats handles configures functions to do all stats computation
// in the datafs system.
func (ss *Sim) ConfigStats() {
	net := ss.Net
	ss.Stats, _ = ss.Root.Mkdir("Stats")
	ss.Current, _ = ss.Stats.Mkdir("Current")

	// last arg(s) are levels to exclude
	counterFunc := axon.StatLoopCounters(ss.Stats, ss.Current, ss.Loops, net, Trial, Cycle)
	ss.AddStat(func(mode Modes, level Levels, phase StatsPhase) {
		counterFunc(mode, level, phase == Start)
	})

	ss.AddStat(func(mode Modes, level Levels, phase StatsPhase) {
		if level != Trial {
			return
		}
		name := "TrialName"
		modeDir := ss.Stats.RecycleDir(mode.String())
		curModeDir := ss.Current.RecycleDir(mode.String())
		levelDir := modeDir.RecycleDir(level.String())
		tsr := datafs.Value[string](levelDir, name)
		ndata := int(ss.Net.Context().NData)
		if phase == Start {
			tsr.SetNumRows(0)
			if ps := plot.GetStylersFrom(tsr); ps == nil {
				ps.Add(func(s *plot.Style) {
					s.On = false
				})
				plot.SetStylersTo(tsr, ps)
			}
			return
		}
		for di := range ndata {
			// saved in apply inputs
			trlNm := datafs.Value[string](curModeDir, name, ndata).String1D(di)
			tsr.AppendRowString(trlNm)
		}
	})

	// up to a point, it is good to use loops over stats in one function,
	// to reduce repetition of boilerplate.
	statNames := []string{"CorSim", "UnitErr", "Err", "NZero", "FirstZero", "LastZero"}
	ss.AddStat(func(mode Modes, level Levels, phase StatsPhase) {
		for _, name := range statNames {
			if name == "NZero" && (mode != Train || level == Trial) {
				return
			}
			modeDir := ss.Stats.RecycleDir(mode.String())
			curModeDir := ss.Current.RecycleDir(mode.String())
			levelDir := modeDir.RecycleDir(level.String())
			subDir := modeDir.RecycleDir((level - 1).String()) // note: will fail for Cycle
			tsr := datafs.Value[float64](levelDir, name)
			ndata := int(ss.Net.Context().NData)
			var stat float64
			if phase == Start {
				tsr.SetNumRows(0)
				if ps := plot.GetStylersFrom(tsr); ps == nil {
					ps.Add(func(s *plot.Style) {
						s.Range.SetMin(0).SetMax(1)
						s.On = true
						switch name {
						case "NZero":
							s.On = false
						case "FirstZero", "LastZero":
							if level < Run {
								s.On = false
							}
						}
					})
					plot.SetStylersTo(tsr, ps)
				}
				switch name {
				case "NZero":
					if level == Epoch {
						datafs.Scalar[float64](curModeDir, name).SetFloat1D(0, 0)
					}
				case "FirstZero", "LastZero":
					if level == Epoch {
						datafs.Scalar[float64](curModeDir, name).SetFloat1D(-1, 0)
					}
				}
				continue
			}
			switch level {
			case Trial:
				out := ss.Net.LayerByName("Output")
				for di := range ndata {
					var stat float64
					switch name {
					case "CorSim":
						stat = 1.0 - float64(axon.LayerStates[axon.LayerPhaseDiff, out.Index, di])
					case "UnitErr":
						stat = out.PctUnitErr(ss.Net.Context())[di]
					case "Err":
						uniterr := datafs.Value[float64](curModeDir, "UnitErr", ndata).Float1D(di)
						stat = 1.0
						if uniterr == 0 {
							stat = 0
						}
					}
					datafs.Value[float64](curModeDir, name, ndata).SetFloat1D(stat, di)
					tsr.AppendRowFloat(stat)
				}
			case Epoch:
				switch name {
				case "NZero":
					err := stats.StatSum.Call(subDir.Value("Err")).Float1D(0)
					stat = curModeDir.Item(name).AsFloat64()
					if err == 0 {
						stat++
					} else {
						stat = 0
					}
					curModeDir.Item(name).SetFloat64(stat)
				case "FirstZero":
					nz := curModeDir.Item("NZero").AsFloat64()
					stat = curModeDir.Item(name).AsFloat64()
					if stat < 0 && nz == 1 {
						stat = curModeDir.Item("Epoch").AsFloat64()
					}
					curModeDir.Item(name).SetFloat64(stat)
				case "LastZero":
					nz := curModeDir.Item("NZero").AsFloat64()
					stat = curModeDir.Item(name).AsFloat64()
					if stat < 0 && nz >= float64(ss.Config.Run.NZero) {
						stat = curModeDir.Item("Epoch").AsFloat64()
					}
					curModeDir.Item(name).SetFloat64(stat)
				default:
					stat = stats.StatMean.Call(subDir.Value(name)).Float1D(0)
				}
				tsr.AppendRowFloat(stat)
			case Run:
				switch name {
				case "NZero", "FirstZero", "LastZero":
					stat = subDir.Value(name).Float1D(-1)
				default:
					stat = stats.StatMean.Call(subDir.Value(name)).Float1D(0)
				}
				tsr.AppendRowFloat(stat)
			}
		}
	})

	perTrlFunc := axon.StatPerTrialMSec(ss.Stats, "Err", Train, Trial, Epoch, Run)
	ss.AddStat(func(mode Modes, level Levels, phase StatsPhase) {
		perTrlFunc(mode, level, phase == Start)
	})

	lays := net.LayersByType(axon.SuperLayer, axon.CTLayer, axon.TargetLayer)
	actGeFunc := axon.StatLayerActGe(ss.Stats, net, lays, Train, Trial, Epoch, Run)
	ss.AddStat(func(mode Modes, level Levels, phase StatsPhase) {
		actGeFunc(mode, level, phase == Start)
	})
}

// StatCounters returns counters string to show at bottom of netview.
func (ss *Sim) StatCounters(md, tm enums.Enum) string {
	counters := ss.Loops.Stacks[md].CountersString()
	return counters
	// di := ss.ViewUpdate.View.Di
	// if tm == Trial {
	// 	ss.TrialStats(di) // get trial stats for current di
	// }
	// ss.StatCounters(di)
	// ctx := &ss.Context
	// mode := ctx.Mode
	// ss.Stats.SetInt("Epoch", trnEpc)
	// trl := ss.Stats.Int("Trial")
	// ss.Stats.SetInt("Di", di)
	// ss.Stats.SetString("TrialName", ss.Stats.StringDi("TrialName", di))
	// ss.ViewUpdate.Text = ss.Stats.Print([]string{"Run", "Epoch", "Trial", "Di", "TrialName", "Cycle", "UnitErr", "TrlErr", "PhaseDiff"})
}

func (ss *Sim) ConfigLogs() {
	// ss.Stats.SetString("RunName", ss.Params.RunName(0)) // used for naming logs, stats, etc
	//
	// ss.Logs.AddCounterItems(Run, Epoch, Trial, Cycle)
	// ss.Logs.AddStatIntNoAggItem(AlmOdes, Trial, "Di")
	//
	// ss.Logs.AddCopyFromFloatItems(Train, []Times{Epoch, Run}, Test, Epoch, "Tst", "PhaseDiff", "UnitErr", "PctCor", "PctErr")
	//
	// axon.LogInputLayer(&ss.Logs, ss.Net, Train)
	//
	// axon.LogAddPCAItems(&ss.Logs, ss.Net, Train, Run, Epoch, Trial)
	//
	// ss.Logs.AddLayerTensorItems(ss.Net, "Act", Test, Trial, "InputLayer", "TargetLayer")
}

//////// GUI

// ConfigGUI configures the Cogent Core GUI interface for this simulation.
func (ss *Sim) ConfigGUI() {
	title := "Axon Random Associator"
	ss.GUI.MakeBody(ss, "ra25", title, `This demonstrates a basic Axon model. See <a href="https://github.com/emer/emergent">emergent on GitHub</a>.</p>`)
	ss.GUI.FS = ss.Root
	ss.GUI.DataRoot = "Root"
	ss.GUI.CycleUpdateInterval = 10

	nv := ss.GUI.AddNetView("Network")
	nv.Options.MaxRecs = 300
	nv.SetNet(ss.Net)
	ss.TrainUpdate.Config(nv, axon.Phase)
	ss.TestUpdate.Config(nv, axon.Phase)
	ss.GUI.OnStop = func() {
		ss.TrainUpdate.UpdateWhenStopped()
	}

	nv.SceneXYZ().Camera.Pose.Pos.Set(0, 1, 2.75) // more "head on" than default which is more "top down"
	nv.SceneXYZ().Camera.LookAt(math32.Vec3(0, 0, 0), math32.Vec3(0, 1, 0))

	ss.GUI.UpdateFiles()
	ss.InitStats()
	ss.GUI.FinalizeGUI(false)

	// if ss.Config.Run.GPU {
	// 	// vgpu.Debug = ss.Config.Debug // when debugging GPU..
	// 	ss.Net.ConfigGPUnoGUI(&ss.Context) // must happen after gui or no gui
	// 	core.TheApp.AddQuitCleanFunc(func() {
	// 		ss.Net.GPU.Destroy()
	// 	})
	// }
}

func (ss *Sim) MakeToolbar(p *tree.Plan) {
	ss.GUI.AddLooperCtrl(p, ss.Loops)

	tree.Add(p, func(w *core.Separator) {})
	ss.GUI.AddToolbarItem(p, egui.ToolbarItem{
		Label:   "Reset RunLog",
		Icon:    icons.Reset,
		Tooltip: "Reset the accumulated log of all Runs, which are tagged with the ParamSet used",
		Active:  egui.ActiveAlways,
		Func: func() {
			// ss.Logs.ResetLog(Train, Run)
			// ss.GUI.UpdatePlot(Train, Run)
		},
	})

	tree.Add(p, func(w *core.Separator) {})
	ss.GUI.AddToolbarItem(p, egui.ToolbarItem{
		Label:   "New Seed",
		Icon:    icons.Add,
		Tooltip: "Generate a new initial random seed to get different results.  By default, Init re-establishes the same initial seed every time.",
		Active:  egui.ActiveAlways,
		Func: func() {
			ss.RandSeeds.NewSeeds()
		},
	})
	ss.GUI.AddToolbarItem(p, egui.ToolbarItem{
		Label:   "README",
		Icon:    icons.FileMarkdown,
		Tooltip: "Opens your browser on the README file that contains instructions for how to run this model.",
		Active:  egui.ActiveAlways,
		Func: func() {
			core.TheApp.OpenURL("https://github.com/emer/axon/blob/main/examples/ra25/README.md")
		},
	})
}

func (ss *Sim) RunGUI() {
	ss.Init()
	ss.ConfigGUI()
	ss.GUI.Body.RunMainWindow()
}

func (ss *Sim) RunNoGUI() {
	if ss.Config.Params.Note != "" {
		mpi.Printf("Note: %s\n", ss.Config.Params.Note)
	}
	if ss.Config.Log.SaveWeights {
		mpi.Printf("Saving final weights per run\n")
	}
	runName := ss.Params.RunName(ss.Config.Run.Run)
	datafs.Scalar[string](ss.Current, "RunName").SetString1D(runName, 0)
	netName := ss.Net.Name

	ss.Init()

	axon.OpenLogFile(ss.Config.Log.Trial, ss.StatsData(Train, Trial).GetDirTable(nil), netName, runName, "trl")
	axon.OpenLogFile(ss.Config.Log.Epoch, ss.StatsData(Train, Epoch).GetDirTable(nil), netName, runName, "epc")
	axon.OpenLogFile(ss.Config.Log.Run, ss.StatsData(Train, Run).GetDirTable(nil), netName, runName, "run")

	axon.OpenLogFile(ss.Config.Log.TestTrial, ss.StatsData(Test, Trial).GetDirTable(nil), netName, runName, "trl")
	axon.OpenLogFile(ss.Config.Log.TestEpoch, ss.StatsData(Test, Epoch).GetDirTable(nil), netName, runName, "epc")

	mpi.Printf("Running %d Runs starting at %d\n", ss.Config.Run.NRuns, ss.Config.Run.Run)
	ss.Loops.Loop(Train, Run).Counter.SetCurMaxPlusN(ss.Config.Run.Run, ss.Config.Run.NRuns)

	if ss.Config.Run.StartWts != "" { // this is just for testing -- not usually needed
		ss.Loops.Step(Train, 1, Trial) // get past NewRun
		ss.Net.OpenWeightsJSON(core.Filename(ss.Config.Run.StartWts))
		mpi.Printf("Starting with initial weights from: %s\n", ss.Config.Run.StartWts)
	}

	// if ss.Config.Run.GPU {
	// 	ss.Net.ConfigGPUnoGUI(&ss.Context)
	// }
	// mpi.Printf("Set NThreads to: %d\n", ss.Net.NThreads)

	ss.Loops.Run(Train)

	ss.StatsData(Train, Epoch).GetDirTable(nil).CloseLog()

	// ss.Net.GPU.Destroy() // safe even if no GPU
}
