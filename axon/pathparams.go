// Code generated by "goal build"; DO NOT EDIT.
//line pathparams.goal:1
// Copyright (c) 2023, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"encoding/json"
	"strings"
	"sync/atomic"

	"cogentcore.org/core/math32"
)

//gosl:start

const (
	// StartOff is the starting offset.
	StartOff int32 = iota

	// Number of items.
	Nitems

	// Number of StartN elements.
	StartNN
)

// StartN holds a starting offset index and a number of items
// arranged from Start to Start+N (exclusive).
// This is not 16 byte padded and only for use on CPU side.
type StartN struct {

	// starting offset
	Start uint32

	// number of items --
	N uint32

	pad, pad1 uint32 // todo: see if we can do without these?
}

// PathIndexes contains path-level index information into global memory arrays
type PathIndexes struct {
	PathIndex  uint32 // index of the pathway in global path list: [Layer][SendPaths]
	RecvLayer  uint32 // index of the receiving layer in global list of layers
	RecvNeurSt uint32 // starting index of neurons in recv layer -- so we don't need layer to get to neurons
	RecvNeurN  uint32 // number of neurons in recv layer
	SendLayer  uint32 // index of the sending layer in global list of layers
	SendNeurSt uint32 // starting index of neurons in sending layer -- so we don't need layer to get to neurons
	SendNeurN  uint32 // number of neurons in send layer
	SynapseSt  uint32 // start index into global Synapse array: [Layer][SendPaths][Synapses]
	SendConSt  uint32 // start index into global PathSendCon array: [Layer][SendPaths][SendNeurons]
	RecvConSt  uint32 // start index into global PathRecvCon array: [Layer][RecvPaths][RecvNeurons]
	RecvSynSt  uint32 // start index into global sender-based Synapse index array: [Layer][SendPaths][Synapses]
	GBufSt     uint32 // start index into global PathGBuf global array: [Layer][RecvPaths][RecvNeurons][MaxDelay+1]
	GSynSt     uint32 // start index into global PathGSyn global array: [Layer][RecvPaths][RecvNeurons]

	pad, pad1, pad2 uint32
}

// RecvNIndexToLayIndex converts a neuron's index in network level global list of all neurons
// to receiving layer-specific index-- e.g., for accessing GBuf and GSyn values.
// Just subtracts RecvNeurSt -- docu-function basically..
func (pi *PathIndexes) RecvNIndexToLayIndex(ni uint32) uint32 {
	return ni - pi.RecvNeurSt
}

// SendNIndexToLayIndex converts a neuron's index in network level global list of all neurons
// to sending layer-specific index.  Just subtracts SendNeurSt -- docu-function basically..
func (pi *PathIndexes) SendNIndexToLayIndex(ni uint32) uint32 {
	return ni - pi.SendNeurSt
}

// GScaleValues holds the conductance scaling values.
// These are computed once at start and remain constant thereafter,
// and therefore belong on Params and not on PathValues.
type GScaleValues struct {

	// scaling factor for integrating synaptic input conductances (G's), originally computed as a function of sending layer activity and number of connections, and typically adapted from there -- see Path.PathScale adapt params
	Scale float32 `edit:"-"`

	// normalized relative proportion of total receiving conductance for this pathway: PathScale.Rel / sum(PathScale.Rel across relevant paths)
	Rel float32 `edit:"-"`

	pad, pad1 float32
}

// PathParams contains all of the path parameters.
// These values must remain constant over the course of computation.
// On the GPU, they are loaded into a uniform.
type PathParams struct {

	// functional type of path, which determines functional code path
	// for specialized layer types, and is synchronized with the Path.Type value
	PathType PathTypes

	pad, pad1, pad2 int32

	// recv and send neuron-level pathway index array access info
	Indexes PathIndexes `display:"-"`

	// synaptic communication parameters: delay, probability of failure
	Com SynComParams `display:"inline"`

	// pathway scaling parameters for computing GScale:
	// modulates overall strength of pathway, using both
	// absolute and relative factors, with adaptation option to maintain target max conductances
	PathScale PathScaleParams `display:"inline"`

	// slowly adapting, structural weight value parameters,
	// which control initial weight values and slower outer-loop adjustments
	SWts SWtParams `display:"add-fields"`

	// synaptic-level learning parameters for learning in the fast LWt values.
	Learn LearnSynParams `display:"add-fields"`

	// conductance scaling values
	GScale GScaleValues `display:"inline"`

	// Params for RWPath and TDPredPath for doing dopamine-modulated learning
	// for reward prediction: Da * Send activity.
	// Use in RWPredLayer or TDPredLayer typically to generate reward predictions.
	// If the Da sign is positive, the first recv unit learns fully; for negative,
	// second one learns fully.
	// Lower lrate applies for opposite cases.  Weights are positive-only.
	RLPred RLPredPathParams `display:"inline"`

	// for trace-based learning in the MatrixPath. A trace of synaptic co-activity
	// is formed, and then modulated by dopamine whenever it occurs.
	// This bridges the temporal gap between gating activity and subsequent activity,
	// and is based biologically on synaptic tags.
	// Trace is reset at time of reward based on ACh level from CINs.
	Matrix MatrixPathParams `display:"inline"`

	// Basolateral Amygdala pathway parameters.
	BLA BLAPathParams `display:"inline"`

	// Hip bench parameters.
	Hip HipPathParams `display:"inline"`
}

func (pt *PathParams) Defaults() {
	pt.Com.Defaults()
	pt.SWts.Defaults()
	pt.PathScale.Defaults()
	pt.Learn.Defaults()
	pt.RLPred.Defaults()
	pt.Matrix.Defaults()
	pt.BLA.Defaults()
	pt.Hip.Defaults()
}

func (pt *PathParams) Update() {
	pt.Com.Update()
	pt.PathScale.Update()
	pt.SWts.Update()
	pt.Learn.Update()
	pt.RLPred.Update()
	pt.Matrix.Update()
	pt.BLA.Update()
	pt.Hip.Update()

	if pt.PathType == CTCtxtPath {
		pt.Com.GType = ContextG
	}
}

func (pt *PathParams) ShouldDisplay(field string) bool {
	switch field {
	case "RLPred":
		return pt.PathType == RWPath || pt.PathType == TDPredPath
	case "Matrix":
		return pt.PathType == VSMatrixPath || pt.PathType == DSMatrixPath
	case "BLA":
		return pt.PathType == BLAPath
	case "Hip":
		return pt.PathType == HipPath
	default:
		return true
	}
}

func (pt *PathParams) AllParams() string {
	str := ""
	b, _ := json.MarshalIndent(&pt.Com, "", " ")
	str += "Com: {\n " + JsonToParams(b)
	b, _ = json.MarshalIndent(&pt.PathScale, "", " ")
	str += "PathScale: {\n " + JsonToParams(b)
	b, _ = json.MarshalIndent(&pt.SWts, "", " ")
	str += "SWt: {\n " + JsonToParams(b)
	b, _ = json.MarshalIndent(&pt.Learn, "", " ")
	str += "Learn: {\n " + strings.Replace(JsonToParams(b), " LRate: {", "\n  LRate: {", -1)

	switch pt.PathType {
	case RWPath, TDPredPath:
		b, _ = json.MarshalIndent(&pt.RLPred, "", " ")
		str += "RLPred: {\n " + JsonToParams(b)
	case VSMatrixPath, DSMatrixPath:
		b, _ = json.MarshalIndent(&pt.Matrix, "", " ")
		str += "Matrix: {\n " + JsonToParams(b)
	case BLAPath:
		b, _ = json.MarshalIndent(&pt.BLA, "", " ")
		str += "BLA: {\n " + JsonToParams(b)
	case HipPath:
		b, _ = json.MarshalIndent(&pt.BLA, "", " ")
		str += "Hip: {\n " + JsonToParams(b)
	}
	return str
}

func (pt *PathParams) IsInhib() bool {
	return pt.Com.GType == InhibitoryG
}

func (pt *PathParams) IsExcitatory() bool {
	return pt.Com.GType == ExcitatoryG
}

// SetFixedWts sets parameters for fixed, non-learning weights
// with a default of Mean = 0.8, Var = 0 strength
func (pt *PathParams) SetFixedWts() {
	pt.SWts.Init.SPct = 0
	pt.Learn.Learn.SetBool(false)
	pt.SWts.Adapt.On.SetBool(false)
	pt.SWts.Adapt.SigGain = 1
	pt.SWts.Init.Mean = 0.8
	pt.SWts.Init.Var = 0.0
	pt.SWts.Init.Sym.SetBool(false)
}

// SynRecvLayerIndex converts the Synapse RecvIndex of recv neuron's index
// in network level global list of all neurons to receiving
// layer-specific index.
func (pt *PathParams) SynRecvLayerIndex(syni uint32) uint32 {
	return pt.Indexes.RecvNIndexToLayIndex(SynapseIxs.Value(int(SynRecvIndex), int(syni)))
}

// SynSendLayerIndex converts the Synapse SendIndex of sending neuron's index
// in network level global list of all neurons to sending
// layer-specific index.
func (pt *PathParams) SynSendLayerIndex(syni uint32) uint32 {
	return pt.Indexes.SendNIndexToLayIndex(SynapseIxs.Value(int(SynSendIndex), int(syni)))
}

//////// Cycle

// GatherSpikes integrates G*Raw and G*Syn values for given recv neuron
// while integrating the Recv Path-level GSyn integrated values.
func (pt *PathParams) GatherSpikes(ctx *Context, ly *LayerParams, ni, di, lni uint32) {
	nix := GetNetworkIxs(0)
	maxd := nix.MaxData
	bi := pt.Indexes.GBufSt + pt.Com.ReadIndex(lni, di, ctx.CyclesTotal, pt.Indexes.RecvNeurN, maxd)
	gRaw := pt.Com.FloatFromGBuf(PathGBuf.Value1D(int(bi)))
	PathGBuf.Set1D(0, int(bi))
	gsi := lni*maxd + di
	gsyn := PathGSyns.Value1D(int(pt.Indexes.GSynSt + gsi))
	pt.GatherSpikesGSyn(ctx, ly, ni, di, gRaw, &gsyn)
	PathGSyns.Set1D(gsyn, int(pt.Indexes.GSynSt+gsi))
}

// GatherSpikes integrates G*Raw and G*Syn values for given neuron
// from the given Path-level GRaw value, first integrating
// pathway-level GSyn value.
func (pt *PathParams) GatherSpikesGSyn(ctx *Context, ly *LayerParams, ni, di uint32, gRaw float32, gSyn *float32) {
	switch pt.Com.GType {
	case ExcitatoryG:
		*gSyn = ly.Acts.Dt.GeSynFromRaw(*gSyn, gRaw)
		Neurons.SetAdd(gRaw, int(GeRaw), int(ni), int(di))
		Neurons.SetAdd(*gSyn, int(GeSyn), int(ni), int(di))
	case InhibitoryG:
		*gSyn = ly.Acts.Dt.GiSynFromRaw(*gSyn, gRaw)
		Neurons.SetAdd(gRaw, int(GiRaw), int(ni), int(di))
		Neurons.SetAdd(*gSyn, int(GiSyn), int(ni), int(di))
	case ModulatoryG:
		*gSyn = ly.Acts.Dt.GeSynFromRaw(*gSyn, gRaw)
		Neurons.SetAdd(gRaw, int(GModRaw), int(ni), int(di))
		Neurons.SetAdd(*gSyn, int(GModSyn), int(ni), int(di))
	case MaintG:
		*gSyn = ly.Acts.Dt.GeSynFromRaw(*gSyn, gRaw)
		Neurons.SetAdd(gRaw, int(GMaintRaw), int(ni), int(di))
	// note: Syn happens via NMDA in Act
	case ContextG:
		Neurons.SetAdd(gRaw, int(CtxtGeRaw), int(ni), int(di))
	}
}

// SendSpike sends a spike from the sending neuron at index sendIndex
// into the GBuf buffer on the receiver side. The buffer on the receiver side
// is a ring buffer, which is used for modelling the time delay between
// sending and receiving spikes.
func (pt *PathParams) SendSpike(ctx *Context, ni, di, lni uint32) {
	sendVal := pt.GScale.Scale * pt.Com.FloatToIntFactor() // pre-bake in conversion to uint factor
	if pt.PathType == CTCtxtPath {
		if uint32(ctx.Cycle) != uint32(ctx.ThetaCycles)-1-pt.Com.DelLen {
			return
		}
		sendVal *= Neurons.Value(int(Burst), int(ni), int(di)) // Burst is regular CaSpkP for all non-SuperLayer neurons
	} else {
		if Neurons.Value(int(Spike), int(ni), int(di)) == 0 {
			return
		}
	}
	nix := GetNetworkIxs(0)
	maxd := nix.MaxData
	recvNeurSt := pt.Indexes.RecvNeurSt
	cni := pt.Indexes.SendConSt + lni
	synst := pt.Indexes.SynapseSt + PathSendCon.Value(int(cni), int(StartOff))
	synn := PathSendCon.Value(int(cni), int(Nitems))
	for ci := uint32(0); ci < synn; ci++ {
		syni := synst + ci
		ri := SynapseIxs.Value(int(SynRecvIndex), int(syni))
		bi := pt.Indexes.GBufSt + pt.Com.WriteIndex(ri-recvNeurSt, di, ctx.CyclesTotal, pt.Indexes.RecvNeurN, maxd)
		sv := int32(sendVal * Synapses.Value(int(Wt), int(syni)))
		atomic.AddInt32(&PathGBuf.Values[bi], sv)
	}
}

///////////////////////////////////////////////////
// DWt

// DWtSyn is the overall entry point for weight change (learning) at given synapse.
// It selects appropriate function based on pathway type.
// rpl is the receiving layer SubPool
func (pt *PathParams) DWtSyn(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool, isTarget bool) {
	switch pt.PathType {
	case RWPath:
		pt.DWtSynRWPred(ctx, syni, si, ri, di, layPool, subPool)
	case TDPredPath:
		pt.DWtSynTDPred(ctx, syni, si, ri, di, layPool, subPool)
	case VSMatrixPath:
		pt.DWtSynVSMatrix(ctx, syni, si, ri, di, layPool, subPool)
	case DSMatrixPath:
		pt.DWtSynDSMatrix(ctx, syni, si, ri, di, layPool, subPool)
	case VSPatchPath:
		pt.DWtSynVSPatch(ctx, syni, si, ri, di, layPool, subPool)
	case BLAPath:
		pt.DWtSynBLA(ctx, syni, si, ri, di, layPool, subPool)
	case HipPath:
		pt.DWtSynHip(ctx, syni, si, ri, di, layPool, subPool, isTarget) // by default this is the same as DWtSynCortex (w/ unused Hebb component in the algorithm) except that it uses WtFromDWtSynNoLimits
	default:
		if pt.Learn.Hebb.On.IsTrue() {
			pt.DWtSynHebb(ctx, syni, si, ri, di, layPool, subPool)
		} else {
			pt.DWtSynCortex(ctx, syni, si, ri, di, layPool, subPool, isTarget)
		}
	}
}

// SynCa gets the synaptic calcium P (potentiation) and D (depression)
// values, using optimized computation.
func (pt *PathParams) SynCa(ctx *Context, si, ri, di uint32, syCaP, syCaD *float32) {
	rb0 := Neurons.Value(int(SpkBin0), int(ri), int(di))
	sb0 := Neurons.Value(int(SpkBin0), int(si), int(di))
	rb1 := Neurons.Value(int(SpkBin1), int(ri), int(di))
	sb1 := Neurons.Value(int(SpkBin1), int(si), int(di))
	rb2 := Neurons.Value(int(SpkBin2), int(ri), int(di))
	sb2 := Neurons.Value(int(SpkBin2), int(si), int(di))
	rb3 := Neurons.Value(int(SpkBin3), int(ri), int(di))
	sb3 := Neurons.Value(int(SpkBin3), int(si), int(di))
	rb4 := Neurons.Value(int(SpkBin4), int(ri), int(di))
	sb4 := Neurons.Value(int(SpkBin4), int(si), int(di))
	rb5 := Neurons.Value(int(SpkBin5), int(ri), int(di))
	sb5 := Neurons.Value(int(SpkBin5), int(si), int(di))
	rb6 := Neurons.Value(int(SpkBin6), int(ri), int(di))
	sb6 := Neurons.Value(int(SpkBin6), int(si), int(di))
	rb7 := Neurons.Value(int(SpkBin7), int(ri), int(di))
	sb7 := Neurons.Value(int(SpkBin7), int(si), int(di))

	b0 := 0.1 * (rb0 * sb0)
	b1 := 0.1 * (rb1 * sb1)
	b2 := 0.1 * (rb2 * sb2)
	b3 := 0.1 * (rb3 * sb3)
	b4 := 0.1 * (rb4 * sb4)
	b5 := 0.1 * (rb5 * sb5)
	b6 := 0.1 * (rb6 * sb6)
	b7 := 0.1 * (rb7 * sb7)

	pt.Learn.KinaseCa.FinalCa(b0, b1, b2, b3, b4, b5, b6, b7, syCaP, syCaD)
}

// DWtSynCortex computes the weight change (learning) at given synapse for cortex.
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pt *PathParams) DWtSynCortex(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool, isTarget bool) {
	var syCaP, syCaD float32
	pt.SynCa(ctx, si, ri, di, &syCaP, &syCaD)

	dtr := syCaD                   // delta trace, caD reflects entire window
	if pt.PathType == CTCtxtPath { // layer 6 CT pathway
		dtr = Neurons.Value(int(BurstPrv), int(si), int(di))
	}
	// save delta trace for GUI
	SynapseTraces.Set(dtr, int(DTr), int(syni), int(di))
	// TrFromCa(prev-multiTrial Integrated Trace, deltaTrace), as a mixing func
	tr := pt.Learn.Trace.TrFromCa(SynapseTraces.Value(int(Tr), int(syni), int(di)), dtr)
	// save new trace, updated w/ credit assignment (dependent on Tau in the TrFromCa function
	SynapseTraces.Set(tr, int(Tr), int(syni), int(di))
	// failed con, no learn
	if Synapses.Value(int(Wt), int(syni)) == 0 {
		return
	}

	// error-driven learning
	var err float32
	if isTarget {
		err = syCaP - syCaD // for target layers, syn Ca drives error signal directly
	} else {
		err = tr * (Neurons.Value(int(NrnCaP), int(ri), int(di)) - Neurons.Value(int(NrnCaD), int(ri), int(di))) // hiddens: recv NMDA Ca drives error signal w/ trace credit
	}
	// note: trace ensures that nothing changes for inactive synapses..
	// sb immediately -- enters into zero sum.
	// also other types might not use, so need to do this per learning rule
	lwt := Synapses.Value(int(LWt), int(syni)) // linear weight
	if err > 0 {
		err *= (1 - lwt)
	} else {
		err *= lwt
	}
	if pt.PathType == CTCtxtPath { // rn.RLRate IS needed for other pathways, just not the context one
		SynapseTraces.Set(pt.Learn.LRate.Eff*err, int(DiDWt), int(syni), int(di))
	} else {
		SynapseTraces.Set(Neurons.Value(int(RLRate), int(ri), int(di))*pt.Learn.LRate.Eff*err, int(DiDWt), int(syni), int(di))
	}
}

// DWtSynHebb computes the weight change (learning) at given synapse for cortex.
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pt *PathParams) DWtSynHebb(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	rNrnCaP := Neurons.Value(int(NrnCaP), int(ri), int(di))
	sNrnCap := Neurons.Value(int(NrnCaP), int(si), int(di))
	lwt := Synapses.Value(int(LWt), int(syni)) // linear weight
	hebb := rNrnCaP * (pt.Learn.Hebb.Up*sNrnCap*(1-lwt) - pt.Learn.Hebb.Down*(1-sNrnCap)*lwt)
	// not: Neurons[RLRate, ri, di]*
	SynapseTraces.Set(pt.Learn.LRate.Eff*hebb, int(DiDWt), int(syni), int(di))
}

// DWtSynHip computes the weight change (learning) at given synapse for cortex + Hip (CPCA Hebb learning).
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
// Adds proportional CPCA learning rule for hip-specific paths
func (pt *PathParams) DWtSynHip(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool, isTarget bool) {
	var syCaP, syCaD float32
	pt.SynCa(ctx, si, ri, di, &syCaP, &syCaD)
	dtr := syCaD // delta trace, caD reflects entire window
	// save delta trace for GUI
	SynapseTraces.Set(dtr, int(DTr), int(syni), int(di))
	// TrFromCa(prev-multiTrial Integrated Trace, deltaTrace), as a mixing func
	tr := pt.Learn.Trace.TrFromCa(SynapseTraces.Value(int(Tr), int(syni), int(di)), dtr)
	// save new trace, updated w/ credit assignment (dependent on Tau in the TrFromCa function
	SynapseTraces.Set(tr, int(Tr), int(syni), int(di))
	// failed con, no learn
	if Synapses.Value(int(Wt), int(syni)) == 0 {
		return
	}

	// error-driven learning part
	rNrnCaP := Neurons.Value(int(NrnCaP), int(ri), int(di))
	rNrnCaD := Neurons.Value(int(NrnCaD), int(ri), int(di))
	var err float32
	if isTarget {
		err = syCaP - syCaD // for target layers, syn Ca drives error signal directly
	} else {
		err = tr * (rNrnCaP - rNrnCaD) // hiddens: recv NMDA Ca drives error signal w/ trace credit
	}
	// note: trace ensures that nothing changes for inactive synapses..
	// sb immediately -- enters into zero sum.
	// also other types might not use, so need to do this per learning rule
	lwt := Synapses.Value(int(LWt), int(syni)) // linear weight
	if err > 0 {
		err *= (1 - lwt)
	} else {
		err *= lwt
	}

	// hebbian-learning part
	sNrnCap := Neurons.Value(int(NrnCaP), int(si), int(di))
	savg := 0.5 + pt.Hip.SAvgCor*(pt.Hip.SNominal-0.5)
	savg = 0.5 / math32.Max(pt.Hip.SAvgThr, savg) // keep this Sending Average Correction term within bounds (SAvgThr)
	hebb := rNrnCaP * (sNrnCap*(savg-lwt) - (1-sNrnCap)*lwt)

	// setting delta weight (note: impossible to be CTCtxtPath)
	dwt := Neurons.Value(int(RLRate), int(ri), int(di)) * pt.Learn.LRate.Eff * (pt.Hip.Hebb*hebb + pt.Hip.Err*err)
	SynapseTraces.Set(dwt, int(DiDWt), int(syni), int(di))
}

// DWtSynBLA computes the weight change (learning) at given synapse for BLAPath type.
// Like the BG Matrix learning rule, a synaptic tag "trace" is established at CS onset (ACh)
// and learning at US / extinction is a function of trace * delta from US activity
// (temporal difference), which limits learning.
func (pt *PathParams) DWtSynBLA(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	dwt := float32(0)
	ach := GlobalScalars.Value(int(GvACh), int(di))
	if GlobalScalars.Value(int(GvHasRew), int(di)) > 0 { // learn and reset
		ract := Neurons.Value(int(CaSpkD), int(ri), int(di))
		if ract < pt.Learn.Trace.LearnThr {
			ract = 0
		}
		tr := SynapseTraces.Value(int(Tr), int(syni), int(di))
		ustr := pt.BLA.USTrace
		tr = ustr*Neurons.Value(int(Burst), int(si), int(di)) + (1.0-ustr)*tr
		delta := Neurons.Value(int(CaSpkP), int(ri), int(di)) - Neurons.Value(int(SpkPrv), int(ri), int(di))
		if delta < 0 { // neg delta learns slower in Acq, not Ext
			delta *= pt.BLA.NegDeltaLRate
		}
		dwt = tr * delta * ract
		SynapseTraces.Set(0.0, int(Tr), int(syni), int(di))
	} else if ach > pt.BLA.AChThr {
		// note: the former NonUSLRate parameter is not used -- Trace update Tau replaces it..  elegant
		dtr := ach * Neurons.Value(int(Burst), int(si), int(di))
		SynapseTraces.Set(dtr, int(DTr), int(syni), int(di))
		tr := pt.Learn.Trace.TrFromCa(SynapseTraces.Value(int(Tr), int(syni), int(di)), dtr)
		SynapseTraces.Set(tr, int(Tr), int(syni), int(di))
	} else {
		SynapseTraces.Set(0.0, int(DTr), int(syni), int(di))
	}
	lwt := Synapses.Value(int(LWt), int(syni))
	if dwt > 0 {
		dwt *= (1 - lwt)
	} else {
		dwt *= lwt
	}
	SynapseTraces.Set(Neurons.Value(int(RLRate), int(ri), int(di))*pt.Learn.LRate.Eff*dwt, int(DiDWt), int(syni), int(di))
}

// DWtSynRWPred computes the weight change (learning) at given synapse,
// for the RWPredPath type
func (pt *PathParams) DWtSynRWPred(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars.Value(int(GvDA), int(di))
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	if NeuronIxs.Value(int(NrnNeurIndex), int(ri)) == 0 {
		if Neurons.Value(int(Ge), int(ri), int(di)) > Neurons.Value(int(Act), int(ri), int(di)) && da > 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons.Value(int(Ge), int(ri), int(di)) < Neurons.Value(int(Act), int(ri), int(di)) && da < 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr                                                                                    // negative case
		if Neurons.Value(int(Ge), int(ri), int(di)) > Neurons.Value(int(Act), int(ri), int(di)) && da < 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons.Value(int(Ge), int(ri), int(di)) < Neurons.Value(int(Act), int(ri), int(di)) && da > 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons.Value(int(CaSpkP), int(si), int(di)) // no recv unit activation
	SynapseTraces.Set(eff_lr*dwt, int(DiDWt), int(syni), int(di))
}

// DWtSynTDPred computes the weight change (learning) at given synapse,
// for the TDRewPredPath type
func (pt *PathParams) DWtSynTDPred(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars.Value(int(GvDA), int(di))
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	ni := NeuronIxs.Value(int(NrnNeurIndex), int(ri))
	if ni == 0 {
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons.Value(int(SpkPrv), int(si), int(di)) // no recv unit activation, prior trial act
	SynapseTraces.Set(eff_lr*dwt, int(DiDWt), int(syni), int(di))
}

// DWtSynVSMatrix computes the weight change (learning) at given synapse,
// for the VSMatrixPath type.
func (pt *PathParams) DWtSynVSMatrix(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	// note: rn.RLRate already has BurstGain * ACh * DA * (D1 vs. D2 sign reversal) factored in.

	hasRew := GlobalScalars.Value(int(GvHasRew), int(di)) > 0
	ach := GlobalScalars.Value(int(GvACh), int(di))
	if !hasRew && ach < 0.1 {
		SynapseTraces.Set(0.0, int(DTr), int(syni), int(di))
		return
	}
	rlr := Neurons.Value(int(RLRate), int(ri), int(di))

	rplus := Neurons.Value(int(CaSpkP), int(ri), int(di))
	rminus := Neurons.Value(int(CaSpkD), int(ri), int(di))
	sact := Neurons.Value(int(CaSpkD), int(si), int(di))
	dtr := ach * (pt.Matrix.Delta * sact * (rplus - rminus))
	if rminus > pt.Learn.Trace.LearnThr { // key: prevents learning if < threshold
		dtr += ach * (pt.Matrix.Credit * sact * rminus)
	}
	if hasRew {
		tr := SynapseTraces.Value(int(Tr), int(syni), int(di))
		if pt.Matrix.VSRewLearn.IsTrue() {
			tr += (1 - GlobalScalars.Value(int(GvGoalMaint), int(di))) * dtr
		}
		dwt := rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces.Set(dwt, int(DiDWt), int(syni), int(di))
		SynapseTraces.Set(0.0, int(Tr), int(syni), int(di))
		SynapseTraces.Set(0.0, int(DTr), int(syni), int(di))
	} else {
		dtr *= rlr
		SynapseTraces.Set(dtr, int(DTr), int(syni), int(di))
		SynapseTraces.SetAdd(dtr, int(Tr), int(syni), int(di))
	}
}

// DWtSynDSMatrix computes the weight change (learning) at given synapse,
// for the DSMatrixPath type.
func (pt *PathParams) DWtSynDSMatrix(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in.

	rlr := Neurons.Value(int(RLRate), int(ri), int(di))
	if GlobalScalars.Value(int(GvHasRew), int(di)) > 0 { // US time -- use DA and current recv activity
		tr := SynapseTraces.Value(int(Tr), int(syni), int(di))
		dwt := rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces.Set(dwt, int(DiDWt), int(syni), int(di))
		SynapseTraces.Set(0.0, int(Tr), int(syni), int(di))
		SynapseTraces.Set(0.0, int(DTr), int(syni), int(di))
	} else {
		pfmod := pt.Matrix.BasePF + Neurons.Value(int(GModSyn), int(ri), int(di))
		rplus := Neurons.Value(int(CaSpkP), int(ri), int(di))
		rminus := Neurons.Value(int(CaSpkD), int(ri), int(di))
		sact := Neurons.Value(int(CaSpkD), int(si), int(di))
		dtr := rlr * (pt.Matrix.Delta * sact * (rplus - rminus))
		if rminus > pt.Learn.Trace.LearnThr { // key: prevents learning if < threshold
			dtr += rlr * (pt.Matrix.Credit * pfmod * sact * rminus)
		}
		SynapseTraces.Set(dtr, int(DTr), int(syni), int(di))
		SynapseTraces.SetAdd(dtr, int(Tr), int(syni), int(di))
	}
}

// DWtSynVSPatch computes the weight change (learning) at given synapse,
// for the VSPatchPath type.
func (pt *PathParams) DWtSynVSPatch(ctx *Context, syni, si, ri, di uint32, layPool, subPool *Pool) {
	ract := Neurons.Value(int(SpkPrv), int(ri), int(di)) // t-1
	if ract < pt.Learn.Trace.LearnThr {
		ract = 0
	}
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in.
	// and also the logic that non-positive DA leads to weight decreases.
	sact := Neurons.Value(int(SpkPrv), int(si), int(di)) // t-1
	dwt := Neurons.Value(int(RLRate), int(ri), int(di)) * pt.Learn.LRate.Eff * sact * ract
	SynapseTraces.Set(dwt, int(DiDWt), int(syni), int(di))
}

///////////////////////////////////////////////////
// WtFromDWt

// DWtFromDiDWtSyn updates DWt from data parallel DiDWt values
func (pt *PathParams) DWtFromDiDWtSyn(ctx *Context, syni uint32) {
	dwt := float32(0)
	for di := uint32(0); di < ctx.NData; di++ {
		dwt += SynapseTraces.Value(int(DiDWt), int(syni), int(di))
	}
	Synapses.SetAdd(dwt, int(DWt), int(syni))
}

// WtFromDWtSyn is the overall entry point for updating weights from weight changes.
func (pt *PathParams) WtFromDWtSyn(ctx *Context, syni uint32) {
	switch pt.PathType {
	case RWPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case TDPredPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case BLAPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case HipPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	default:
		pt.WtFromDWtSynCortex(ctx, syni)
	}
}

// WtFromDWtSynCortex updates weights from dwt changes
func (pt *PathParams) WtFromDWtSynCortex(ctx *Context, syni uint32) {
	dwt := Synapses.Value(int(DWt), int(syni))
	Synapses.SetAdd(dwt, int(DSWt), int(syni))
	wt := Synapses.Value(int(Wt), int(syni))
	lwt := Synapses.Value(int(LWt), int(syni))

	pt.SWts.WtFromDWt(&wt, &lwt, dwt, Synapses.Value(int(SWt), int(syni)))
	Synapses.Set(0, int(DWt), int(syni))
	Synapses.Set(wt, int(Wt), int(syni))
	Synapses.Set(lwt, int(LWt), int(syni))
	// pj.Com.Fail(&sy.Wt, sy.SWt) // skipping for now -- not useful actually
}

// WtFromDWtSynNoLimits -- weight update without limits
func (pt *PathParams) WtFromDWtSynNoLimits(ctx *Context, syni uint32) {
	dwt := Synapses.Value(int(DWt), int(syni))
	if dwt == 0 {
		return
	}
	Synapses.SetAdd(dwt, int(Wt), int(syni))
	if Synapses.Value(int(Wt), int(syni)) < 0 {
		Synapses.Set(0, int(Wt), int(syni))
	}
	Synapses.Set(Synapses.Value(int(Wt), int(syni)), int(LWt), int(syni))
	Synapses.Set(0, int(DWt), int(syni))
}

//gosl:end pathparams
