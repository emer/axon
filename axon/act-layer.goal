// Copyright (c) 2023, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	// "fmt"
	"cogentcore.org/core/gpu"
	"cogentcore.org/core/math32"
	"cogentcore.org/lab/tensor"
	"github.com/emer/axon/v2/fsfffb"
)

//gosl:start

////////  ApplyExt

// ApplyExtFlags gets the clear mask and set mask for updating neuron flags
// based on layer type, and whether input should be applied to Target (else Ext)
func (ly *LayerParams) ApplyExtFlags(clearMask, setMask *NeuronFlags, toTarg *bool) {
	*clearMask = NeuronHasExt | NeuronHasTarg | NeuronHasCmpr
	*toTarg = false
	switch ly.Type {
	case TargetLayer:
		*setMask = NeuronHasTarg
		*toTarg = true
	case CompareLayer:
		*setMask = NeuronHasCmpr
		*toTarg = true
	default:
		*setMask = NeuronHasExt
	}
	return
}

// InitExt initializes external input state for given neuron
func (ly *LayerParams) InitExt(ni, di uint32) {
	Neurons[ni, di, Ext] = 0.0
	Neurons[ni, di, Target] = 0.0
	NeuronClearFlag(NeuronHasExt|NeuronHasTarg|NeuronHasCmpr, ni, di)
}

// ApplyExtValue applies given external value to given neuron,
// setting flags based on type of layer.
// Should only be called on Input, Target, Compare layers.
// Negative values are not valid, and will be interpreted as missing inputs.
func (ly *LayerParams) ApplyExtValue(ni, di uint32, val float32) {
	if val < 0 {
		return
	}
	var clearMask, setMask NeuronFlags
	var toTarg bool
	ly.ApplyExtFlags(&clearMask, &setMask, &toTarg)
	if toTarg {
		Neurons[ni, di, Target] = val
	} else {
		Neurons[ni, di, Ext] = val
	}
	NeuronClearFlag(clearMask, ni, di)
	NeuronSetFlag(setMask, ni, di)
}

func (ly *LayerParams) ApplyExtsNeuron(ni, di uint32) {
	lni := ni - ly.Indexes.NeurSt // layer-based 
	ly.InitExt(ni, di)
	if IsExtLayerType(ly.Type) {
		ei := ly.Indexes.ExtsSt + lni
		ly.ApplyExtValue(ni, di, Exts[ei, di])
	}
}

// SetNeuronExtPosNeg sets neuron Ext value based on neuron index
// with positive values going in first unit, negative values rectified
// to positive in 2nd unit
func SetNeuronExtPosNeg(ctx *Context, ni, di uint32, val float32) {
	if ni == 0 {
		if val >= 0 {
			Neurons[ni, di, Ext] = val
		} else {
			Neurons[ni, di, Ext] = float32(0)
		}
	} else {
		if val >= 0 {
			Neurons[ni, di, Ext] = float32(0)
		} else {
			Neurons[ni, di, Ext] = -val
		}
	}
}

// IsTarget returns true if this layer is a Target layer.
// By default, returns true for layers of Type == TargetLayer
// Other Target layers include the PulvinarLayer in deep predictive learning.
// It is used in SynScale to not apply it to target layers.
// In both cases, Target layers are purely error-driven.
func (ly *LayerParams) IsTarget() bool {
	return ly.Type == TargetLayer || ly.Type == PulvinarLayer
}

// IsInput returns true if this layer is an Input layer.
// By default, returns true for layers of Type == axon.InputLayer
// Used to prevent adapting of inhibition or TrgAvg values.
func (ly *LayerParams) IsInput() bool {
	return ly.Type == InputLayer
}

// IsInputOrTarget returns true if this layer is either an Input
// or a Target layer.
func (ly *LayerParams) IsInputOrTarget() bool {
	return (ly.IsTarget() || ly.IsInput())
}

// IsLearnTrgAvg returns true if this layer has Learn.TrgAvgAct.RescaleOn set for learning
// adjustments based on target average activity levels, and the layer is not an
// input or target layer.
func (ly *LayerParams) IsLearnTrgAvg() bool {
	if ly.IsInput() || ly.IsTarget() || ly.Learn.TrgAvgAct.RescaleOn.IsFalse() {
		return false
	}
	return true
}

// LearnTrgAvgErrLRate returns the effective error-driven learning rate for adjusting
// target average activity levels.  This is 0 if !IsLearnTrgAvg() and otherwise
// is Learn.TrgAvgAct.ErrLRate
func (ly *LayerParams) LearnTrgAvgErrLRate() float32 {
	if !ly.IsLearnTrgAvg() {
		return 0
	}
	return ly.Learn.TrgAvgAct.ErrLRate
}

//////// Cycle

// GatherSpikes integrates G*Raw and G*Syn values for given recv neuron
// while integrating the Recv Path-level GSyn integrated values.
func (ly *LayerParams) GatherSpikes(ctx *Context, ni, di uint32) {
	lni := ni - ly.Indexes.NeurSt
	ly.GatherSpikesInit(ctx, ni, di)
	for pti := uint32(0); pti < ly.Indexes.RecvN; pti++ {
		npti := RecvPathIxs.Value1D(int(ly.Indexes.RecvSt+pti))
		pt := GetPaths(npti)
		pt.GatherSpikes(ctx, ly, ni, di, lni)
	}
	ly.GiFromSpikes(ctx, ni, di)
}

// GatherSpikesInit initializes G*Raw and G*Syn values for given neuron
// prior to integration.
func (ly *LayerParams) GatherSpikesInit(ctx *Context, ni, di uint32) {
	Neurons[ni, di, GeRaw] = 0.0
	Neurons[ni, di, GiRaw] = 0.0
	Neurons[ni, di, GModRaw] = 0.0
	Neurons[ni, di, GModSyn] = 0.0
	Neurons[ni, di, GMaintRaw] = 0.0
	Neurons[ni, di, CtxtGeRaw] = 0.0
	Neurons[ni, di, GeSyn] = NeuronAvgs[ni, GeBase]
	Neurons[ni, di, GiSyn] = NeuronAvgs[ni, GiBase]
}

// GiFromSpikes gets the Spike, GeRaw and GeExt from neurons in the pools
// where Spike drives FBsRaw = raw feedback signal,
// GeRaw drives FFsRaw = aggregate feedforward excitatory spiking input.
// GeExt represents extra excitatory input from other sources.
// Then integrates new inhibitory conductances therefrom,
// at the layer and pool level.
// Called separately by Network.CycleImpl on all Layers
// Also updates all AvgMax values at the Cycle level.
func (ly *LayerParams) GiFromSpikes(ctx *Context, ni, di uint32) {
	pi := ly.PoolIndex(NeuronIxs[ni, NrnSubPool])
	spk := Neurons[ni, di, Spike]
	geRaw := Neurons[ni, di, GeRaw]
	geExt := Neurons[ni, di, GeExt]
	PoolInhibRawIncrInt(pi, di, spk, geRaw, geExt)
	PoolAvgMaxUpdate(pi, di, ni)
	if PoolIxs[pi, PoolIsLayer] == 0 { // also update layer pool if I am a subpool
		lpi := ly.PoolIndex(0)
		PoolInhibRawIncrInt(lpi, di, spk, geRaw, geExt)
		PoolAvgMaxUpdate(lpi, di, ni)
	}
}

// LayerGi updates the layer-level Gi inhibition from spikes.
func (ly *LayerParams) LayerGi(ctx *Context, li, di uint32) {
	lpi := ly.PoolIndex(0)
	PoolAvgMaxCalc(lpi, di)
	PoolInhibIntToRaw(lpi, di)
	ly.LayPoolGiFromSpikes(ctx, lpi, di)
}

// BetweenGi computes inhibition Gi between layers.
func (ly *LayerParams) BetweenGi(ctx *Context, di uint32) {
	lpi := ly.PoolIndex(0)
	maxGi := Pools[lpi, di, fsfffb.TotalGi]
	maxGi = ly.BetweenLayerGiMax(di, maxGi, ly.LayInhib.Index1)
	maxGi = ly.BetweenLayerGiMax(di, maxGi, ly.LayInhib.Index2)
	maxGi = ly.BetweenLayerGiMax(di, maxGi, ly.LayInhib.Index3)
	maxGi = ly.BetweenLayerGiMax(di, maxGi, ly.LayInhib.Index4)
	Pools[lpi, di, fsfffb.TotalGi] = maxGi // our inhib is max of us and everyone in the layer pool
}

// BetweenLayerGiMax returns max gi value for input maxGi vs
// the given layIndex layer
func (ly *LayerParams) BetweenLayerGiMax(di uint32, maxGi float32, layIndex int32) float32 {
	if layIndex < 0 {
		return maxGi
	}
	oly := GetLayers(uint32(layIndex))
	opi := oly.PoolIndex(0)
	ogi := Pools[opi, di, fsfffb.TotalGi]
	if ogi > maxGi {
		return ogi
	}
	return maxGi
}

// LayPoolGiFromSpikes computes inhibition Gi from Spikes for layer-level pool.
func (ly *LayerParams) LayPoolGiFromSpikes(ctx *Context, lpi, di uint32) {
	PoolInhibSpikesFromRaw(lpi, di)
	PoolInhib(&ly.Inhib.Layer, lpi, di, LayerStates[ly.Index, di, LayerGiMult])
}

// SubPoolGiFromSpikes computes inhibition Gi from Spikes within a sub-pool
// pl is guaranteed not to be the overall layer pool
func (ly *LayerParams) SubPoolGiFromSpikes(ctx *Context, lpi, pi, di uint32, lyInhib bool, giMult float32) {
	PoolInhibSpikesFromRaw(pi, di)
	PoolInhib(&ly.Inhib.Pool, pi, di, giMult)
	if lyInhib {
		PoolInhibLayerMax(pi, di, Pools[lpi, di, fsfffb.TotalGi]) // note: this requires lpl inhib to have been computed before!
	} else {
		PoolInhibPoolMax(pi, di, Pools[pi, di, fsfffb.TotalGi]) // display only
		Pools[lpi, di, fsfffb.GiOrig] = Pools[lpi, di, fsfffb.TotalGi]
	}
}

////////  CycleNeuron methods

// CycleNeuron does one cycle (msec) of updating at the neuron level
// Called directly by Network, iterates over data.
func (ly *LayerParams) CycleNeuron(ctx *Context, ni, di uint32) {
	pi := ly.PoolIndex(NeuronIxs[ni, NrnSubPool])
	lpi := ly.PoolIndex(0)
	ly.GInteg(ctx, pi, ni, di)
	ly.SpikeFromG(ctx, lpi, ni, di)
}

// GInteg integrates conductances G over time (Ge, NMDA, etc).
// calls SpecialGFromRawSyn, GiInteg
func (ly *LayerParams) GInteg(ctx *Context, pi, ni, di uint32) {
	drvGe := float32(0)
	nonDrivePct := float32(0)
	if ly.Type == PulvinarLayer {
		ly.PulvinarDriver(ctx, ni-ly.Indexes.NeurSt, di, &drvGe, &nonDrivePct)
		Neurons[ni, di, Ext] = nonDrivePct // use for regulating inhibition
	}
	saveVal := ly.SpecialPreGs(ctx, pi, ni, di, drvGe, nonDrivePct)

	ly.GFromRawSyn(ctx, ni, di)
	ly.GiInteg(ctx, pi, ni, di)
	ly.SpecialPostGs(ctx, ni, di, saveVal)
}

/////////  GInteg

// SpecialPreGs is used for special layer types to do things to the
// conductance values prior to doing the standard updates in GFromRawSyn
// drvAct is for Pulvinar layers, activation of driving neuron
func (ly *LayerParams) SpecialPreGs(ctx *Context, pi, ni, di uint32, drvGe float32, nonDrivePct float32) float32 {
	saveVal := float32(0)               // sometimes we need to use a value computed here, for the post Gs step
	pil := pi - ly.PoolSt
	pnn := uint32(PoolNNeurons(pi))
	pni := NeuronIxs[ni, NrnNeurIndex] - uint32(PoolIxs[pi, PoolNeurSt])
	nrnCtxtGe := Neurons[ni, di, CtxtGe]
	nrnGeRaw := Neurons[ni, di, GeRaw]
	hasRew := GlobalScalars[GvHasRew, di] > 0
	switch ly.Type {
	case PTPredLayer, CTLayer:
		geCtxt := ly.CT.GeGain * nrnCtxtGe
		Neurons[ni, di, GeRaw] += geCtxt
		if ly.CT.DecayDt > 0 {
			Neurons[ni, di, CtxtGe] -= ly.CT.DecayDt * nrnCtxtGe
		}
		ctxExt := ly.Acts.Dt.GeSynFromRawSteady(geCtxt)
		Neurons[ni, di, GeSyn] += ctxExt
		saveVal = ctxExt // used In PostGs to set nrn.GeExt
	case PTMaintLayer:
		if ly.Acts.SMaint.On.IsTrue() {
			saveVal = ly.Acts.SMaint.Inhib * Neurons[ni, di, GMaintRaw] // used In PostGs to set nrn.GeExt
		}
	case PulvinarLayer:
		if ctx.PlusPhase.IsFalse() {
			break
		}
		// geSyn, goes into nrn.GeExt in PostGs, so inhibition gets it
		saveVal = nonDrivePct*Neurons[ni, di, GeSyn] + ly.Acts.Dt.GeSynFromRawSteady(drvGe)
		Neurons[ni, di, GeRaw] = nonDrivePct*nrnGeRaw + drvGe
		Neurons[ni, di, GeSyn] = saveVal
	case VSGatedLayer:
		dr := float32(0)
		if pil == 0 {
			dr = GlobalScalars[GvVSMatrixJustGated, di]
		} else {
			dr = GlobalScalars[GvVSMatrixHasGated, di]
		}
		dr = math32.Abs(dr)
		Neurons[ni, di, GeRaw] = dr
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(dr)

	case BLALayer:
		if ly.Learn.NeuroMod.IsBLAExt() {
			md := max(-GlobalScalars[GvDA, di], float32(0)) // ext is modulated by negative da
			geCtxt := md * ly.CT.GeGain * Neurons[ni, di, CtxtGeOrig]
			Neurons[ni, di, GeRaw] += geCtxt
			ctxExt := ly.Acts.Dt.GeSynFromRawSteady(geCtxt)
			Neurons[ni, di, GeSyn] += ctxExt
			saveVal = ctxExt // used In PostGs to set nrn.GeExt
		}
	case LHbLayer:
		geRaw := float32(0)
		if ni == 0 {
			geRaw = 0.2 * math32.Abs(GlobalScalars[GvLHbDip, di])
		} else {
			geRaw = 0.2 * math32.Abs(GlobalScalars[GvLHbBurst, di])
		}
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case DrivesLayer:
		dr := GlobalVectors[GvDrives, pil-1, di]
		geRaw := dr
		if dr > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, pnn, dr)
		}
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case UrgencyLayer:
		ur := GlobalScalars[GvUrgency, di]
		geRaw := ur
		if ur > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, pnn, ur)
		}
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case USLayer:
		us := RubiconUSStimValue(di, pil-1, ly.Learn.NeuroMod.Valence)
		geRaw := us
		if us > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, pnn, us)
		}
		// D2Mod = final
		if ly.Learn.NeuroMod.DAMod == D1Mod || (ly.Learn.NeuroMod.DAMod == D2Mod && hasRew && ctx.PlusPhase.IsTrue()) {
			Neurons[ni, di, GeRaw] = geRaw
			Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
		}
	case PVLayer:
		if hasRew && ctx.PlusPhase.IsTrue() {
			pv := float32(0)
			if ly.Learn.NeuroMod.Valence == Positive {
				pv = GlobalScalars[GvPVpos, di]
			} else {
				pv = GlobalScalars[GvPVneg, di]
			}
			pc := ly.Acts.PopCode.EncodeGe(pni, ly.Indexes.NNeurons, pv)
			Neurons[ni, di, GeRaw] = pc
			Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(pc)
		}
	case LDTLayer:
		geRaw := 0.4 * GlobalScalars[GvACh, di]
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case VTALayer:
		geRaw := ly.RWDa.GeFromDA(GlobalScalars[GvVtaDA, di])
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)

	case RewLayer:
		NeuronSetFlag(NeuronHasExt, ni, di)
		SetNeuronExtPosNeg(ctx, ni, di, GlobalScalars[GvRew, di]) // Rew must be set in Context!
	case RWDaLayer:
		geRaw := ly.RWDa.GeFromDA(GlobalScalars[GvDA, di])
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case TDDaLayer:
		geRaw := ly.TDDa.GeFromDA(GlobalScalars[GvDA, di])
		Neurons[ni, di, GeRaw] = geRaw
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(geRaw)
	case TDIntegLayer:
		NeuronSetFlag(NeuronHasExt, ni, di)
		SetNeuronExtPosNeg(ctx, ni, di, GlobalScalars[GvRewPred, di])
	default:
	}
	return saveVal
}

// SpecialPostGs is used for special layer types to do things
// after the standard updates in GFromRawSyn.
// It is passed the saveVal from SpecialPreGs
func (ly *LayerParams) SpecialPostGs(ctx *Context, ni, di uint32, saveVal float32) {
	if ly.Type != DSMatrixLayer {
		ly.GNeuroMod(ctx, ni, di)
	}

	switch ly.Type {
	case PulvinarLayer, PTMaintLayer, CTLayer, BLALayer:
		Neurons[ni, di, GeExt] = saveVal
	case PTPredLayer:
		Neurons[ni, di, GeExt] = saveVal
		orig := Neurons[ni, di, CtxtGeOrig]
		if orig < 0.05 {
			Neurons[ni, di, Ge] = 0.0
		}
	case DSMatrixLayer:
		if GlobalScalars[GvHasRew, di] > 0 {
			ly.GNeuroMod(ctx, ni, di)
		} else {
			pi := ly.PoolIndex(NeuronIxs[ni, NrnSubPool])
			nda := ly.DSMatrix.PatchBurstGain*Pools[pi, di, fsfffb.DAD1] - Pools[pi, di, fsfffb.DAD2]
			ggain := 1.0 + ly.Learn.NeuroMod.DASign() * ly.DSMatrix.PatchDAModGain * nda
			Neurons[ni, di, Ge] *= ggain
			Neurons[ni, di, Gi] *= ggain
		}
	default:
	}
}

// GFromRawSyn computes overall Ge and GiSyn conductances for neuron
// from GeRaw and GeSyn values, including NMDA, VGCC, AMPA, and GABA-A channels.
// drvAct is for Pulvinar layers, activation of driving neuron
func (ly *LayerParams) GFromRawSyn(ctx *Context, ni, di uint32) {
	extraRaw := float32(0)
	extraSyn := float32(0)
	nrnGModRaw := Neurons[ni, di, GModRaw]
	nrnGModSyn := Neurons[ni, di, GModSyn]
	ach := GlobalScalars[GvACh, di]
	switch ly.Type {
	case PTMaintLayer:
		md := ly.Acts.Dend.ModGain * nrnGModSyn
		if ly.Acts.Dend.ModACh.IsTrue() {
			md *= ach
		}
		md += ly.Acts.Dend.ModBase
		// key: excluding GModMaint here, so active maintenance can persist
		Neurons[ni, di, GeRaw] *= md
		Neurons[ni, di, GeSyn] *= md
		extraRaw = ly.Acts.Dend.ModGain * nrnGModRaw
		if ly.Acts.Dend.ModACh.IsTrue() {
			extraRaw *= ach
		}
		extraSyn = md
	case BLALayer:
		// modulatory pathway from PTp is only used so we can modulate by da
		md := max(-GlobalScalars[GvDA, di], 0.0) // ext is modulated by negative da
		extraRaw = md * nrnGModRaw * ly.Acts.Dend.ModGain
		extraSyn = md * nrnGModSyn * ly.Acts.Dend.ModGain
	default:
		if ly.Acts.Dend.HasMod.IsTrue() {
			md := ly.Acts.Dend.ModBase + ly.Acts.Dend.ModGain * nrnGModSyn
			if md > 1 {
				md = 1
			}
			Neurons[ni, di, GeRaw] *= md
			Neurons[ni, di, GeSyn] *= md
		}
	}
	geRaw := Neurons[ni, di, GeRaw]
	geSyn := Neurons[ni, di, GeSyn]
	ly.Acts.NMDAFromRaw(ctx, ni, di, geRaw+extraRaw)
	ly.Acts.MaintNMDAFromRaw(ctx, ni, di) // uses GMaintRaw directly
	ly.Learn.LearnNMDAFromRaw(ctx, ni, di, geRaw)
	ly.Acts.GvgccFromVm(ctx, ni, di)
	ege := Neurons[ni, di, Gnmda] + Neurons[ni, di, GnmdaMaint] + Neurons[ni, di, Gvgcc] + extraSyn
	ly.Acts.GeFromSyn(ctx, ni, di, geSyn, ege) // sets nrn.GeExt too
	ly.Acts.GkFromVm(ctx, ni, di)
	ly.Acts.GSkCaFromCa(ctx, ni, di)
	Neurons[ni, di, GiSyn] = ly.Acts.GiFromSyn(ctx, ni, di, Neurons[ni, di, GiSyn])
}

// GiInteg adds Gi values from all sources including SubPool computed inhib
// and updates GABAB as well
func (ly *LayerParams) GiInteg(ctx *Context, pi, ni, di uint32) {
	giMult := LayerStates[ly.Index, di, LayerGiMult]
	gi := giMult*Pools[pi, di, fsfffb.TotalGi] + Neurons[ni, di, GiSyn] + Neurons[ni, di, GiNoise] + ly.Learn.NeuroMod.GiFromACh(GlobalScalars[GvACh, di])
	ssgi := Pools[pi, di, fsfffb.SSGi]
	Neurons[ni, di, Gi] = gi
	Neurons[ni, di, SSGiDend] = 0.0
	if ctx.PlusPhase.IsTrue() && (ly.Type == PulvinarLayer) {
		ext := Neurons[ni, di, Ext] // nonDrivePct
		Neurons[ni, di, SSGiDend] = ext * ly.Acts.Dend.SSGi * ssgi
	} else {
		if !ly.IsInputOrTarget() {
			Neurons[ni, di, SSGiDend] = ly.Acts.Dend.SSGi * ssgi
		}
	}
	vm := Neurons[ni, di, VmDend]
	nrnGababM := Neurons[ni, di, GababM]
	nrnGababX := Neurons[ni, di, GababX]
	ly.Acts.GabaB.MX(gi, &nrnGababM, &nrnGababX)
	Neurons[ni, di, GababM] = nrnGababM
	Neurons[ni, di, GababX] = nrnGababX
	nrnGgabaB := ly.Acts.GabaB.GgabaB(nrnGababM, vm)
	Neurons[ni, di, GgabaB] = nrnGgabaB
	// Gk was already init
	Neurons[ni, di, Gk] += nrnGgabaB
}

// GNeuroMod does neuromodulation of conductances
func (ly *LayerParams) GNeuroMod(ctx *Context, ni, di uint32) {
	ggain := ly.Learn.NeuroMod.GGain(GlobalScalars[GvDA, di] + GlobalScalars[GvDAtonic, di])
	Neurons[ni, di, Ge] *= ggain
	Neurons[ni, di, Gi] *= ggain
}

////////  SendSpike

// SpikeFromG computes Vm from Ge, Gi, Gl conductances and then Spike from that
func (ly *LayerParams) SpikeFromG(ctx *Context, lpi, ni, di uint32) {
	ly.Acts.VmFromG(ctx, ni, di)
	ly.Acts.SpikeFromVm(ctx, ni, di)
	ly.Learn.CaFromSpike(ctx, ni, di)
	if !ly.IsNuclear() {
		ly.Learn.GaMFromSpike(ctx, ni, di)
		if !ly.IsTarget() {
			learnNow := ly.Learn.Timing.LearnTiming(ctx, ni, di)
			if learnNow {
				da := GlobalScalars[GvDA, di]
				ach := GlobalScalars[GvACh, di]
				nrnCaD := Neurons[ni, di, CaD]
				mlr := ly.Learn.RLRate.RLRateSigDeriv(nrnCaD, PoolAvgMax(AMCaD, AMCycle, Max, lpi, di))
				modlr := ly.Learn.NeuroMod.LRMod(da, ach)
				dlr := ly.Learn.RLRate.RLRateDiff(Neurons[ni, di, CaP], nrnCaD)
				Neurons[ni, di, RLRate] = mlr * dlr * modlr
			}
		}
	}
	
	lmax := PoolAvgMax(AMGeInt, AMCycle, Max, lpi, di)
	if lmax > 0 {
		Neurons[ni, di, GeIntNorm] = Neurons[ni, di, GeInt] / lmax
	} else {
		Neurons[ni, di, GeIntNorm] = Neurons[ni, di, GeInt]
	}
	if ctx.MinusPhase.IsFalse() && ctx.PlusPhase.IsFalse() {
		return
	}
	lrnCyc := ctx.Cycle - ctx.ISICycles
	if lrnCyc >= ly.Acts.Dt.MaxCycStart {
		Neurons[ni, di, CaPMaxCa] += ly.Learn.CaSpike.Dt.PDt * (Neurons[ni, di, CaM] - Neurons[ni, di, CaPMaxCa])
		spkmax := Neurons[ni, di, CaPMaxCa]
		if spkmax > Neurons[ni, di, CaPMax] {
			Neurons[ni, di, CaPMax] = spkmax
		}
	}
	if ly.Type != IOLayer { // uses bins for itself
		CaBinIncrement(Neurons[ni, di, CaSyn], ctx.CyclesTotal, ni, di)
	}
}

// SendSpike sends spike to receivers for all neurons that spiked
// last step in Cycle, integrated the next time around.
// Called directly by Network, iterates over data.
func (ly *LayerParams) SendSpike(ctx *Context, ni, di uint32) {
	pi := ly.PoolIndex(NeuronIxs[ni, NrnSubPool])
	lpi := ly.PoolIndex(0)
	lni := ni - ly.Indexes.NeurSt
	ly.PostSpike(ctx, lpi, pi, ni, di)

	for pti := uint32(0); pti < ly.Indexes.SendN; pti++ {
		pt := GetPaths(ly.Indexes.SendSt + pti)
		pt.SendSpike(ctx, ni, di, lni)
	}
}

// PostSpikeSpecial does updates at neuron level after spiking has been computed.
// This is where special layer types add extra code.
func (ly *LayerParams) PostSpikeSpecial(ctx *Context, lpi, pi, ni, di uint32) {
	Neurons[ni, di, Burst] = Neurons[ni, di, CaP]
	li := ly.Index
	pil := pi - ly.PoolSt // 0-n pool index
	pnn := uint32(PoolNNeurons(pi))
	pni := NeuronIxs[ni, NrnNeurIndex] - uint32(PoolIxs[pi, PoolNeurSt])
	hasRew := GlobalScalars[GvHasRew, di] > 0
	switch ly.Type {
	case SuperLayer:
		if ctx.PlusPhase.IsTrue() {
			actMax := PoolAvgMax(AMCaP, AMCycle, Max, lpi, di)
			actAvg := PoolAvgMax(AMCaP, AMCycle, Avg, lpi, di)
			thr := ly.Bursts.ThrFromAvgMax(actAvg, actMax)
			if Neurons[ni, di, CaP] < thr {
				Neurons[ni, di, Burst] = 0.0
			}
		}
	case PTPredLayer, CTLayer:
		if ctx.Cycle == ctx.ThetaCycles-1 {
			if ly.CT.DecayTau == 0 {
				Neurons[ni, di, CtxtGe] = Neurons[ni, di, CtxtGeRaw]
			} else {
				Neurons[ni, di, CtxtGe] += Neurons[ni, di, CtxtGeRaw]
			}
			Neurons[ni, di, CtxtGeOrig] = Neurons[ni, di, CtxtGe]
		}
	case VSGatedLayer:
		dr := float32(0)
		if pil == 0 {
			dr = GlobalScalars[GvVSMatrixJustGated, di]
		} else {
			dr = GlobalScalars[GvVSMatrixHasGated, di]
		}
		Neurons[ni, di, Act] = dr

	case IOLayer:
		ly.IOLearn(ctx, lpi, pi, ni, di)
	case CNeLayer:
		ly.CNeLearn(ctx, lpi, pi, ni, di)
	case CNiIOLayer, CNiUpLayer:
		ly.CNiLearn(ctx, lpi, pi, ni, di)
	case BLALayer:
		if ctx.Cycle == ctx.ThetaCycles-1 {
			if hasRew {
				Neurons[ni, di, CtxtGe] = 0.0
				Neurons[ni, di, CtxtGeOrig] = 0.0
			} else if GlobalScalars[GvACh, di] > 0.1 {
				Neurons[ni, di, CtxtGe] = Neurons[ni, di, CtxtGeRaw]
				Neurons[ni, di, CtxtGeOrig] = Neurons[ni, di, CtxtGe]
			}
		}
	case LHbLayer:
		if pni == 0 {
			Neurons[ni, di, Act] = GlobalScalars[GvLHbDip, di]
		} else {
			Neurons[ni, di, Act] = GlobalScalars[GvLHbBurst, di]
		}
		Neurons[ni, di, GeSyn] = ly.Acts.Dt.GeSynFromRawSteady(Neurons[ni, di, GeRaw])
	case DrivesLayer:
		dr := GlobalVectors[GvDrives, pil-1, di]
		act := dr
		if dr > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, pnn, dr)
		}
		Neurons[ni, di, Act] = act
	case UrgencyLayer:
		ur := GlobalScalars[GvUrgency, di]
		act := ur
		if ur > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, pnn, ur)
		}
		Neurons[ni, di, Act] = act
	case USLayer:
		us := RubiconUSStimValue(di, pil-1, ly.Learn.NeuroMod.Valence)
		act := us
		if us > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, pnn, us)
		}
		// D2Mod = final
		if ly.Learn.NeuroMod.DAMod == D1Mod || (ly.Learn.NeuroMod.DAMod == D2Mod && hasRew && ctx.PlusPhase.IsTrue()) {
			Neurons[ni, di, Act] = act
		}
	case PVLayer:
		if hasRew {
			pv := float32(0)
			if ly.Learn.NeuroMod.Valence == Positive {
				pv = GlobalScalars[GvPVpos, di]
			} else {
				pv = GlobalScalars[GvPVneg, di]
			}
			act := ly.Acts.PopCode.EncodeValue(pni, ly.Indexes.NNeurons, pv)
			Neurons[ni, di, Act] = act
		}
	case LDTLayer:
		// I set this in CyclePost
		Neurons[ni, di, Act] = GlobalScalars[GvAChRaw, di]
	case VTALayer:
		// I set this in CyclePost
		Neurons[ni, di, Act] = GlobalScalars[GvVtaDA, di]

	case RewLayer:
		Neurons[ni, di, Act] = GlobalScalars[GvRew, di]
	case RWPredLayer:
		// clipped linear
		Neurons[ni, di, Act] = ly.RWPred.PredRange.ClampValue(Neurons[ni, di, Ge])
		if pni == 0 {
			LayerStates[li, di, LayerRewPredPos] = Neurons[ni, di, ActInt] 
		} else {
			LayerStates[li, di, LayerRewPredNeg] = Neurons[ni, di, ActInt]
		}
	case RWDaLayer:
		// I set this in CyclePost
		Neurons[ni, di, Act] = GlobalScalars[GvDA, di]
	case TDPredLayer:
		// linear
		Neurons[ni, di, Act] = Neurons[ni, di, Ge]
		if pni == 0 {
			LayerStates[li, di, LayerRewPredPos] = Neurons[ni, di, ActInt]
		} else {
			LayerStates[li, di, LayerRewPredNeg] = Neurons[ni, di, ActInt]
		}
	case TDIntegLayer:
		Neurons[ni, di, Act] = GlobalScalars[GvRewPred, di]
	case TDDaLayer:
		// I set this in CyclePost
		Neurons[ni, di, Act] = GlobalScalars[GvDA, di]
	default:
	}
}

// PostSpike does updates at neuron level after spiking has been computed.
// It calls PostSpikeSpecial.  It also updates the CaPCyc stats.
func (ly *LayerParams) PostSpike(ctx *Context, lpi, pi, ni, di uint32) {
	ly.PostSpikeSpecial(ctx, lpi, pi, ni, di)
	intdt := ly.Acts.Dt.IntDt
	Neurons[ni, di, GeInt] += intdt * (Neurons[ni, di, Ge] - Neurons[ni, di, GeInt])
	Neurons[ni, di, GiInt] += intdt * (Neurons[ni, di, GiSyn] - Neurons[ni, di, GiInt])
	// act int is reset at start of the plus phase -- needs faster integration:
	if ctx.PlusPhase.IsTrue() {
		intdt *= 3.0
	}
	// using reg act here now
	Neurons[ni, di, ActInt] += intdt * (Neurons[ni, di, Act] - Neurons[ni, di, ActInt])
}

// CyclePost is called after the standard Cycle update, as a separate
// network layer loop.
// This is reserved for any kind of special ad-hoc types that
// need to do something special after Spiking is finally computed and Sent.
// Typically used for updating global values in the Context state,
// such as updating a neuromodulatory signal such as dopamine.
// Any updates here must also be done in gpu_wgsl/gpu_cyclepost.wgsl
func (ly *LayerParams) CyclePost(ctx *Context, di uint32) {
	lpi := ly.PoolIndex(0)
	ly.CyclePostLayer(ctx, lpi, di)
	switch ly.Type {
	case VSMatrixLayer, BGThalLayer:
		ly.GatedFromCaPMax(ctx, di)
	case DSMatrixLayer:
		ly.GatedFromCaPMax(ctx, di)
		for spi := uint32(1); spi < ly.Indexes.NPools; spi++ {
			pi := ly.PoolIndex(spi)
			ly.CyclePostDSMatrixLayer(ctx, pi, di, int32(spi))
		}
	case CeMLayer:
		ly.CyclePostCeMLayer(ctx, lpi, di)
	case VSPatchLayer:
		for spi := uint32(1); spi < ly.Indexes.NPools; spi++ {
			pi := ly.PoolIndex(spi)
			ly.CyclePostVSPatchLayer(ctx, pi, di, int32(spi))
		}
	case DSPatchLayer:
		for spi := uint32(1); spi < ly.Indexes.NPools; spi++ {
			pi := ly.PoolIndex(spi)
			ly.CyclePostDSPatchLayer(ctx, pi, di, int32(spi))
		}
	case LDTLayer:
		srcLay1Act := ly.LDTSrcLayAct(ly.LDT.SrcLay1Index, di)
		srcLay2Act := ly.LDTSrcLayAct(ly.LDT.SrcLay2Index, di)
		srcLay3Act := ly.LDTSrcLayAct(ly.LDT.SrcLay3Index, di)
		srcLay4Act := ly.LDTSrcLayAct(ly.LDT.SrcLay4Index, di)
		ly.CyclePostLDTLayer(ctx, di, srcLay1Act, srcLay2Act, srcLay3Act, srcLay4Act)
	case VTALayer:
		ly.CyclePostVTALayer(ctx, di)
	case RWDaLayer:
		ly.CyclePostRWDaLayer(ctx, di)
	case TDPredLayer:
		ly.CyclePostTDPredLayer(ctx, di)
	case TDIntegLayer:
		ly.CyclePostTDIntegLayer(ctx, di)
	case TDDaLayer:
		ly.CyclePostTDDaLayer(ctx, di)
	default:
	}
}

////////  Special CyclePost methods for different layer types

// CyclePostLayer is called for all layer types
func (ly *LayerParams) CyclePostLayer(ctx *Context, lpi, di uint32) {
	casp := PoolAvgMax(AMCaP, AMCycle, Max, lpi, di)
	if ctx.Cycle >= ly.Acts.Dt.MaxCycStart {
		if casp > ly.Inhib.ActAvg.RTThr && LayerStates[ly.Index, di, LayerRT] <= 0 {
			LayerStates[ly.Index, di, LayerRT] = float32(ctx.Cycle)
		}
		if PoolsInt[lpi, di, PoolGated] > 0 && LayerStates[ly.Index, di, GatedRT] <= 0 {
			LayerStates[ly.Index, di, GatedRT] = float32(ctx.Cycle)
		}
	}
}

// LDTSrcLayAct returns the overall activity level for given source layer
// for purposes of computing ACh salience value.
// Typically the input is a superior colliculus (SC) layer that rapidly
// accommodates after the onset of a stimulus.
// using lpl.AvgMax.CaP.Cycle.Max for layer activity measure.
func (ly *LayerParams) LDTSrcLayAct(layIndex int32, di uint32) float32 {
	if layIndex < 0 {
		return 0
	}
	oly := GetLayers(uint32(layIndex))
	opi := oly.PoolIndex(0)
	return PoolAvgMax(AMCaP, AMCycle, Avg, opi, di)
}


func (ly *LayerParams) CyclePostLDTLayer(ctx *Context, di uint32, srcLay1Act, srcLay2Act, srcLay3Act, srcLay4Act float32) {
	ach := ly.LDT.ACh(ctx, di, srcLay1Act, srcLay2Act, srcLay3Act, srcLay4Act)

	GlobalScalars[GvAChRaw, di] = ach
	if ach > GlobalScalars[GvACh, di] { // instant up
		GlobalScalars[GvACh, di] = ach
	} else {
		GlobalScalars[GvACh, di] += ly.Acts.Dt.IntDt * (ach - GlobalScalars[GvACh, di])
	}
}

func (ly *LayerParams) CyclePostRWDaLayer(ctx *Context, di uint32) {
	pli := uint32(ly.RWDa.RWPredLayIndex)
	pred := LayerStates[pli, di, LayerRewPredPos] - LayerStates[pli, di, LayerRewPredNeg]
	GlobalScalars[GvRewPred, di] = pred // record
	da := float32(0)
	if GlobalScalars[GvHasRew, di] > 0 {
		da = GlobalScalars[GvRew, di] - pred
	}
	GlobalScalars[GvDA, di] = da // updates global value that will be copied to layers next cycle.
}

func (ly *LayerParams) CyclePostTDPredLayer(ctx *Context, di uint32) {
	if ctx.PlusPhase.IsFalse() {
		return
	}
	pred := LayerStates[ly.Index, di, LayerRewPredPos] - LayerStates[ly.Index, di, LayerRewPredNeg]
	GlobalScalars[GvPrevPred, di] = pred
}

func (ly *LayerParams) CyclePostTDIntegLayer(ctx *Context, di uint32) {
	rew := float32(0)
	if GlobalScalars[GvHasRew, di] > 0 {
		rew = GlobalScalars[GvRew, di]
	}
	rpval := float32(0)
	if ctx.PlusPhase.IsTrue() {
		pli := uint32(ly.TDInteg.TDPredLayIndex)
		pred := LayerStates[pli, di, LayerRewPredPos] - LayerStates[pli, di, LayerRewPredNeg]
		rpval = rew + ly.TDInteg.Discount*ly.TDInteg.PredGain*pred
		LayerStates[ly.Index, di, LayerRewPredPos] = rpval // our plus phase = new integrated value
	} else {
		rpval = ly.TDInteg.PredGain * GlobalScalars[GvPrevPred, di]
		LayerStates[ly.Index, di, LayerRewPredNeg] = rpval // our minus phase = prior integrated value
	}
	GlobalScalars[GvRewPred, di] = rpval // global value will be copied to layers next cycle
}

func (ly *LayerParams) CyclePostTDDaLayer(ctx *Context, di uint32) {
	ili := uint32(ly.TDDa.TDIntegLayIndex)
	da := LayerStates[ili, di, LayerRewPredPos] - LayerStates[ili, di, LayerRewPredNeg]
	if ctx.PlusPhase.IsFalse() {
		da = 0
	}
	GlobalScalars[GvDA, di] = da // updates global value that will be copied to layers next cycle.
}

func (ly *LayerParams) CyclePostCeMLayer(ctx *Context, lpi, di uint32) {
	casd := PoolAvgMax(AMCaD, AMCycle, Max, lpi, di)	
	if ly.Learn.NeuroMod.Valence == Positive {
		GlobalScalars[GvCeMpos, di] = casd
	} else {
		GlobalScalars[GvCeMneg, di] = casd
	}
}

func (ly *LayerParams) CyclePostVTALayer(ctx *Context, di uint32) {
	ly.VTA.VTADA(ctx, di, GlobalScalars[GvACh, di], (GlobalScalars[GvHasRew, di] > 0))
}

// note: needs to iterate over sub-pools in layer!
func (ly *LayerParams) CyclePostVSPatchLayer(ctx *Context, pi, di uint32, spi int32) {
	casd := PoolAvgMax(AMCaD, AMCycle, Avg, pi, di)
	if ly.Learn.NeuroMod.DAMod == D1Mod {
		GlobalVectors[GvVSPatchD1, uint32(spi-1), di] = casd
	} else {
		GlobalVectors[GvVSPatchD2, uint32(spi-1), di] = casd
	}
}

////////  Phase timescale

// DecayStateNeuronsAll decays neural activation state by given proportion
// (default decay values are ly.Params.Acts.Decay.Act, Glong, AHP)
// for all data parallel indexes. Does not decay pool or layer state.
// This is used for minus phase of Pulvinar layers to clear state in prep
// for driver plus phase.
func (ly *LayerParams) DecayStateNeuronsAll(ctx *Context, decay, glong, ahp float32) {
	nn := ly.Indexes.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.Indexes.NeurSt + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			ly.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		}
	}
}

// NewStateLayer does NewState at the layer level, called 
func (ly *LayerParams) NewStateLayer(ctx *Context) {
	actMinusAvg := float32(0)
	actPlusAvg := float32(0)
	np := uint32(ly.Indexes.NPools)
	
	for di := uint32(0); di < ctx.NData; di++ {
		lpi := ly.PoolIndex(0)

		actMinusAvg += PoolAvgMax(AMAct, AMMinus, Avg, lpi, di)
		actPlusAvg += PoolAvgMax(AMAct, AMPlus, Avg, lpi, di)

		LayerStates[ly.Index, di, LayerRT] = -1.0
		LayerStates[ly.Index, di, GatedRT] = -1.0

		for spi := uint32(0); spi < np; spi++ {
			pi := ly.PoolIndex(spi)
			ly.NewStatePool(ctx, pi, di) // also calls DecayState on pool
		}
	}

	// note: long-running averages must be based on aggregate data, drive adaptation
	// of Gi layer inhibition.
	davg := 1 / float32(ctx.NData)
	actMinusAvg *= davg
	actPlusAvg *= davg
	for di := uint32(0); di < ctx.NData; di++ {
		ly.NewStateLayerActAvg(ctx, di, actMinusAvg, actPlusAvg)
	}
}

// NewStateLayerActAvg updates ActAvg.ActMAvg and ActPAvg based on current values
// that have been averaged across NData already.
func (ly *LayerParams) NewStateLayerActAvg(ctx *Context, di uint32, actMinusAvg, actPlusAvg float32) {
	mavg := LayerStates[ly.Index, di, LayerActMAvg]
	pavg := LayerStates[ly.Index, di, LayerActPAvg]
	ly.Inhib.ActAvg.AvgFromAct(&mavg, actMinusAvg, ly.Acts.Dt.LongAvgDt)
	ly.Inhib.ActAvg.AvgFromAct(&pavg, actPlusAvg, ly.Acts.Dt.LongAvgDt)
	LayerStates[ly.Index, di, LayerActMAvg] = mavg
	LayerStates[ly.Index, di, LayerActPAvg] = pavg
}

func (ly *LayerParams) NewStatePool(ctx *Context, pi, di uint32) {
	PoolsInt[pi, di, Clamped] = 0
	if ly.Acts.Clamp.Add.IsFalse() && ly.IsInput() {
		PoolsInt[pi, di, Clamped] = 1
	}
	PoolInhibDecay(pi, di, ly.Acts.Decay.Act)
	PoolsInt[pi, di, PoolGated] = 0
}

// NewStateNeuron handles all initialization at start of new input pattern.
// Should already have presented the external input to the network at this point.
func (ly *LayerParams) NewStateNeuron(ctx *Context, ni, di uint32) {
	Neurons[ni, di, BurstPrv] = Neurons[ni, di, Burst]
	Neurons[ni, di, CaDPrev] = Neurons[ni, di, CaD]
	Neurons[ni, di, CaPMax] = 0.0
	Neurons[ni, di, CaPMaxCa] = 0.0
	ly.Acts.DecayState(ctx, ni, di, ly.Acts.Decay.Act, ly.Acts.Decay.Glong, ly.Acts.Decay.AHP)
	// Note: synapse-level Ca decay happens in DWt
	ly.Acts.KNaNewState(ctx, ni, di)
	if ly.IsNuclear() {
		ly.NuclearLearnReset(ctx, ni, di)
	}
}

// Beta1Neuron does neuron level Beta1 updating.
func (ly *LayerParams) Beta1Neuron(ctx *Context, ni, di uint32) {
	Neurons[ni, di, Beta1] = Neurons[ni, di, CaP]
}

// Beta2Neuron does neuron level Beta2 updating.
func (ly *LayerParams) Beta2Neuron(ctx *Context, ni, di uint32) {
	Neurons[ni, di, Beta2] = Neurons[ni, di, CaP]
}

////////  Minus Phase

func (ly *LayerParams) MinusPhasePool(ctx *Context, pi uint32) {
	for di := uint32(0); di < ctx.NData; di++ {
		PoolCycleToMinus(pi, di)
		if ly.Acts.Clamp.Add.IsFalse() && ly.IsTarget() {
			PoolsInt[pi, di, Clamped] = 1
		}
	}
	if PoolIxs[pi, PoolIsLayer] == 0 {
		return
	}
	geIntMinusMax := float32(0)
	giIntMinusMax := float32(0)
	for di := uint32(0); di < ctx.NData; di++ {
		geIntMinusMax = math32.Max(geIntMinusMax, PoolAvgMax(AMGeInt, AMMinus, Max, pi, di))
		giIntMinusMax = math32.Max(giIntMinusMax, PoolAvgMax(AMGiInt, AMMinus, Max, pi, di))
	}
	for di := uint32(0); di < ctx.NData; di++ {
		ly.AvgGeM(ctx, di, geIntMinusMax, giIntMinusMax)
	}
}	
	
// AvgGeM computes the average and max GeInt, GiInt in minus phase
// (AvgMaxGeM, AvgMaxGiM) stats, updated in MinusPhase,
// using values that already max across NData.
func (ly *LayerParams) AvgGeM(ctx *Context, di uint32, geIntMinusMax, giIntMinusMax float32) {
	gem := LayerStates[ly.Index, di, LayerAvgMaxGeM]
	gim := LayerStates[ly.Index, di, LayerAvgMaxGiM]
	gem += ly.Acts.Dt.LongAvgDt * (geIntMinusMax - gem)
	gim += ly.Acts.Dt.LongAvgDt * (giIntMinusMax - gim)
	LayerStates[ly.Index, di, LayerAvgMaxGeM] = gem
	LayerStates[ly.Index, di, LayerAvgMaxGiM] = gim
}

// MinusPhaseNeuron does neuron level minus-phase updating
func (ly *LayerParams) MinusPhaseNeuron(ctx *Context, ni, di uint32) {
	Neurons[ni, di, ActM] = Neurons[ni, di, ActInt]
}

// MinusPhasePost does special algorithm processing at end of minus
func (ly *LayerParams) MinusPhasePost(ctx *Context) {
	switch ly.Type {
	case VSMatrixLayer, DSMatrixLayer:
		ly.MatrixGated(ctx) // need gated state for decisions about action processing, so do in minus too
	case PulvinarLayer:
		ly.DecayStateNeuronsAll(ctx, 1, 1, 0)
	default:
	}
}

// PlusPhaseStartNeuron does neuron level plus-phase start:
// applies Target inputs as External inputs.
func (ly *LayerParams) PlusPhaseStartNeuron(ctx *Context, ni, di uint32) {
	if NeuronHasFlag(NeuronHasTarg, ni, di) { // will be clamped in plus phase
		Neurons[ni, di, Ext] = Neurons[ni, di, Target]
		NeuronSetFlag(NeuronHasExt, ni, di)
		// get fresh update on plus phase output acts
		Neurons[ni, di, ISI] = -1.0
		Neurons[ni, di, ISIAvg] = -1.0
		// reset for plus phase
		Neurons[ni, di, ActInt] = ly.Acts.Init.Act
	}
}

func (ly *LayerParams) PlusPhaseEndPool(ctx *Context, pi, di uint32) {
	PoolCycleToPlus(pi, di)
}

// PlusPhaseEndNeuron does neuron level plus-phase end updating.
func (ly *LayerParams) PlusPhaseEndNeuron(ctx *Context, ni, di uint32) {
	pi := ly.PoolIndex(NeuronIxs[ni, NrnSubPool])
	lpi := ly.PoolIndex(0)
	Neurons[ni, di, ActP] = Neurons[ni, di, ActInt]
	nrnCaP := Neurons[ni, di, CaP]
	nrnCaD := Neurons[ni, di, CaD]
	ly.Learn.CaLearn.ETrace(ctx, ni, di, nrnCaD)
	
	da := GlobalScalars[GvDA, di]
	ach := GlobalScalars[GvACh, di]
	mlr := ly.Learn.RLRate.RLRateSigDeriv(nrnCaD, PoolAvgMax(AMCaD, AMCycle, Max, lpi, di))
	modlr := ly.Learn.NeuroMod.LRMod(da, ach)
	dlr := float32(1)
	hasRew := (GlobalScalars[GvHasRew, di]) > 0
	setRLRate := true

	switch ly.Type {
	case DSPatchLayer:
		if hasRew { // reward time
			mlr = 1 // don't use sig deriv
		} else {
			modlr = 1 // don't use mod
		}
	case VSPatchLayer:
		da = GlobalScalars[GvVSPatchPosRPE, di] // our own personal
		modlr = ly.Learn.NeuroMod.LRMod(da, ach)
		mlr = ly.Learn.RLRate.RLRateSigDeriv(Neurons[ni, di, CaDPrev], 1) // note: don't have proper max here
	case VSMatrixLayer, DSMatrixLayer:
		// note: modlr is further modulated by PF in PostPlus
		if hasRew { // reward time
			mlr = 1 // don't use sig deriv
		} else {
			modlr = 1 // don't use mod
		}
	case BLALayer:
		dlr = ly.Learn.RLRate.RLRateDiff(nrnCaP, Neurons[ni, di, CaDPrev]) // delta on previous trial
		if !ly.Learn.NeuroMod.IsBLAExt() && PoolIxs[pi, PoolNeurSt] == 0 { // first pool
			dlr = 0 // first pool is novelty / curiosity -- no learn
		}
	default:
		dlr = ly.Learn.RLRate.RLRateDiff(nrnCaP, nrnCaD)
		if !ly.IsTarget() {
			setRLRate = ly.Learn.Timing.On.IsFalse() // else computed at time of learning
		}
	}
	if setRLRate {
		Neurons[ni, di, RLRate] = mlr * dlr * modlr
	}
	var tau float32
	sahpN := Neurons[ni, di, SahpN]
	nrnSaphCa := Neurons[ni, di, SahpCa]
	ly.Acts.Sahp.NinfTauFromCa(nrnSaphCa, &sahpN, &tau)
	nrnSaphCa = ly.Acts.Sahp.CaInt(nrnSaphCa, nrnCaD)
	Neurons[ni, di, SahpN] = sahpN
	Neurons[ni, di, SahpCa] = nrnSaphCa
	Neurons[ni, di, Gsahp] = ly.Acts.Sahp.GsAHP(sahpN)
}

// PlusPhaseEndPost does special algorithm processing at end of plus.
func (ly *LayerParams) PlusPhaseEndPost(ctx *Context) {
	ly.PlusPhaseEndActAvg(ctx)
	ly.PhaseDiffFromActs(ctx) // GPU syncs down the state before this
	np := ly.Indexes.NPools
	if ly.Type == PTMaintLayer && ly.CT.OFCposPT.IsTrue() {
		for spi := uint32(1); spi < np; spi++ {
			for di := uint32(0); di < ctx.NData; di++ {
				pi := ly.PoolIndex(spi)
				val := PoolAvgMax(AMCaD, AMCycle, Avg, pi, di)
				GlobalVectors[GvOFCposPTMaint, uint32(spi-1), di] = val
			}
		}
	}

	if ly.Acts.Decay.OnRew.IsTrue() {
		for di := uint32(0); di < ctx.NData; di++ {
			hasRew := (GlobalScalars[GvHasRew, di] > 0)
			giveUp := (GlobalScalars[GvGiveUp, di] > 0)
			if hasRew || giveUp {
				ly.DecayState(ctx, di, 1, 1, 1)
				for spi := uint32(0); spi < np; spi++ {
					pi := ly.PoolIndex(spi)
					PoolAvgMaxZero(pi, di)
				}
			}
		}
	}
	if ly.Type == VSMatrixLayer || ly.Type == DSMatrixLayer {
		ly.MatrixGated(ctx) 
	}
}

// PlusPhaseEndActAvg updates ActAvg and DTrgAvg at the plus phase end.
// Note: could be done on GPU but not worth it at this point..
func (ly *LayerParams) PlusPhaseEndActAvg(ctx *Context) {
	nn := ly.Indexes.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.Indexes.NeurSt + lni
		if NeuronIsOff(ni) {
			continue
		}
		dTrgSum := float32(0)
		avgSum := float32(0)
		for di := uint32(0); di < ctx.NData; di++ {
			dTrgSum += ly.LearnTrgAvgErrLRate() * (Neurons[ni, di, CaP] - Neurons[ni, di, CaD])
			avgSum += ly.Acts.Dt.LongAvgDt * (Neurons[ni, di, ActM] - NeuronAvgs[ni, ActAvg])
		}
		NeuronAvgs[ni, DTrgAvg] += dTrgSum
		NeuronAvgs[ni, ActAvg] += avgSum
	}
}

//gosl:end

////////  Apply Ext

// InitExt initializes external input state.
// Should be called prior to ApplyExt on all layers receiving Ext input.
func (ly *Layer) InitExt() {
	if !ly.Type.IsExt() {
		return
	}
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.InitExt(ni, di)
			Exts[ly.Params.Indexes.ExtsSt + lni, di] = -1 // missing by default
		}
	}
}

// ApplyExtAll applies external input in the form of a tensor.Float32 or 64.
// Negative values and NaNs are not valid, and will be interpreted as missing inputs.
// This version applies all NData data parallel inputs at once, with outer dimension
// equal to NData.
// If dimensionality of tensor matches that of layer, and is 2D or 4D,
// then each dimension is iterated separately, so any mismatch preserves
// dimensional structure.
// Otherwise, the flat 1D view of the tensor is used.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext.
// Also sets the Exts values on layer, which are used for the GPU version,
// which requires calling the network ApplyExts() method -- is a no-op for CPU.
func (ly *Layer) ApplyExtAll(ctx *Context, ext tensor.Values) {
	gpu.VectorizeFunc(0, int(ctx.NData), func(idx uint32) {
		ed := ext.SubSpace(int(idx))
		ly.ApplyExt(idx, ed)
	})
}

// ApplyExt applies external input in the form of an tensor.Float32 or 64.
// Negative values and NaNs are not valid, and will be interpreted as missing inputs.
// The given data index di is the data parallel index (0 < di < MaxData):
// must present inputs separately for each separate data parallel set.
// If dimensionality of tensor matches that of layer, and is 2D or 4D,
// then each dimension is iterated separately, so any mismatch preserves
// dimensional structure.
// Otherwise, the flat 1D view of the tensor is used.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext.
// Also sets the Exts values on layer, which are used for the GPU version,
// which requires calling the network ApplyExts() method -- is a no-op for CPU.
func (ly *Layer) ApplyExt(di uint32, ext tensor.Tensor) {
	switch {
	case ext.NumDims() == 2 && ly.Shape.NumDims() == 4: // special case
		ly.ApplyExt2Dto4D(di, ext)
	case ext.NumDims() != ly.Shape.NumDims() || !(ext.NumDims() == 2 || ext.NumDims() == 4):
		ly.ApplyExt1DTsr(di, ext)
	case ext.NumDims() == 2:
		ly.ApplyExt2D(di, ext)
	case ext.NumDims() == 4:
		ly.ApplyExt4D(di, ext)
	}
}

// ApplyExtVal applies given external value to given neuron
// using clearMask, setMask, and toTarg from ApplyExtFlags.
// Also saves Val in Exts for potential use by GPU.
func (ly *Layer) ApplyExtValue(lni, di uint32, val float32, clearMask, setMask NeuronFlags, toTarg bool) {
	ni := ly.NeurStIndex + lni
	if NeuronIsOff(ni) {
		return
	}
	Exts[ly.Params.Indexes.ExtsSt + lni, di] = val
	if val < 0 {
		return
	}
	if toTarg {
		Neurons[ni, di, Target] = val
	} else {
		Neurons[ni, di, Ext] = val
	}
	NeuronClearFlag(clearMask, ni, di)
	NeuronSetFlag(setMask, ni, di)
}

// ApplyExtFlags gets the clear mask and set mask for updating neuron flags
// based on layer type, and whether input should be applied to Target (else Ext)
func (ly *Layer) ApplyExtFlags() (clearMask, setMask NeuronFlags, toTarg bool) {
	ly.Params.ApplyExtFlags(&clearMask, &setMask, &toTarg)
	return
}

// ApplyExt2D applies 2D tensor external input
func (ly *Layer) ApplyExt2D(di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	ymx := min(ext.DimSize(0), ly.Shape.DimSize(0))
	xmx := min(ext.DimSize(1), ly.Shape.DimSize(1))
	for y := 0; y < ymx; y++ {
		for x := 0; x < xmx; x++ {
			idx := []int{y, x}
			val := float32(ext.Float(idx...))
			lni := uint32(ly.Shape.IndexTo1D(idx...))
			ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
		}
	}
}

// ApplyExt2Dto4D applies 2D tensor external input to a 4D layer
func (ly *Layer) ApplyExt2Dto4D(di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	lNy, lNx, _, _ := tensor.Projection2DShape(&ly.Shape, false)

	ymx := min(ext.DimSize(0), lNy)
	xmx := min(ext.DimSize(1), lNx)
	for y := 0; y < ymx; y++ {
		for x := 0; x < xmx; x++ {
			idx := []int{y, x}
			val := float32(ext.Float(idx...))
			lni := uint32(tensor.Projection2DIndex(&ly.Shape, false, y, x))
			ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
		}
	}
}

// ApplyExt4D applies 4D tensor external input
func (ly *Layer) ApplyExt4D(di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	ypmx := min(ext.DimSize(0), ly.Shape.DimSize(0))
	xpmx := min(ext.DimSize(1), ly.Shape.DimSize(1))
	ynmx := min(ext.DimSize(2), ly.Shape.DimSize(2))
	xnmx := min(ext.DimSize(3), ly.Shape.DimSize(3))
	for yp := 0; yp < ypmx; yp++ {
		for xp := 0; xp < xpmx; xp++ {
			for yn := 0; yn < ynmx; yn++ {
				for xn := 0; xn < xnmx; xn++ {
					idx := []int{yp, xp, yn, xn}
					val := float32(ext.Float(idx...))
					lni := uint32(ly.Shape.IndexTo1D(idx...))
					ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
				}
			}
		}
	}
}

// ApplyExt1DTsr applies external input using 1D flat interface into tensor.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1DTsr(di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(ext.Len(), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := float32(ext.Float1D(int(lni)))
		ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
	}
}

// ApplyExt1D applies external input in the form of a flat 1-dimensional slice of floats
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1D(di uint32, ext []float64) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(len(ext), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := float32(ext[lni])
		ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
	}
}

// ApplyExt1D32 applies external input in the form of a flat 1-dimensional slice of float32s.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1D32(di uint32, ext []float32) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(len(ext), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := ext[lni]
		ly.ApplyExtValue(lni, di, val, clearMask, setMask, toTarg)
	}
}

// UpdateExtFlags updates the neuron flags for external input based on current
// layer Type field -- call this if the Type has changed since the last
// ApplyExt* method call.
func (ly *Layer) UpdateExtFlags(ctx *Context) {
	clearMask, setMask, _ := ly.ApplyExtFlags()
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			NeuronClearFlag(clearMask, ni, di)
			NeuronSetFlag(setMask, ni, di)
		}
	}
}

// TargToExt sets external input Ext from target values Target
// This is done at end of MinusPhase to allow targets to drive activity in plus phase.
// This can be called separately to simulate alpha cycles within theta cycles, for example.
func (ly *Layer) TargToExt(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			if !NeuronHasFlag(NeuronHasTarg, ni, di) { // will be clamped in plus phase
				continue
			}
			Neurons[ni, di, Ext] =  Neurons[ni, di, Target]
			NeuronSetFlag(NeuronHasExt, ni, di)
			Neurons[ni, di, ISI] = -1 // get fresh update on plus phase output acts
			Neurons[ni, di, ISIAvg] = -1
		}
	}
}

// ClearTargExt clears external inputs Ext that were set from target values Target.
// This can be called to simulate alpha cycles within theta cycles, for example.
func (ly *Layer) ClearTargExt(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			if !NeuronHasFlag(NeuronHasTarg, ni, di) { // will be clamped in plus phase
				continue
			}
			Neurons[ni, di, Ext] = 0
			NeuronClearFlag(NeuronHasExt, ni, di)
			Neurons[ni, di, ISI] = -1 // get fresh update on plus phase output acts
			Neurons[ni, di, ISIAvg] = -1
		}
	}
}

