// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"bytes"
	"fmt"
	"math/rand"
	"os"
	"runtime"
	"sort"
	"strings"
	"testing"

	"cogentcore.org/lab/base/randx"
	"cogentcore.org/core/math32"
	"cogentcore.org/lab/tensor"
	"github.com/emer/emergent/v2/emer"
	"github.com/emer/emergent/v2/etime"
	"github.com/emer/emergent/v2/paths"
	"github.com/stretchr/testify/assert"
	"golang.org/x/exp/maps"
)

func init() {
	// must lock main thread for gpu!
	runtime.LockOSThread()
}

// tolerance levels -- different tests pass at different levels
var (
	Tol3 = float32(1.0e-3)
	Tol4 = float32(1.0e-4)
	Tol5 = float32(1.0e-5)
	Tol6 = float32(1.0e-6)
	Tol7 = float32(1.0e-7)
	Tol8 = float32(1.0e-8)
)

// number of distinct sets of learning parameters to test
const nLearnParams = 1

// Note: subsequent params applied after Base
var layerParams = LayerSheets{
	"Base": {
		{Sel: "Layer", Doc: "layer defaults",
			Set: func(ly *LayerParams) {
				ly.Acts.Gbar.L =      20
				ly.Learn.RLRate.On.SetBool(false)
				ly.Inhib.Layer.FB =   0.5
			}},
	},
	"InhibOff": {
		{Sel: "Layer", Doc: "layer defaults",
			Set: func(ly *LayerParams) {
				ly.Acts.Gbar.L =    20
				ly.Inhib.Layer.On.SetBool(false)
			}},
	},
	"FullDecay": {
		{Sel: "Layer", Doc: "layer defaults",
			Set: func(ly *LayerParams) {
				ly.Acts.Decay.Act =    1
				ly.Acts.Decay.Glong =  1
				ly.Acts.Decay.AHP =    1
			}},
	},
	"SubMean": {
	},
}

var pathParams = PathSheets{
	"Base": {
		{Sel: "Path", Doc: "for reproducibility, identical weights",
			Set: func(pt *PathParams) {
				pt.SWts.Init.Var =  0
			}},
		{Sel: ".BackPath", Doc: "top-down back-pathways MUST have lower relative weight scale, otherwise network hallucinates",
			Set: func(pt *PathParams) {
				pt.PathScale.Rel =  0.2
			}},
	},
	"InhibOff": {
		{Sel: ".InhibPath", Doc: "weaker inhib",
			Set: func(pt *PathParams) {
				pt.PathScale.Abs =  0.1
			}},
	},
	"FullDecay": {
	},
	"SubMean": {
		{Sel: "Path", Doc: "submean used in some models but not by default",
			Set: func(pt *PathParams) {
				pt.Learn.DWt.SubMean =  1
			}},
	},
}

func newTestNet(nData int) *Network {
	testNet := NewNetwork("testNet")
	testNet.SetRandSeed(42) // critical for ActAvg values
	testNet.SetMaxData(nData)

	inLay := testNet.AddLayer("Input", InputLayer, 4, 1)
	hidLay := testNet.AddLayer("Hidden", SuperLayer, 4, 1)
	outLay := testNet.AddLayer("Output", TargetLayer, 4, 1)

	one2one := paths.NewOneToOne()
	testNet.ConnectLayers(inLay, hidLay, one2one, ForwardPath)
	testNet.ConnectLayers(hidLay, outLay, one2one, ForwardPath)
	testNet.ConnectLayers(outLay, hidLay, one2one, BackPath)

	testNet.Rubicon.SetNUSs(4, 3)
	testNet.Rubicon.Defaults()

	testNet.Build()
	testNet.Defaults()
	ApplyParamSheets(testNet, layerParams["Base"], pathParams["Base"])
	testNet.InitWeights()  // get GScale here
	testNet.ThetaCycleStart(etime.Train, false)
	return testNet
}

// full connectivity
func newTestNetFull(nData int) *Network {
	testNet := NewNetwork("testNetFull")
	testNet.SetRandSeed(42) // critical for ActAvg values
	testNet.SetMaxData(nData)

	inLay := testNet.AddLayer("Input", InputLayer, 4, 1)
	hidLay := testNet.AddLayer("Hidden", SuperLayer, 4, 1)
	outLay := testNet.AddLayer("Output", TargetLayer, 4, 1)

	full := paths.NewFull()
	testNet.ConnectLayers(inLay, hidLay, full, ForwardPath)
	testNet.ConnectLayers(hidLay, outLay, full, ForwardPath)
	testNet.ConnectLayers(outLay, hidLay, full, BackPath)

	testNet.Build()
	testNet.Defaults()
	ApplyParamSheets(testNet, layerParams["Base"], pathParams["Base"])
	testNet.InitWeights()                      // get GScale here
	testNet.ThetaCycleStart(etime.Train, false)
	return testNet
}

func TestSynValues(t *testing.T) {
	tol := Tol8
	testNet := newTestNet(1)
	hidLay := testNet.LayerByName("Hidden")
	p, err := hidLay.RecvPathBySendName("Input")
	if err != nil {
		t.Error(err)
	}
	fmIn := p.(*Path)

	bfWt := fmIn.SynValue("Wt", 1, 1)
	if math32.IsNaN(bfWt) {
		t.Errorf("Wt syn var not found")
	}
	bfLWt := fmIn.SynValue("LWt", 1, 1)

	fmIn.SetSynValue("Wt", 1, 1, .15)

	afWt := fmIn.SynValue("Wt", 1, 1)
	afLWt := fmIn.SynValue("LWt", 1, 1)

	CompareFloats(tol, []float32{bfWt, bfLWt, afWt, afLWt}, []float32{0.5, 0.5, 0.15, 0.42822415}, "syn val setting test", t)
}

func newInPats() *tensor.Float32 {
	inPats := tensor.NewFloat32(4, 4, 1)
	for pi := range 4 {
		inPats.Set(1, pi, pi, 0)
	}
	return inPats
}

func CompareFloats(tolerance float32, out, cor []float32, msg string, t *testing.T) {
	t.Helper()
	hadErr := false
	for i := range out {
		if math32.IsNaN(out[1]) {
			t.Errorf("%v err: out: %v is NaN, index: %v\n", msg, out[i], i)
		}
		dif := math32.Abs(out[i] - cor[i])
		if dif > tolerance { // allow for small numerical diffs
			hadErr = true
			t.Errorf("%v err: out: %v, cor: %v, dif: %v index: %v\n", msg, out[i], cor[i], dif, i)
		}
	}
	if hadErr {
		fmt.Printf("\t%s := []float32{", msg)
		for i := range out {
			fmt.Printf("%g", out[i])
			if i < len(out)-1 {
				fmt.Printf(", ")
			}
		}
		fmt.Printf("}\n")
	}
}

func TestNewNetwork(t *testing.T) {
	testNet := NewNetwork("testNet")
	assert.Equal(t, "testNet", testNet.Name)
	assert.Equal(t, 0, testNet.NumLayers())
	assert.IsType(t, &Network{}, testNet)
}

func TestInterfaceType(t *testing.T) {
	testNet := NewNetwork("testNet")
	_, ok := testNet.EmerNetwork.(emer.Network)
	assert.True(t, ok)
}

func TestSpikeProp(t *testing.T) {
	net := NewNetwork("SpikeNet")
	inLay := net.AddLayer("Input", InputLayer, 1, 1)
	hidLay := net.AddLayer("Hidden", SuperLayer, 1, 1)

	pt := net.ConnectLayers(inLay, hidLay, paths.NewOneToOne(), ForwardPath)

	net.Build()
	net.Defaults()
	ApplyParamSheets(net, layerParams["Base"], pathParams["Base"])

	net.InitExt()

	pat := tensor.NewFloat32(1, 1)
	pat.Set(1, 0, 0)

	for del := range 5 {
		pt.Params.Com.Delay = uint32(del)
		pt.Params.Com.MaxDelay = uint32(del) // now need to ensure that >= Delay
		net.InitWeights()                  // resets Gbuf
		net.ThetaCycleStart(etime.Train, false)
		net.MinusPhaseStart()

		inLay.ApplyExt(0, pat)

		inCyc := 0
		hidCyc := 0
		for cyc := range 100 {
			net.Cycle(true)
			// fmt.Println(cyc, Neurons[hidLay.NeurStIndex, 0, Ge], Neurons[hidLay.NeurStIndex, 0, GeRaw])
			if Neurons[inLay.NeurStIndex, 0, Spike] > 0 {
				// fmt.Println("in spike:", cyc)
				inCyc = cyc
			}

			ge := Neurons[hidLay.NeurStIndex, 0, Ge]
			if ge > 0 {
				// fmt.Println("hid recv:", cyc, ge)
				hidCyc = cyc
				break
			// } else {
			// 	fmt.Println("hid not recv:", cyc, ge)
			}
		}
		if hidCyc-inCyc != del+1 {
			t.Errorf("SpikeProp error -- delay: %d  actual: %d\n", del, hidCyc-inCyc)
		}
	}
}

// poolValues adds pool vals to given vals map
func poolValues(pi uint32, di int, vals map[string]float32, key string) {
	for i := range uint32(PoolVarsTotal) {
		kk := key + fmt.Sprintf("\t%s", PoolVarName(i))
		vals[kk] = Pools[pi, di, i]
	}
	for i := range uint32(PoolIntVarsTot) {
		kk := key + fmt.Sprintf("\t%s", PoolIntVarName(i))
		vals[kk] = float32(PoolsInt[pi, di, i])
	}
}

// layerStates adds LayerStates vals to given vals map
func layerStates(li, di int, vals map[string]float32, key string) {
	for i := range uint32(LayerVarsN) {
		kk := key + fmt.Sprintf("\t%s", LayerVars(i).String())
		vals[kk] = LayerStates[li, di, i]
	}
}

// TestInitWeights tests that initializing the weights results in same state
func TestInitWeights(t *testing.T) {
	nData := 3
	testNet := newTestNet(nData)
	inPats := newInPats()

	valMapA := make(map[string]float32)
	valMapB := make(map[string]float32)

	inLay := testNet.LayerByName("Input")
	outLay := testNet.LayerByName("Output")

	var vals []float32

	valMap := valMapA
	for wi := range 2 {
		// fmt.Println("\n########## Pass:", wi)
		if wi == 1 {
			valMap = valMapB
		}
		testNet.SetRandSeed(42) // critical for ActAvg values
		testNet.InitWeights()
		testNet.InitExt()
		for li := range 3 {
			ly := testNet.Layers[li]
			for di := range nData {
				for _, vnm := range NeuronVarNames {
					ly.UnitValues(&vals, vnm, di)
					for ni := range 4 {
						key := fmt.Sprintf("Layer: %s\tUnit: %d\tDi: %d\t%s", ly.Name, ni, di, vnm)
						valMap[key] = vals[ni]
					}
				}
			}
		}
		for li := range 3 {
			ly := testNet.Layers[li]
			for di := range nData {
				lpi := ly.Params.PoolIndex(0)
				lnm := fmt.Sprintf("%s: di: %d", ly.Name, di)
				poolValues(lpi, di, valMap, lnm)
				layerStates(li, di, valMap, lnm)
			}
		}

		for pi := range 4 {
			testNet.ThetaCycleStart(etime.Train, false)
			testNet.MinusPhaseStart()

			inpat := inPats.SubSpace(pi)
			testNet.InitExt()
			for di := range nData {
				inLay.ApplyExt(uint32(di), inpat)
				outLay.ApplyExt(uint32(di), inpat)
			}
			testNet.ApplyExts() // key now for GPU

			for qtr := range 4 {
				for range 50 {
					testNet.Cycle(true)
				}
				if qtr == 2 {
					testNet.MinusPhaseEnd()
					testNet.PlusPhaseStart()
				}
			}
			testNet.PlusPhaseEnd()
			testNet.DWt()
			testNet.WtFromDWt()
		}
	}
	ReportValDiffs(t, Tol8, valMapA, valMapB, "init1", "init2")
}

func TestGPUState(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	
	testNetA := newTestNet(1)
	
	GPUInit()
	UseGPU = true

	testNetB := newTestNet(1)

	RunCycleInc(1)
	// get everything back
	RunDone(CtxVar, GlobalScalarsVar, GlobalVectorsVar, LayerStatesVar, PoolsVar, PoolsIntVar, NeuronsVar, NeuronAvgsVar, SynapsesVar, SynapseTracesVar, PathGBufVar, PathGSynsVar)
	// note: the following requires turning off read-only in vars.go
	// RunDone(LayersVar, PathsVar, NetworkIxsVar, NeuronIxsVar, SynapseIxsVar, PathSendConVar, RecvPathIxsVar, PathRecvConVar, RecvSynIxsVar, CtxVar, GlobalScalarsVar, GlobalVectorsVar, LayerStatesVar, PoolsVar, PoolsIntVar, NeuronsVar, NeuronAvgsVar, SynapsesVar, SynapseTracesVar, PathGBufVar, PathGSynsVar)
	// assert.Equal(t, testNetA.LayerParams, testNetB.LayerParams)
	// assert.Equal(t, testNetA.LayerParams, Layers)
	// assert.Equal(t, testNetA.PathParams, testNetB.PathParams)
	// assert.Equal(t, testNetA.NetworkIxs, testNetB.NetworkIxs)
	// assert.Equal(t, testNetA.NeuronIxs.Values, testNetB.NeuronIxs.Values)
	// assert.Equal(t, testNetA.SynapseIxs.Values, testNetB.SynapseIxs.Values)
	// assert.Equal(t, testNetA.PathSendCon.Values, testNetB.PathSendCon.Values)
	// assert.Equal(t, testNetA.RecvPathIxs.Values, testNetB.RecvPathIxs.Values)
	// assert.Equal(t, testNetA.PathRecvCon.Values, testNetB.PathRecvCon.Values)
	// assert.Equal(t, testNetA.RecvSynIxs.Values, testNetB.RecvSynIxs.Values)
	assert.NotEqual(t, testNetA.Ctx, testNetB.Ctx)
	assert.Equal(t, testNetA.Neurons.Values, testNetB.Neurons.Values)
	assert.Equal(t, testNetA.NeuronAvgs.Values, testNetB.NeuronAvgs.Values)
	assert.Equal(t, testNetA.LayerStates.Values, testNetB.LayerStates.Values)
	assert.Equal(t, testNetA.GlobalScalars.Values, testNetB.GlobalScalars.Values)
	assert.Equal(t, testNetA.GlobalVectors.Values, testNetB.GlobalVectors.Values)
	assert.Equal(t, testNetA.Exts.Values, testNetB.Exts.Values)
	assert.Equal(t, testNetA.Pools.Values, testNetB.Pools.Values)
	assert.Equal(t, testNetA.PoolsInt.Values, testNetB.PoolsInt.Values)
	assert.Equal(t, testNetA.PathGBuf.Values, testNetB.PathGBuf.Values)
	assert.Equal(t, testNetA.PathGSyns.Values, testNetB.PathGSyns.Values)
	assert.Equal(t, testNetA.Synapses.Values, testNetB.Synapses.Values)
	assert.Equal(t, testNetA.SynapseTraces.Values, testNetB.SynapseTraces.Values)
}

func TestNetAct(t *testing.T) {
	NetActTest(t, Tol6, false) // gpu
}

func TestNetActShort(t *testing.T) {
	NetActTestShort(t, Tol7, false)
}

func TestGPUAct(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	NetActTestShort(t, Tol6, true)
	// NetActTest(t, Tol6, true)
}

// NetActTest runs an activation test on the network and checks
// for key values relative to known standards.
// Note: use NetDebugAct for printf debugging of all values --
// "this is only a test"
func NetActTest(t *testing.T, tol float32, gpu bool) {
	if gpu {
		GPUInit()
		UseGPU = true
	}

	testNet := newTestNet(1)
	testNet.InitExt()
	inPats := newInPats()

	inLay := testNet.LayerByName("Input")
	hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")

	qtr0HidActs := []float32{0.6944439, 0, 0, 0}
	qtr0HidGes := []float32{0.35238016, 0, 0, 0}
	qtr0HidGis := []float32{0.15478326, 0.15478326, 0.15478326, 0.15478326}
	qtr0OutActs := []float32{0.55552065, 0, 0, 0}
	qtr0OutGes := []float32{0.35870183, 0, 0, 0}
	qtr0OutGis := []float32{0.20974194, 0.20974194, 0.20974194, 0.20974194}

	qtr3HidActs := []float32{0.56933826, 0, 0, 0}
	qtr3HidGes := []float32{0.43519318, 0, 0, 0}
	qtr3HidGis := []float32{0.21780372, 0.21780372, 0.21780372, 0.21780372}
	qtr3OutActs := []float32{0.69444436, 0, 0, 0}
	qtr3OutGes := []float32{0.8, 0, 0, 0}
	qtr3OutGis := []float32{0.48570368, 0.48570368, 0.48570368, 0.48570368}

	p1qtr0HidActs := []float32{1.2795964e-10, 0.46289298, 0, 0}
	p1qtr0HidGes := []float32{0.011576458, 0.45270887, 0, 0}
	p1qtr0HidGis := []float32{0.1974176, 0.1974176, 0.1974176, 0.1974176}
	p1qtr0OutActs := []float32{1.5607746e-10, 0, 0, 0}
	p1qtr0OutGes := []float32{0.014116121, 0.26705682, 0, 0}
	p1qtr0OutGis := []float32{0.089714766, 0.089714766, 0.089714766, 0.089714766}

	p1qtr3HidActs := []float32{2.837341e-39, 0.56031764, 0, 0}
	p1qtr3HidGes := []float32{0.0022698431, 0.5705052, 0, 0}
	p1qtr3HidGis := []float32{0.2899328, 0.2899328, 0.2899328, 0.2899328}
	p1qtr3OutActs := []float32{3.460815e-39, 0.69444436, 0, 0}
	p1qtr3OutGes := []float32{0, 0.8, 0, 0}
	p1qtr3OutGis := []float32{0.43397403, 0.43397403, 0.43397403, 0.43397403}

	inActs := []float32{}
	hidActs := []float32{}
	hidGes := []float32{}
	hidGis := []float32{}
	outActs := []float32{}
	outGes := []float32{}
	outGis := []float32{}
	cycPerQtr := 50

	for pi := range 2 {
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()

		inpat := inPats.SubSpace(pi)
		testNet.InitExt()
		inLay.ApplyExt(0, inpat)
		outLay.ApplyExt(0, inpat)
		testNet.ApplyExts() // key now for GPU

		for qtr := range 4 {
			for range cycPerQtr {
				testNet.Cycle(true)
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}

			inLay.UnitValues(&inActs, "Act", 0)
			hidLay.UnitValues(&hidActs, "Act", 0)
			hidLay.UnitValues(&hidGes, "Ge", 0)
			hidLay.UnitValues(&hidGis, "Gi", 0)
			outLay.UnitValues(&outActs, "Act", 0)
			outLay.UnitValues(&outGes, "Ge", 0)
			outLay.UnitValues(&outGis, "Gi", 0)

			if pi == 0 && qtr == 0 {
				CompareFloats(tol, hidActs, qtr0HidActs, "qtr0HidActs", t)
				CompareFloats(tol, hidGes, qtr0HidGes, "qtr0HidGes", t)
				CompareFloats(tol, hidGis, qtr0HidGis, "qtr0HidGis", t)
				CompareFloats(tol, outActs, qtr0OutActs, "qtr0OutActs", t)
				CompareFloats(tol, outGes, qtr0OutGes, "qtr0OutGes", t)
				CompareFloats(tol, outGis, qtr0OutGis, "qtr0OutGis", t)
			}
			if pi == 0 && qtr == 3 {
				CompareFloats(tol, hidActs, qtr3HidActs, "qtr3HidActs", t)
				CompareFloats(tol, hidGes, qtr3HidGes, "qtr3HidGes", t)
				CompareFloats(tol, hidGis, qtr3HidGis, "qtr3HidGis", t)
				CompareFloats(tol, outActs, qtr3OutActs, "qtr3OutActs", t)
				CompareFloats(tol, outGes, qtr3OutGes, "qtr3OutGes", t)
				CompareFloats(tol, outGis, qtr3OutGis, "qtr3OutGis", t)
			}
			if pi == 1 && qtr == 0 {
				CompareFloats(tol, hidActs, p1qtr0HidActs, "p1qtr0HidActs", t)
				CompareFloats(tol, hidGes, p1qtr0HidGes, "p1qtr0HidGes", t)
				CompareFloats(tol, hidGis, p1qtr0HidGis, "p1qtr0HidGis", t)
				CompareFloats(tol, outActs, p1qtr0OutActs, "p1qtr0OutActs", t)
				CompareFloats(tol, outGes, p1qtr0OutGes, "p1qtr0OutGes", t)
				CompareFloats(tol, outGis, p1qtr0OutGis, "p1qtr0OutGis", t)
			}
			if pi == 1 && qtr == 3 {
				CompareFloats(tol, hidActs, p1qtr3HidActs, "p1qtr3HidActs", t)
				CompareFloats(tol, hidGes, p1qtr3HidGes, "p1qtr3HidGes", t)
				CompareFloats(tol, hidGis, p1qtr3HidGis, "p1qtr3HidGis", t)
				CompareFloats(tol, outActs, p1qtr3OutActs, "p1qtr3OutActs", t)
				CompareFloats(tol, outGes, p1qtr3OutGes, "p1qtr3OutGes", t)
				CompareFloats(tol, outGis, p1qtr3OutGis, "p1qtr3OutGis", t)
			}
		}
		testNet.PlusPhaseEnd()
	}

	GPURelease()
	UseGPU = false
}

// NetActTestShort runs an activation test on the network and checks
// for key values relative to known standards: short version for GPU
// which diverges from CPU unfortunately.
// Note: use NetDebugAct for printf debugging of all values --
// "this is only a test"
func NetActTestShort(t *testing.T, tol float32, gpu bool) {
	if gpu {
		GPUInit()
		UseGPU = true
	}

	testNet := newTestNet(1)
	testNet.InitExt()
	inPats := newInPats()

	inLay := testNet.LayerByName("Input")
	hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")

	qtr0HidActs := []float32{0.6944009, 0, 0, 0}
	qtr0HidGes := []float32{0.7366979, 0, 0, 0}
	qtr0HidGis := []float32{0.24779804, 0.24779804, 0.24779804, 0.24779804}
	qtr0OutActs := []float32{0.55272156, 0, 0, 0}
	qtr0OutGes := []float32{0.4743182, 0, 0, 0}
	qtr0OutGis := []float32{0.21460174, 0.21460174, 0.21460174, 0.21460174}

	p1qtr0HidActs := []float32{1.2693764e-08, 0.56647456, 0, 0}
	p1qtr0HidGes := []float32{0.0060101417, 0.6999332, 0, 0}
	p1qtr0HidGis :=  []float32{0.22110817, 0.22110817, 0.22110817, 0.22110817}
	p1qtr0OutActs := []float32{1.0103845e-08, 0.4298971, 0, 0}
	p1qtr0OutGes := []float32{0.005442092, 0.2531417, 0, 0}
	p1qtr0OutGis := []float32{0.08388885, 0.08388885, 0.08388885, 0.08388885}

	inActs := []float32{}
	hidActs := []float32{}
	hidGes := []float32{}
	hidGis := []float32{}
	outActs := []float32{}
	outGes := []float32{}
	outGis := []float32{}

	npats := 1
	qtrs := 1
	cycPerQtr := 40

	for pi := range npats {
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()

		inpat := inPats.SubSpace(pi)
		testNet.InitExt()
		inLay.ApplyExt(0, inpat)
		outLay.ApplyExt(0, inpat)
		testNet.ApplyExts() // key now for GPU

		for qtr := range qtrs {
			for cyc := range cycPerQtr {
				_ = cyc
				testNet.Cycle(true)
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}

			inLay.UnitValues(&inActs, "Act", 0)
			hidLay.UnitValues(&hidActs, "Act", 0)
			hidLay.UnitValues(&hidGes, "Ge", 0)
			hidLay.UnitValues(&hidGis, "Gi", 0)
			outLay.UnitValues(&outActs, "Act", 0)
			outLay.UnitValues(&outGes, "Ge", 0)
			outLay.UnitValues(&outGis, "Gi", 0)

			if pi == 0 && qtr == 0 {
				CompareFloats(tol, hidActs, qtr0HidActs, "qtr0HidActs", t)
				CompareFloats(tol, hidGes, qtr0HidGes, "qtr0HidGes", t)
				CompareFloats(tol, hidGis, qtr0HidGis, "qtr0HidGis", t)
				CompareFloats(tol, outActs, qtr0OutActs, "qtr0OutActs", t)
				CompareFloats(tol, outGes, qtr0OutGes, "qtr0OutGes", t)
				CompareFloats(tol, outGis, qtr0OutGis, "qtr0OutGis", t)
			}
			if pi == 1 && qtr == 0 {
				CompareFloats(tol, hidActs, p1qtr0HidActs, "p1qtr0HidActs", t)
				CompareFloats(tol, hidGes, p1qtr0HidGes, "p1qtr0HidGes", t)
				CompareFloats(tol, hidGis, p1qtr0HidGis, "p1qtr0HidGis", t)
				CompareFloats(tol, outActs, p1qtr0OutActs, "p1qtr0OutActs", t)
				CompareFloats(tol, outGes, p1qtr0OutGes, "p1qtr0OutGes", t)
				CompareFloats(tol, outGis, p1qtr0OutGis, "p1qtr0OutGis", t)
			}
		}
		testNet.PlusPhaseEnd()
	}
	GPURelease()
	UseGPU = false
}

// ReportValDiffs -- reports diffs between a, b values at given tolerance
func ReportValDiffs(t *testing.T, tolerance float32, va, vb map[string]float32, aLabel, bLabel string, exclude ...string) {
	keys := maps.Keys(va)
	sort.Strings(keys)
	nerrs := 0
	for _, k := range keys {
		hasEx := false
		for _, ex := range exclude {
			if strings.Contains(k, ex) {
				hasEx = true
				break
			}
		}
		if hasEx {
			continue
		}
		av := va[k]
		bv := vb[k]
		dif := math32.Abs(av - bv)
		if dif > tolerance { // allow for small numerical diffs
			if nerrs == 0 {
				t.Errorf("Diffs found between two runs (10 max): A = %s  B = %s\n", aLabel, bLabel)
			}
			fmt.Printf("%s\tA: %g\tB: %g\tDiff: %g\n", k, av, bv, dif)
			nerrs++
			if nerrs > 100 {
				fmt.Printf("Max diffs exceeded, increase for more\n")
				break
			}
		}
	}
}

// NetDebugAct prints selected values (if printValues),
// and also returns a map of all values and variables that can be used for a more
// fine-grained diff test, e.g., see the GPU version.
func NetDebugAct(t *testing.T, printValues bool, gpu bool, nData int, initWts bool) map[string]float32 {
	if gpu {
		GPUInit()
		UseGPU = true
	}
	
	testNet := newTestNet(nData)
	ApplyParamSheets(testNet, layerParams["FullDecay"], pathParams["FullDecay"])
	
	return RunDebugAct(t, testNet, printValues, gpu, initWts)
}

// RunDebugAct runs and prints selected values (if printValues),
// and also returns a map of all values and variables that can be used for a more
// fine-grained diff test, e.g., see the GPU version.
func RunDebugAct(t *testing.T, testNet *Network, printValues bool, gpu bool, initWts bool) map[string]float32 {
	ctx := testNet.Context()
	nData := int(ctx.NData)
	valMap := make(map[string]float32)
	inPats := newInPats()
	inLay := testNet.LayerByName("Input")
	// hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")
	_, _ = inLay, outLay

	var vals []float32

	// these control what is printed.
	// the whole thing is run and returned in the valMap
	valsPerRow := 4
	nQtrs := 1     // max 4
	cycPerQtr := 40 // max 50
	nPats := 1     // max 4
	stLayer := 1   // max 2
	edLayer := 2   // max 3
	nNeurs := 1    // max 4 -- number of neuron values to print

	for pi := 0; pi < nPats; pi++ {
		if initWts {
			testNet.SetRandSeed(42) // critical for ActAvg values
			testNet.InitWeights()
		}
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()

		testNet.InitExt()
		for di := 0; di < nData; di++ {
			ppi := (pi + di) % 4
			inpat := inPats.SubSpace(ppi)
			_ = inpat
			inLay.ApplyExt(uint32(di), inpat)
			outLay.ApplyExt(uint32(di), inpat)
		}

		testNet.ApplyExts() // key now for GPU

		for qtr := 0; qtr < nQtrs; qtr++ {
			for cyc := 0; cyc < cycPerQtr; cyc++ {
				// testNet.GPUTestWrite()
				testNet.Cycle(true) // get neuron state

				for ni := 0; ni < 4; ni++ {
					for li := 0; li < 3; li++ {
						ly := testNet.Layers[li]
						for di := 0; di < nData; di++ {
							ppi := (pi + di) % 4
							key := fmt.Sprintf("pat: %d\tqtr: %d\tcyc: %02d\tLayer: %s\tUnit: %d", ppi, qtr, cyc, ly.Name, ni)
							doPrint := (printValues && pi < nPats && qtr < nQtrs && cyc < cycPerQtr && ni < nNeurs && li >= stLayer && li < edLayer)
							if doPrint {
								fmt.Println(key)
							}
							for nvi, vnm := range NeuronVarNames {
								ly.UnitValues(&vals, vnm, di)
								vkey := key + fmt.Sprintf("\t%s", vnm)
								valMap[vkey] = vals[ni]
								if doPrint {
									fmt.Printf("\t%-10s%7.4f", vnm, vals[ni])
									if (int(nvi)+1)%valsPerRow == 0 {
										fmt.Printf("\n")
									}
								}
							}
							if doPrint {
								fmt.Printf("\n")
							}
						}
					}
				}
				for li := 0; li < 3; li++ {
					ly := testNet.Layers[li]
					for di := 0; di < nData; di++ {
						ppi := (pi + di) % 4
						// lpl := ly.Pool(0, uint32(di))
						key := fmt.Sprintf("pat: %d\tqtr: %d\tcyc: %02d\tLayer: %s", ppi, qtr, cyc, ly.Name)
						_ = key
						// StructValues(&lpl.Inhib, valMap, key)
					}
				}
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}
		}

		testNet.PlusPhaseEnd()
		pi += nData - 1
	}

	GPURelease()
	UseGPU = false
	return valMap
}

func TestGPUDiffs(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	nonGPUValues := NetDebugAct(t, false, false, 1, false)
	gpuValues := NetDebugAct(t, false, true, 1, false)
	// note: this has bad tolerance due to NMDA -- can see that if you raise tol to Tol5 etc
	ReportValDiffs(t, Tol4, nonGPUValues, gpuValues, "CPU", "GPU")
}

func TestDebugAct(t *testing.T) {
	t.Skip("skipped in regular testing")
	NetDebugAct(t, true, false, 1, false)
}

func TestDebugGPUAct(t *testing.T) {
	t.Skip("skipped in regular testing")
	NetDebugAct(t, true, true, 1, false)
}

func TestNDataDiffs(t *testing.T) {
	nd1Values := NetDebugAct(t, false, false, 1, true)
	nd4Values := NetDebugAct(t, false, false, 4, true)
	ReportValDiffs(t, Tol8, nd1Values, nd4Values, "nData = 1", "nData = 4")
}

func TestGPUNDataDiffs(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	nd1Values := NetDebugAct(t, false, true, 1, true)
	nd4Values := NetDebugAct(t, false, true, 4, true)
	ReportValDiffs(t, Tol8, nd1Values, nd4Values, "nData = 1", "nData = 4")
}

func TestNetLearn(t *testing.T) {
	NetTestLearn(t, Tol6, false)
}

func TestGPULearn(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	NetTestLearn(t, Tol6, true)
}

func NetTestLearn(t *testing.T, tol float32, gpu bool) {
	if gpu {
		GPUInit()
		UseGPU = true
	}

	testNet := newTestNet(1)
	ctx := testNet.Context()

	inPats := newInPats()
	inLay := testNet.LayerByName("Input")
	hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")

	// allp := testNet.AllParams()
	// os.WriteFile("test_net_act_all_pars.txt", []byte(allp), 0664)

	printCycs := false
	printQtrs := false

	// these are organized by pattern within and then by test iteration (params) outer
	// only the single active synapse is represented -- one per pattern
	// if there are differences, they will multiply over patterns and layers..
	qtr3HidCaP := []float32{0.5500735, 0.5519808, 0.54200256, 0.5519518}
	qtr3HidCaD := []float32{0.52224743, 0.49780282, 0.49890643, 0.50385964}
	qtr3OutCaP := []float32{0.58267534, 0.56314445, 0.5760921, 0.57729626}
	qtr3OutCaD := []float32{0.50602317, 0.4639777, 0.4770922, 0.47976217}
 
	q3hidCaP := make([]float32, 4*nLearnParams)
	q3hidCaD := make([]float32, 4*nLearnParams)
	q3outCaP := make([]float32, 4*nLearnParams)
	q3outCaD := make([]float32, 4*nLearnParams)

	hidDwts := []float32{0.0046170885, 0.00824979, 0.006619337, 0.007457238}
	outDwts := []float32{0.0055950927, 0.010907927, 0.010558605, 0.010816593}
	hidWts := []float32{0.52767503, 0.5493423, 0.539635, 0.5446278}
	outWts := []float32{0.5335216, 0.5650866, 0.56302404, 0.56454736}

	hiddwt := make([]float32, 4*nLearnParams)
	outdwt := make([]float32, 4*nLearnParams)
	hidwt := make([]float32, 4*nLearnParams)
	outwt := make([]float32, 4*nLearnParams)

	hidAct := []float32{}
	hidGes := []float32{}
	hidGis := []float32{}
	hidCaM := []float32{}
	hidCaP := []float32{}
	hidCaD := []float32{}
	outCaP := []float32{}
	outCaD := []float32{}

	cycPerQtr := 50

	testNet.Defaults()
	ApplyParamSheets(testNet, layerParams["Base"], pathParams["Base"])
	testNet.InitWeights()
	testNet.InitExt()

	for pi := 0; pi < 4; pi++ {
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()

		inpat := inPats.SubSpace(pi)
		testNet.InitExt()
		inLay.ApplyExt(0, inpat)
		outLay.ApplyExt(0, inpat)
		testNet.ApplyExts() // key now for GPU

		for qtr := 0; qtr < 4; qtr++ {
			for cyc := 0; cyc < cycPerQtr; cyc++ {
				testNet.Cycle(true)

				hidLay.UnitValues(&hidAct, "Act", 0)
				hidLay.UnitValues(&hidGes, "Ge", 0)
				hidLay.UnitValues(&hidGis, "Gi", 0)
				hidLay.UnitValues(&hidCaM, "LearnCaM", 0)
				hidLay.UnitValues(&hidCaP, "LearnCaP", 0)
				hidLay.UnitValues(&hidCaD, "LearnCaD", 0)

				outLay.UnitValues(&outCaP, "LearnCaP", 0)
				outLay.UnitValues(&outCaD, "LearnCaD", 0)

				if printCycs {
					fmt.Printf("pat: %v qtr: %v cyc: %v\nhid act: %v ges: %v gis: %v\nhid avgss: %v avgs: %v avgm: %v\nout avgs: %v avgm: %v\n", pi, qtr, ctx.Cycle, hidAct, hidGes, hidGis, hidCaM, hidCaP, hidCaD, outCaP, outCaD)
				}
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}

			hidLay.UnitValues(&hidCaP, "LearnCaP", 0)
			hidLay.UnitValues(&hidCaD, "LearnCaD", 0)

			outLay.UnitValues(&outCaP, "LearnCaP", 0)
			outLay.UnitValues(&outCaD, "LearnCaD", 0)

			if qtr == 3 {
				didx := pi
				q3hidCaD[didx] = hidCaD[pi]
				q3hidCaP[didx] = hidCaP[pi]
				q3outCaD[didx] = outCaD[pi]
				q3outCaP[didx] = outCaP[pi]
			}

			if printQtrs {
				fmt.Printf("pat: %v qtr: %v cyc: %v\nhid avgs: %v avgm: %v\nout avgs: %v avgm: %v\n", pi, qtr, ctx.Cycle, hidCaP, hidCaD, outCaP, outCaD)
			}

		}
		testNet.PlusPhaseEnd()

		if printQtrs {
			fmt.Printf("=============================\n")
		}

		testNet.DWt()

		didx := pi

		hiddwt[didx] = hidLay.RecvPaths[0].SynValue("DWt", pi, pi)
		outdwt[didx] = outLay.RecvPaths[0].SynValue("DWt", pi, pi)

		testNet.WtFromDWt()

		hidwt[didx] = hidLay.RecvPaths[0].SynValue("Wt", pi, pi)
		outwt[didx] = outLay.RecvPaths[0].SynValue("Wt", pi, pi)
	}

	CompareFloats(tol, q3hidCaP, qtr3HidCaP, "qtr3HidCaP", t)
	CompareFloats(tol, q3hidCaD, qtr3HidCaD, "qtr3HidCaD", t)
	CompareFloats(tol, q3outCaP, qtr3OutCaP, "qtr3OutCaP", t)
	CompareFloats(tol, q3outCaD, qtr3OutCaD, "qtr3OutCaD", t)

	CompareFloats(tol, hiddwt, hidDwts, "hidDwts", t)
	CompareFloats(tol, outdwt, outDwts, "outDwts", t)
	CompareFloats(tol, hidwt, hidWts, "hidWts", t)
	CompareFloats(tol, outwt, outWts, "outWts", t)

	GPURelease()
	UseGPU = false
}

func TestNetRLRate(t *testing.T) {
	NetTestRLRate(t, Tol6, false)
}

func TestGPURLRate(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	NetTestRLRate(t, Tol6, true)
}

func NetTestRLRate(t *testing.T, tol float32, gpu bool) {
	testNet := newTestNet(1)
	ctx := testNet.Context()
	inPats := newInPats()
	inLay := testNet.LayerByName("Input")
	hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")

	// allp := testNet.AllParams()
	// os.WriteFile("test_net_act_all_pars.txt", []byte(allp), 0664)

	printCycs := false
	printQtrs := false

	patHidRLRates := []float32{5.0000002e-05, 5.0000002e-05, 5.0000002e-05, 5.0000002e-05, 0.00018922218, 0.003436682, 5.0000002e-05, 5.0000002e-05, 5.0000002e-05, 0.00016346086, 0.0029788367, 5.0000002e-05, 5.0000002e-05, 5.0000002e-05, 0.00018306368, 0.0037452176}
	
	// these are organized by pattern within and then by test iteration (params) outer
	// only the single active synapse is represented -- one per pattern
	// if there are differences, they will multiply over patterns and layers..

	qtr3HidCaP := []float32{0.5500735, 0.5519808, 0.54200256, 0.5519518}
	qtr3HidCaD := []float32{0.52224743, 0.49780282, 0.49890643, 0.50385964}
	qtr3OutCaP := []float32{0.58267534, 0.56314445, 0.5760921, 0.57729626}
	qtr3OutCaD := []float32{0.50602317, 0.4639777, 0.4770922, 0.47976217}
 
	q3hidCaP := make([]float32, 4*nLearnParams)
	q3hidCaD := make([]float32, 4*nLearnParams)
	q3outCaP := make([]float32, 4*nLearnParams)
	q3outCaD := make([]float32, 4*nLearnParams)

	hidDwts := []float32{2.3085444e-07, 2.8351906e-05, 1.9717923e-05, 2.7928978e-05}
	outDwts := []float32{0.0055950927, 0.010907927, 0.010558605, 0.010816593}
	hidWts := []float32{0.50000143, 0.50017023, 0.5001184, 0.5001677}
	outWts := []float32{0.5335216, 0.5650866, 0.56302404, 0.56454736}

	hiddwt := make([]float32, 4*nLearnParams)
	outdwt := make([]float32, 4*nLearnParams)
	hidwt := make([]float32, 4*nLearnParams)
	outwt := make([]float32, 4*nLearnParams)
	hidrlrs := make([]float32, 4*4*nLearnParams) // 4 units, 4 pats

	hidAct := []float32{}
	hidGes := []float32{}
	hidGis := []float32{}
	hidCaM := []float32{}
	hidCaP := []float32{}
	hidCaD := []float32{}
	hidRLRate := []float32{}
	outCaP := []float32{}
	outCaD := []float32{}

	cycPerQtr := 50

	testNet.Defaults()
	ApplyParamSheets(testNet, layerParams["Base"], pathParams["Base"])
	hidLay.Params.Learn.RLRate.On.SetBool(true)
	testNet.InitWeights()
	testNet.InitExt()

	for pi := 0; pi < 4; pi++ {
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()
		
		inpat := inPats.SubSpace(pi)
		testNet.InitExt()
		inLay.ApplyExt(0, inpat)
		outLay.ApplyExt(0, inpat)
		testNet.ApplyExts() // key now for GPU

		for qtr := 0; qtr < 4; qtr++ {
			for cyc := 0; cyc < cycPerQtr; cyc++ {
				testNet.Cycle(true)

				hidLay.UnitValues(&hidAct, "Act", 0)
				hidLay.UnitValues(&hidGes, "Ge", 0)
				hidLay.UnitValues(&hidGis, "Gi", 0)
				hidLay.UnitValues(&hidCaM, "LearnCaM", 0)
				hidLay.UnitValues(&hidCaP, "LearnCaP", 0)
				hidLay.UnitValues(&hidCaD, "LearnCaD", 0)

				outLay.UnitValues(&outCaP, "LearnCaP", 0)
				outLay.UnitValues(&outCaD, "LearnCaD", 0)

				if printCycs {
					fmt.Printf("pat: %v qtr: %v cyc: %v\nhid act: %v ges: %v gis: %v\nhid avgss: %v avgs: %v avgm: %v\nout avgs: %v avgm: %v\n", pi, qtr, ctx.Cycle, hidAct, hidGes, hidGis, hidCaM, hidCaP, hidCaD, outCaP, outCaD)
				}
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}

			hidLay.UnitValues(&hidCaP, "LearnCaP", 0)
			hidLay.UnitValues(&hidCaD, "LearnCaD", 0)

			outLay.UnitValues(&outCaP, "LearnCaP", 0)
			outLay.UnitValues(&outCaD, "LearnCaD", 0)

			if qtr == 3 {
				didx := pi
				q3hidCaD[didx] = hidCaD[pi]
				q3hidCaP[didx] = hidCaP[pi]
				q3outCaD[didx] = outCaD[pi]
				q3outCaP[didx] = outCaP[pi]
			}

			if printQtrs {
				fmt.Printf("pat: %v qtr: %v cyc: %v\nhid avgs: %v avgm: %v\nout avgs: %v avgm: %v\n", pi, qtr, ctx.Cycle, hidCaP, hidCaD, outCaP, outCaD)
			}
		}
		testNet.PlusPhaseEnd()
		if gpu {
			// testNet.GPU.SyncNeuronsFromGPU() // RLRate updated after plus
		}

		if printQtrs {
			fmt.Printf("=============================\n")
		}

		hidLay.UnitValues(&hidRLRate, "RLRate", 0)
		ridx := pi * 4
		copy(hidrlrs[ridx:ridx+4], hidRLRate)

		testNet.DWt()
		if gpu {
			// testNet.GPU.SyncSynapsesFromGPU()
		}

		didx := pi

		hiddwt[didx] = hidLay.RecvPaths[0].SynValue("DWt", pi, pi)
		outdwt[didx] = outLay.RecvPaths[0].SynValue("DWt", pi, pi)

		testNet.WtFromDWt()
		if gpu {
			// testNet.GPU.SyncSynapsesFromGPU()
		}

		hidwt[didx] = hidLay.RecvPaths[0].SynValue("Wt", pi, pi)
		outwt[didx] = outLay.RecvPaths[0].SynValue("Wt", pi, pi)
	}

	CompareFloats(tol, hidrlrs, patHidRLRates, "patHidRLRates", t)

	CompareFloats(tol, q3hidCaP, qtr3HidCaP, "qtr3HidCaP", t)
	CompareFloats(tol, q3hidCaD, qtr3HidCaD, "qtr3HidCaD", t)
	CompareFloats(tol, q3outCaP, qtr3OutCaP, "qtr3OutCaP", t)
	CompareFloats(tol, q3outCaD, qtr3OutCaD, "qtr3OutCaD", t)

	CompareFloats(tol, hiddwt, hidDwts, "hidDwts", t)
	CompareFloats(tol, outdwt, outDwts, "outDwts", t)
	CompareFloats(tol, hidwt, hidWts, "hidWts", t)
	CompareFloats(tol, outwt, outWts, "outWts", t)

	// testNet.GPU.Destroy()
}

// NetDebugLearn prints selected values (if printValues),
// and also returns a map of all values and variables that can be used for a more
// fine-grained diff test, e.g., see the GPU version.
func NetDebugLearn(t *testing.T, printValues bool, gpu bool, maxData, nData int, initWts, submean, slowAdapt bool) map[string]float32 {
	var testNet *Network
	rand.Seed(1337)

	if submean {
		testNet = newTestNetFull(maxData) // otherwise no effect
	} else {
		testNet = newTestNet(maxData)
	}
	ApplyParamSheets(testNet, layerParams["FullDecay"], pathParams["FullDecay"])
	ctx := testNet.Context()

	if submean {
		ApplyParamSheets(testNet, layerParams["SubMean"], pathParams["SubMean"])
	}

	ctx.NData = uint32(nData)
	return RunDebugLearn(t, testNet, printValues, gpu, initWts, slowAdapt)
}

// RunDebugLearn prints selected values (if printValues),
// and also returns a map of all values and variables that can be used for a more
// fine-grained diff test, e.g., see the GPU version.
func RunDebugLearn(t *testing.T, testNet *Network, printValues bool, gpu bool, initWts, slowAdapt bool) map[string]float32 {
	ctx := testNet.Context()
	nData := int(ctx.NData)
	valMap := make(map[string]float32)
	inPats := newInPats()
	inLay := testNet.LayerByName("Input")
	// hidLay := testNet.LayerByName("Hidden")
	outLay := testNet.LayerByName("Output")
	_, _ = inLay, outLay

	if gpu {
		// testNet.ConfigGPUnoGUI()
		// testNet.GPU.CycleByCycle = true // key for printing results cycle-by-cycle
	}

	// these control what is printed.
	// the whole thing is run and returned in the valMap
	valsPerRow := 8
	nPats := 4   // max 4
	stLayer := 1 // max 2
	edLayer := 2 // max 3
	nNeurs := 4  // max 4 -- number of neuron values to print
	var vals []float32

	syncAfterWt := false // shows slow adapt errors earlier if true

	for pi := 0; pi < 4; pi++ {
		if initWts {
			testNet.SetRandSeed(42) // critical for ActAvg values
			testNet.InitWeights()
		}
		testNet.ThetaCycleStart(etime.Train, false)
		testNet.MinusPhaseStart()

		testNet.InitExt()
		for di := 0; di < nData; di++ {
			ppi := (pi + di) % 4
			inpat := inPats.SubSpace(ppi)
			_ = inpat
			inLay.ApplyExt(uint32(di), inpat)
			outLay.ApplyExt(uint32(di), inpat)
		}

		testNet.ApplyExts() // key now for GPU

		for qtr := 0; qtr < 4; qtr++ {
			for cyc := 0; cyc < 50; cyc++ {
				testNet.Cycle(true)
			}
			if qtr == 2 {
				testNet.MinusPhaseEnd()
				testNet.PlusPhaseStart()
			}
		}

		testNet.PlusPhaseEnd()
		testNet.DWt()

		if syncAfterWt {
			testNet.WtFromDWt()
			if slowAdapt {
				// testNet.GPU.SyncSynCaFromGPU() // will be sent back and forth
				testNet.SlowAdapt()
			}
		}
		if gpu {
			// testNet.GPU.SyncSynapsesFromGPU()
			// testNet.GPU.SyncSynCaFromGPU()
		}

		for ni := 0; ni < 4; ni++ {
			for li := 1; li < 3; li++ {
				ly := testNet.Layers[li]
				for di := 0; di < nData; di++ {
					ppi := (pi + di) % 4
					key := fmt.Sprintf("pat: %d\tLayer: %s\tUnit: %d", ppi, ly.Name, ni)
					doPrint := (printValues && pi < nPats && ni < nNeurs && li >= stLayer && li < edLayer)
					if doPrint {
						fmt.Println(key + fmt.Sprintf("  di: %d", di))
					}
					for nvi, vnm := range NeuronVarNames {
						ly.UnitValues(&vals, vnm, di)
						vkey := key + fmt.Sprintf("\t%s", vnm)
						valMap[vkey] = vals[ni]
						if doPrint {
							fmt.Printf("\t%-10s%7.4f", vnm, vals[ni])
							if (int(nvi)+1)%valsPerRow == 0 {
								fmt.Printf("\n")
							}
						}
					}
					lnm := fmt.Sprintf("%s: di: %d", ly.Name, di)
					lpi := ly.Params.PoolIndex(0)
					poolValues(lpi, di, valMap, lnm)
					layerStates(li, di, valMap, lnm)
					if doPrint {
						fmt.Printf("\n")
					}
					for svi, snm := range SynapseVarNames {
						val := ly.RecvPaths[0].SynValDi(snm, ni, ni, di)
						vkey := key + fmt.Sprintf("\t%s", snm)
						valMap[vkey] = val
						if doPrint {
							fmt.Printf("\t%-10s%7.4f", snm, val)
							if (int(svi)+1)%valsPerRow == 0 {
								fmt.Printf("\n")
							}
						}
					}
					if doPrint {
						fmt.Printf("\n")
					}
				}
			}
		}

		if !syncAfterWt {
			testNet.WtFromDWt()
			if slowAdapt {
				testNet.SlowAdapt()
			}
			if gpu {
				// testNet.GPU.SyncSynapsesFromGPU()
				// testNet.GPU.SyncSynCaFromGPU()
			}
		}

		pi += nData - 1
	}

	// testNet.GPU.Destroy()
	return valMap
}

func TestDebugLearn(t *testing.T) {
	t.Skip("skipped in regular testing")
	NetDebugLearn(t, true, false, 2, 2, true, false, false)
}

func TestNDataLearn(t *testing.T) {
	nd1Values := NetDebugLearn(t, false, false, 1, 1, true, false, false)
	nd4Values := NetDebugLearn(t, false, false, 4, 4, true, false, false)
	ReportValDiffs(t, Tol7, nd1Values, nd4Values, "nData = 1", "nData = 4", "DWt", "ActAvg", "DTrgAvg", "TimeCycle", "LearnPeakCyc")
}

func TestNDataMaxDataLearn(t *testing.T) {
	nd84Values := NetDebugLearn(t, false, false, 8, 4, false, false, false)
	nd44Values := NetDebugLearn(t, false, false, 4, 4, false, false, false)
	ReportValDiffs(t, Tol8, nd84Values, nd44Values, "maxData = 8, nData = 4", "maxData = 4, nData = 4", "DWt", "ActAvg", "DTrgAvg")
}

func TestGPUNDataLearn(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	nd1Values := NetDebugLearn(t, false, true, 1, 1, true, false, false)
	nd4Values := NetDebugLearn(t, false, true, 4, 4, true, false, false)
	ReportValDiffs(t, Tol8, nd1Values, nd4Values, "nData = 1", "nData = 4", "DWt", "ActAvg", "DTrgAvg")
}

func TestGPUNDataMaxDataLearn(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	nd84Values := NetDebugLearn(t, false, true, 8, 4, false, false, false)
	nd44Values := NetDebugLearn(t, false, true, 4, 4, false, false, false)
	ReportValDiffs(t, Tol8, nd84Values, nd44Values, "maxData = 8, nData = 4", "maxData = 4, nData = 4", "DWt", "ActAvg", "DTrgAvg")
}

func TestGPULearnDiff(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	// fmt.Printf("\n#############\nCPU\n")
	cpuValues := NetDebugLearn(t, false, false, 1, 1, false, false, false)
	// fmt.Printf("\n#############\nGPU\n")
	gpuValues := NetDebugLearn(t, false, true, 1, 1, false, false, false)
	ReportValDiffs(t, Tol4, cpuValues, gpuValues, "CPU", "GPU")
}

func TestGPUSubMeanLearn(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	// fmt.Printf("\n#############\nCPU\n")
	cpuValues := NetDebugLearn(t, false, false, 1, 1, false, true, false)
	// fmt.Printf("\n#############\nGPU\n")
	gpuValues := NetDebugLearn(t, false, true, 1, 1, false, true, false)
	// this has bad tolerance, due to GABAB -- similar to NMDA issues
	ReportValDiffs(t, Tol3, cpuValues, gpuValues, "CPU", "GPU")
}

func TestGPUSlowAdaptLearn(t *testing.T) {
	if os.Getenv("TEST_GPU") != "true" {
		t.Skip("Set TEST_GPU env var to run GPU tests")
	}
	// fmt.Printf("\n#############\nCPU\n")
	cpuValues := NetDebugLearn(t, false, false, 1, 1, false, false, true)
	// fmt.Printf("\n#############\nGPU\n")
	gpuValues := NetDebugLearn(t, false, true, 1, 1, false, false, true)
	// this has bad tolerance, due to GABAB -- similar to NMDA issues
	ReportValDiffs(t, Tol3, cpuValues, gpuValues, "CPU", "GPU")
}

func TestInhibAct(t *testing.T) {
	tol := Tol6

	inPats := newInPats()
	inhibNet := NewNetwork("InhibNet")
	inhibNet.SetRandSeed(42) // critical for ActAvg values

	inLay := inhibNet.AddLayer("Input", InputLayer, 4, 1)
	hidLay := inhibNet.AddLayer("Hidden", SuperLayer, 4, 1)
	outLay := inhibNet.AddLayer("Output", TargetLayer, 4, 1)

	one2one := paths.NewOneToOne()

	inhibNet.ConnectLayers(inLay, hidLay, one2one, ForwardPath)
	inhibNet.ConnectLayers(inLay, hidLay, one2one, InhibPath)
	inhibNet.ConnectLayers(hidLay, outLay, one2one, ForwardPath)
	inhibNet.ConnectLayers(outLay, hidLay, one2one, BackPath)

	inhibNet.Build()
	inhibNet.Defaults()
	ApplyParamSheets(inhibNet, layerParams["Base"], pathParams["Base"])
	inhibNet.InitWeights() // get GScale
	inhibNet.ThetaCycleStart(etime.Train, false)
	inhibNet.MinusPhaseStart()

	inhibNet.InitWeights()
	inhibNet.InitExt()
	ctx := inhibNet.Context()

	printCycs := false
	printQtrs := false

	qtr0HidActs := []float32{0, 0, 0, 0}
	qtr0HidGes := []float32{0.30250484, 0, 0, 0}
	qtr0HidGis := []float32{0.46865422, 0, 0, 0}
	qtr0OutActs := []float32{0, 0, 0, 0}
	qtr0OutGes := []float32{0, 0, 0, 0}
	qtr0OutGis := []float32{0, 0, 0, 0}

	qtr3HidActs := []float32{0, 0, 0, 0}
	qtr3HidGes := []float32{0.46512103, 0, 0, 0}
	qtr3HidGis := []float32{0.44725537, 0, 0, 0}
	qtr3OutActs := []float32{0.7936507, 0, 0, 0}
	qtr3OutGes := []float32{0.8, 0, 0, 0}
	qtr3OutGis := []float32{0.39994955, 0.39994955, 0.39994955, 0.39994955}

	inActs := []float32{}
	hidActs := []float32{}
	hidGes := []float32{}
	hidGis := []float32{}
	outActs := []float32{}
	outGes := []float32{}
	outGis := []float32{}

	cycPerQtr := 50

	for pi := 0; pi < 4; pi++ {
		inpat := inPats.SubSpace(pi)
		inLay.ApplyExt(0, inpat)
		outLay.ApplyExt(0, inpat)

		inhibNet.ThetaCycleStart(etime.Train, false)
		inhibNet.MinusPhaseStart()
		for qtr := 0; qtr < 4; qtr++ {
			for cyc := 0; cyc < cycPerQtr; cyc++ {
				inhibNet.Cycle(true)

				if printCycs {
					inLay.UnitValues(&inActs, "Act", 0)
					hidLay.UnitValues(&hidActs, "Act", 0)
					hidLay.UnitValues(&hidGes, "Ge", 0)
					hidLay.UnitValues(&hidGis, "Gi", 0)
					outLay.UnitValues(&outActs, "Act", 0)
					outLay.UnitValues(&outGes, "Ge", 0)
					outLay.UnitValues(&outGis, "Gi", 0)
					fmt.Printf("pat: %v qtr: %v cyc: %v\nin acts: %v\nhid acts: %v ges: %v gis: %v\nout acts: %v ges: %v gis: %v\n", pi, qtr, cyc, inActs, hidActs, hidGes, hidGis, outActs, outGes, outGis)
				}
			}
			if qtr == 2 {
				inhibNet.MinusPhaseEnd()
				inhibNet.PlusPhaseStart()
			}

			if printCycs && printQtrs {
				fmt.Printf("=============================\n")
			}

			inLay.UnitValues(&inActs, "Act", 0)
			hidLay.UnitValues(&hidActs, "Act", 0)
			hidLay.UnitValues(&hidGes, "Ge", 0)
			hidLay.UnitValues(&hidGis, "Gi", 0)
			outLay.UnitValues(&outActs, "Act", 0)
			outLay.UnitValues(&outGes, "Ge", 0)
			outLay.UnitValues(&outGis, "Gi", 0)

			if printQtrs {
				fmt.Printf("pat: %v qtr: %v cyc: %v\nin acts: %v\nhid acts: %v ges: %v gis: %v\nout acts: %v ges: %v gis: %v\n", pi, qtr, ctx.Cycle, inActs, hidActs, hidGes, hidGis, outActs, outGes, outGis)
			}

			if printCycs && printQtrs {
				fmt.Printf("=============================\n")
			}

			if pi == 0 && qtr == 0 {
				CompareFloats(tol, hidActs, qtr0HidActs, "qtr0HidActs", t)
				CompareFloats(tol, hidGes, qtr0HidGes, "qtr0HidGes", t)
				CompareFloats(tol, hidGis, qtr0HidGis, "qtr0HidGis", t)
				CompareFloats(tol, outActs, qtr0OutActs, "qtr0OutActs", t)
				CompareFloats(tol, outGes, qtr0OutGes, "qtr0OutGes", t)
				CompareFloats(tol, outGis, qtr0OutGis, "qtr0OutGis", t)
			}
			if pi == 0 && qtr == 3 {
				CompareFloats(tol, hidActs, qtr3HidActs, "qtr3HidActs", t)
				CompareFloats(tol, hidGes, qtr3HidGes, "qtr3HidGes", t)
				CompareFloats(tol, hidGis, qtr3HidGis, "qtr3HidGis", t)
				CompareFloats(tol, outActs, qtr3OutActs, "qtr3OutActs", t)
				CompareFloats(tol, outGes, qtr3OutGes, "qtr3OutGes", t)
				CompareFloats(tol, outGis, qtr3OutGis, "qtr3OutGis", t)
			}
		}
		inhibNet.PlusPhaseEnd()

		if printQtrs {
			fmt.Printf("=============================\n")
		}
	}
}

func saveToFile(net *Network, t *testing.T) {
	var buf bytes.Buffer
	net.WriteWeightsJSON(&buf)
	wb := buf.Bytes()
	fmt.Printf("testNet Trained Weights:\n\n%v\n", string(wb))

	fp, err := os.Create("testdata/testnet_train.wts")
	defer fp.Close()
	if err != nil {
		t.Error(err)
	}
	fp.Write(wb)
}

func TestSendGatherIndexes(t *testing.T) {
	nData := uint32(3)
	net := newTestNet(int(nData))

	maxDel := net.NetIxs().MaxDelay + 1
	maxCyc := int32(2 * maxDel)
	nni := net.NetIxs().NNeurons

	type vals struct {
		cyc int32
		ri  uint32
		bi  uint32
	}

	bimap := make(map[uint32]string)
	bivals := make(map[uint32][]vals)
	rimap := make(map[uint32]string)

	for cyc := int32(0); cyc < maxCyc; cyc++ {
		for ni := uint32(0); ni < nni; ni++ {
			for di := uint32(0); di < nData; di++ {
				li := NeuronIxs[ni, NrnLayIndex]
				ly := net.Layers[li]
				if len(ly.SendPaths) > 0 {
					ptt := ly.SendPaths[0]
					pt := ptt.Params
					deli := pt.Com.WriteOff(int32(cyc))
					_ = deli
					scon := ptt.SendCon[ni-ptt.Send.NeurStIndex]
					for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
						syni := ptt.SynStIndex + syi
						recvIndex := pt.SynRecvLayerIndex(syni) // note: layer-specific is ok here
						ri := SynapseIxs[syni, SynRecvIndex]
						_ = ri
						npti := pt.Indexes.NPathNeurSt + recvIndex
						_ = npti
						// bio := pt.Indexes.GBufSt + pjcom.WriteIndexOff(recvIndex, di, wrOff, pt.Indexes.RecvNeurN, nData)
						// bi := pt.Indexes.GBufSt + pjcom.WriteIndex(recvIndex, di, int32(cyc), pt.Indexes.RecvNeurN, nData)
						// sidx := syni*nData + di
						// key := fmt.Sprintf("send: cyc: %d  synidx: %03d  bi: %03d  si: %02d  di: %d  ri: %02d\n", cyc, sidx, bi, ni, di, ri)
						// if bio != bi {
						// 	t.Errorf("writeIndex %d != WriteIndexOff %d: %s\n", bi, bio, key)
						// }

						// vl, ok := bivals[bi]
						// vl = append(vl, vals{cyc: cyc, ri: ri, bi: bi})
						// bivals[bi] = vl
// 
						// cur, ok := bimap[bi]
						// if !ok {
						// 	bimap[bi] = key
						// } else {
						// 	bimap[bi] = cur + key
						// }
						// cur, ok = rimap[ri]
						// if !ok {
						// 	rimap[ri] = key
						// } else {
						// 	rimap[ri] = cur + key
						// }
						// if pj.Send.Index == 42 && pj.Recv.Index == 44 && di == 0 && ri == 2783 {
						// 	fmt.Printf("send: cyc: %d  bi: %d  di: %02d  ni: %04d  scale: %g  sv: %d\n", ctx.CyclesTotal, bi, di, ni, scale, sv)
						// }
					}
				}
				if len(ly.RecvPaths) > 0 {
					// lni := ni - ly.NeurStIndex
					// pj := ly.RecvPaths[0]
					// bi := pt.Indexes.GBufSt + pt.Com.ReadIndex(lni, di, int32(cyc), pt.Indexes.RecvNeurN, nData)
					// key := fmt.Sprintf("recv: cyc: %d  bi: %03d  di: %d  ri: %02d\n", cyc, bi, di, ni)
// 
					// vl, ok := bivals[bi]
					// vl = append(vl, vals{cyc: cyc, ri: ni, bi: bi})
					// bivals[bi] = vl
// 
					// cur, ok := bimap[bi]
					// if !ok {
					// 	bimap[bi] = key
					// } else {
					// 	bimap[bi] = cur + key
					// }
					// cur, ok = rimap[ni]
					// if !ok {
					// 	rimap[ni] = key
					// } else {
					// 	rimap[ni] = cur + key
					// }
				}
			}
		}
	}

	maxDeli := int32(maxDel)
	for _, vls := range bivals {
		fvl := vls[0]
		for _, vl := range vls {
			if vl.ri != fvl.ri {
				t.Errorf("recv index mismatch -- each bi must be for 1 ri: first: %#v  cur: %#v\n", vl, fvl)
			}
			if vl.cyc%maxDeli != fvl.cyc%maxDeli {
				t.Errorf("cyc mismatch -- must be multiple of maxDel: %d %#v  cur: %#v\n", maxDel, vl, fvl)
			}
		}
	}

	if false { // print for human consumption
		keys := maps.Keys(bimap)
		sort.Slice(keys, func(i, j int) bool {
			return keys[i] < keys[j]
		})
		for i, bi := range keys {
			fmt.Printf("%d  bi: %d\n%s\n", i, bi, bimap[bi])
		}

		keys = maps.Keys(rimap)
		sort.Slice(keys, func(i, j int) bool {
			return keys[i] < keys[j]
		})
		for i, ri := range keys {
			fmt.Printf("%d  ri: %d\n%s\n", i, ri, rimap[ri])
		}
	}
}

func TestRubiconGiveUp(t *testing.T) {
	t.Skip("")
	gp := &GiveUpParams{}
	gp.Defaults()
	rnd := randx.NewGlobalRand()
	for v := float32(-1.0); v <= float32(1); v += 0.01 {
		p, b := gp.Prob(v, 1, rnd)
		fmt.Printf("%g\tp: %g\tb: %v\n", v, p, b)
	}
}

// func TestGateSync(t *testing.T) {
// 	newTestNet(1)
// 	minusBins := int32(6)
// 	plusBins := int32(2)
// 	ni := uint32(0)
// 	di := uint32(0)
// 	for i := range 8 {
// 		Neurons[ni, di, CaBin0 + NeuronVars(i)] = float32(i)
// 	}
// 	ly := &Layers[0]
// 	ly.Learn.GateSync.ShiftBins(1, minusBins, plusBins, ni, di)
// 	for i := range minusBins {
// 		// fmt.Println(i, Neurons[ni, di, CaBin0 + NeuronVars(i)])
// 		assert.Equal(t, float32(i+1), Neurons[ni, di, CaBin0 + NeuronVars(i)])
// 	}
// 	for i := range plusBins {
// 		assert.Equal(t, float32(0), Neurons[ni, di, CaBin0 + NeuronVars(minusBins+i)])
// 	}
// 	ly.Learn.GateSync.ShiftBins(-1, 6, 2, ni, di)
// 	for i := range minusBins {
// 		// fmt.Println(i, Neurons[ni, di, CaBin0 + NeuronVars(i)])
// 		assert.Equal(t, float32(max(i,1)), Neurons[ni, di, CaBin0 + NeuronVars(i)])
// 	}
// 	for i := range plusBins {
// 		assert.Equal(t, float32(0), Neurons[ni, di, CaBin0 + NeuronVars(minusBins+i)])
// 	}
// }

// func TestSWtInit(t *testing.T) {
// 	pj := &PathParams{}
// 	pj.Defaults()
// 
// 	nsamp := 100
// 	dt := table.New()
// 	dt.AddFloat32Column("Wt")
// 	dt.AddFloat32Column("LWt")
// 	dt.AddFloat32Column("SWt")
// 	dt.SetNumRows(nsamp)
// 
// 	/////////////////////////////////////////////
// 	mean := float32(0.5)
// 	vr := float32(0.25)
// 	spct := float32(0.5)
// 	pj.SWts.Init.Var = vr
// 
// 	nt := NewNetwork("test")
// 	nt.SetRandSeed(1)
// 
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 	for i := 0; i < nsamp; i++ {
// 		pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 		dt.SetFloat("Wt", i, float64(sy.Wt))
// 		dt.SetFloat("LWt", i, float64(sy.LWt))
// 		dt.SetFloat("SWt", i, float64(sy.SWt))
// 	}
// 	ix := table.NewIndexView(dt)
// 	desc := stats.DescAll(ix)
// 
// 	meanRow := desc.RowsByString("Agg", "Mean", table.Equals, table.UseCase)[0]
// 	minRow := desc.RowsByString("Agg", "Min", table.Equals, table.UseCase)[0]
// 	maxRow := desc.RowsByString("Agg", "Max", table.Equals, table.UseCase)[0]
// 	semRow := desc.RowsByString("Agg", "Sem", table.Equals, table.UseCase)[0]
// 
// 	if desc.Float("Wt", minRow) > 0.3 || desc.Float("Wt", maxRow) < 0.7 {
// 		t.Errorf("SPct: %g\t Wt Min and Max should be < 0.3, > 0.7 not: %g, %g\n", spct, desc.Float("Wt", minRow), desc.Float("Wt", maxRow))
// 	}
// 	if desc.Float("Wt", meanRow) < 0.45 || desc.Float("Wt", meanRow) > 0.55 {
// 		t.Errorf("SPct: %g\t Wt Mean should be > 0.45, < 0.55 not: %g\n", spct, desc.Float("Wt", meanRow))
// 	}
// 	if desc.Float("Wt", semRow) < 0.01 || desc.Float("Wt", semRow) > 0.02 {
// 		t.Errorf("SPct: %g\t Wt SEM should be > 0.01, < 0.02 not: %g\n", spct, desc.Float("Wt", semRow))
// 	}
// 
// 	// b := bytes.NewBuffer(nil)
// 	// desc.WriteCSV(b, table.Tab, table.Headers)
// 	// fmt.Printf("%s\n", string(b.Bytes()))
// 
// 	/////////////////////////////////////////////
// 	mean = float32(0.5)
// 	vr = float32(0.25)
// 	spct = float32(1.0)
// 	pj.SWts.Init.Var = vr
// 
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 	for i := 0; i < nsamp; i++ {
// 		pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 		dt.SetFloat("Wt", i, float64(sy.Wt))
// 		dt.SetFloat("LWt", i, float64(sy.LWt))
// 		dt.SetFloat("SWt", i, float64(sy.SWt))
// 	}
// 	desc = stats.DescAll(ix)
// 	if desc.Float("Wt", minRow) > 0.3 || desc.Float("Wt", maxRow) < 0.7 {
// 		t.Errorf("SPct: %g\t Wt Min and Max should be < 0.3, > 0.7 not: %g, %g\n", spct, desc.Float("Wt", minRow), desc.Float("Wt", maxRow))
// 	}
// 	if desc.Float("Wt", meanRow) < 0.45 || desc.Float("Wt", meanRow) > 0.55 {
// 		t.Errorf("SPct: %g\t Wt Mean should be > 0.45, < 0.55 not: %g\n", spct, desc.Float("Wt", meanRow))
// 	}
// 	if desc.Float("Wt", semRow) < 0.01 || desc.Float("Wt", semRow) > 0.02 {
// 		t.Errorf("SPct: %g\t Wt SEM should be > 0.01, < 0.02 not: %g\n", spct, desc.Float("Wt", semRow))
// 	}
// 	if desc.Float("LWt", minRow) != 0.5 || desc.Float("LWt", maxRow) != 0.5 {
// 		t.Errorf("SPct: %g\t LWt Min and Max should both be 0.5, not: %g, %g\n", spct, desc.Float("LWt", minRow), desc.Float("LWt", maxRow))
// 	}
// 	// b.Reset()
// 	// desc.WriteCSV(b, table.Tab, table.Headers)
// 	// fmt.Printf("%s\n", string(b.Bytes()))
// 
// 	/////////////////////////////////////////////
// 	mean = float32(0.5)
// 	vr = float32(0.25)
// 	spct = float32(0.0)
// 	pj.SWts.Init.Var = vr
// 
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 	for i := 0; i < nsamp; i++ {
// 		pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 		dt.SetFloat("Wt", i, float64(sy.Wt))
// 		dt.SetFloat("LWt", i, float64(sy.LWt))
// 		dt.SetFloat("SWt", i, float64(sy.SWt))
// 	}
// 	desc = stats.DescAll(ix)
// 	if desc.Float("Wt", minRow) > 0.3 || desc.Float("Wt", maxRow) < 0.7 {
// 		t.Errorf("SPct: %g\t Wt Min and Max should be < 0.3, > 0.7 not: %g, %g\n", spct, desc.Float("Wt", minRow), desc.Float("Wt", maxRow))
// 	}
// 	if desc.Float("Wt", meanRow) < 0.45 || desc.Float("Wt", meanRow) > 0.55 {
// 		t.Errorf("SPct: %g\t Wt Mean should be > 0.45, < 0.55 not: %g\n", spct, desc.Float("Wt", meanRow))
// 	}
// 	if desc.Float("Wt", semRow) < 0.01 || desc.Float("Wt", semRow) > 0.02 {
// 		t.Errorf("SPct: %g\t Wt SEM should be > 0.01, < 0.02 not: %g\n", spct, desc.Float("Wt", semRow))
// 	}
// 	if desc.Float("SWt", minRow) != 0.5 || desc.Float("SWt", maxRow) != 0.5 {
// 		t.Errorf("SPct: %g\t SWt Min and Max should both be 0.5, not: %g, %g\n", spct, desc.Float("LWt", minRow), desc.Float("LWt", maxRow))
// 	}
// 	// b.Reset()
// 	// desc.WriteCSV(b, table.Tab, table.Headers)
// 	// fmt.Printf("%s\n", string(b.Bytes()))
// 
// 	/////////////////////////////////////////////
// 	mean = float32(0.1)
// 	vr = float32(0.05)
// 	spct = float32(0.0)
// 	pj.SWts.Init.Var = vr
// 
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 	for i := 0; i < nsamp; i++ {
// 		pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 		dt.SetFloat("Wt", i, float64(sy.Wt))
// 		dt.SetFloat("LWt", i, float64(sy.LWt))
// 		dt.SetFloat("SWt", i, float64(sy.SWt))
// 	}
// 	desc = stats.DescAll(ix)
// 	if desc.Float("Wt", minRow) > 0.08 || desc.Float("Wt", maxRow) < 0.12 {
// 		t.Errorf("SPct: %g\t Wt Min and Max should be < 0.08, > 0.12 not: %g, %g\n", spct, desc.Float("Wt", minRow), desc.Float("Wt", maxRow))
// 	}
// 	if desc.Float("Wt", meanRow) < 0.08 || desc.Float("Wt", meanRow) > 0.12 {
// 		t.Errorf("SPct: %g\t Wt Mean should be > 0.08, < 0.12 not: %g\n", spct, desc.Float("Wt", meanRow))
// 	}
// 	if desc.Float("SWt", minRow) != 0.5 || desc.Float("SWt", maxRow) != 0.5 {
// 		t.Errorf("SPct: %g\t SWt Min and Max should both be 0.5, not: %g, %g\n", spct, desc.Float("LWt", minRow), desc.Float("LWt", maxRow))
// 	}
// 	// b.Reset()
// 	// desc.WriteCSV(b, table.Tab, table.Headers)
// 	// fmt.Printf("%s\n", string(b.Bytes()))
// 
// 	/////////////////////////////////////////////
// 	mean = float32(0.8)
// 	vr = float32(0.05)
// 	spct = float32(0.5)
// 	pj.SWts.Init.Var = vr
// 
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 	for i := 0; i < nsamp; i++ {
// 		pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 		dt.SetFloat("Wt", i, float64(sy.Wt))
// 		dt.SetFloat("LWt", i, float64(sy.LWt))
// 		dt.SetFloat("SWt", i, float64(sy.SWt))
// 	}
// 	desc = stats.DescAll(ix)
// 	if desc.Float("Wt", minRow) > 0.76 || desc.Float("Wt", maxRow) < 0.84 {
// 		t.Errorf("SPct: %g\t Wt Min and Max should be < 0.66, > 0.74 not: %g, %g\n", spct, desc.Float("Wt", minRow), desc.Float("Wt", maxRow))
// 	}
// 	if desc.Float("Wt", meanRow) < 0.79 || desc.Float("Wt", meanRow) > 0.81 {
// 		t.Errorf("SPct: %g\t Wt Mean should be > 0.65, < 0.75 not: %g\n", spct, desc.Float("Wt", meanRow))
// 	}
// 	if desc.Float("SWt", minRow) < 0.76 || desc.Float("SWt", maxRow) > 0.83 {
// 		t.Errorf("SPct: %g\t SWt Min and Max should be < 0.76, > 0.83, not: %g, %g\n", spct, desc.Float("SWt", minRow), desc.Float("SWt", maxRow))
// 	}
// 	// b.Reset()
// 	// desc.WriteCSV(b, table.Tab, table.Headers)
// 	// fmt.Printf("%s\n", string(b.Bytes()))
// }
// 
// 
// func TestSWtLinLearn(t *testing.T) {
// 	pj := &PathParams{}
// 	pj.Defaults()
// 	sy := &Synapse{}
// 
// 	nt := NewNetwork("test")
// 	nt.SetRandSeed(1)
// 
// 	/////////////////////////////////////////////
// 	mean := float32(0.1)
// 	vr := float32(0.05)
// 	spct := float32(0.0)
// 	dwt := float32(0.1)
// 	pj.SWts.Init.Var = vr
// 	pj.SWts.Adapt.SigGain = 1
// 	nlrn := 10
// 	// fmt.Printf("Wts Mean: %g\t Var: %g\t SPct: %g\n", mean, vr, spct)
// 
// 	pj.SWts.InitWeightsSyn(&nt.Rand, sy, mean, spct)
// 	// fmt.Printf("Wt: %g\t LWt: %g\t SWt: %g\n", sy.Wt, sy.LWt, sy.SWt)
// 	for i := 0; i < nlrn; i++ {
// 		sy.DWt = dwt
// 		pj.SWts.WtFromDWt(&sy.DWt, &sy.Wt, &sy.LWt, sy.SWt)
// 		// fmt.Printf("Wt: %g\t LWt: %g\t SWt: %g\n", sy.Wt, sy.LWt, sy.SWt)
// 	}
// 	if sy.Wt != 1 {
// 		t.Errorf("SPct: %g\t Wt should be 1 not: %g\n", spct, sy.Wt)
// 	}
// 	if sy.LWt != 1 {
// 		t.Errorf("SPct: %g\t LWt should be 1 not: %g\n", spct, sy.LWt)
// 	}
// 	if sy.SWt != 0.5 {
// 		t.Errorf("SPct: %g\t SWt should be 0.5 not: %g\n", spct, sy.SWt)
// 	}
// }

