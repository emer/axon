// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"log"

	"cogentcore.org/core/math32"
	"cogentcore.org/core/math32/minmax"
)

// index naming:
// lni = layer-based neuron index (0 = first neuron in layer)
// ni  = absolute network-level neuron index

// layer-cpu.goal has supporting CPU-only algorithm methods 
// layerparams.goal has core GPU / CPU code.

////////  Phase-level

// todo: all of this could be moved to layer params:

// NewState handles all initialization at start of new input pattern.
// Does NOT call InitGScale()
func (ly *Layer) NewState(ctx *Context) {
	nn := ly.NNeurons
	np := ly.NPools

	actMinusAvg := float32(0)
	actPlusAvg := float32(0)

	for di := uint32(0); di < ctx.NData; di++ {
		lpi := ly.Params.PoolIndex(0)

		actMinusAvg += PoolAvgMax(AMAct, AMMinus, Avg, lpi, di)
		actPlusAvg += PoolAvgMax(AMAct, AMPlus, Avg, lpi, di)

		ly.Params.NewStateLayer(ctx, di)

		for spi := uint32(0); spi < np; spi++ {
			pi := ly.Params.PoolIndex(spi)
			ly.Params.NewStatePool(ctx, pi, di) // also calls DecayState on pool
		}

		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			if NrnIsOff(ni) {
				continue
			}
			// note: this calls the basic neuron-level DecayState
			ly.Params.NewStateNeuron(ctx, ni, di)
		}
	}

	// note: long-running averages must be based on aggregate data, drive adaptation
	// of Gi layer inhibition.
	davg := 1 / float32(ctx.NData)
	actMinusAvg *= davg
	actPlusAvg *= davg
	for di := uint32(0); di < ctx.NData; di++ {
		ly.Params.NewStateLayerActAvg(ctx, di, actMinusAvg, actPlusAvg)
	}

	// note: would be somewhat more expensive to only clear the di specific subset
	// but all di are decayed every trial anyway so no big deal
	ly.InitPathGBuffs(ctx)
}

// NewStateNeurons only calls the neurons part of new state -- for misbehaving GPU
func (ly *Layer) NewStateNeurons(ctx *Context) {
	nn := ly.NNeurons
	for di := uint32(0); di < ctx.NData; di++ {
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			// note: this calls the basic neuron-level DecayState
			ly.Params.NewStateNeuron(ctx, ni, di)
		}
	}
}

// DecayState decays activation state by given proportion
// (default decay values are ly.Params.Acts.Decay.Act, Glong)
func (ly *Layer) DecayState(ctx *Context, di uint32, decay, glong, ahp float32) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		// Note: synapse-level Ca decay happens in DWt
		if ahp == 1 {
			lt := ly.Type
			if lt == PTMaintLayer {
				Neurons[CtxtGe, ni, di] = 0
				Neurons[CtxtGeRaw, ni, di] = 0
				Neurons[CtxtGeOrig, ni, di] = 0
			}
		}
	}
	ly.DecayStateLayer(ctx, di, decay, glong, ahp)
}

// DecayStateLayer does layer-level decay, but not neuron level
func (ly *Layer) DecayStateLayer(ctx *Context, di uint32, decay, glong, ahp float32) {
	np := ly.NPools
	for spi := uint32(0); spi < np; spi++ {
		pi := ly.Params.PoolIndex(spi)
		PoolInhibDecay(pi, di, decay)
	}
}

// DecayStatePool decays activation state by given proportion in given sub-pool index (0 based)
func (ly *Layer) DecayStatePool(ctx *Context, pool int, decay, glong, ahp float32) {
	spi := uint32(pool + 1) // 1 based
	for di := uint32(0); di < ctx.NData; di++ {
		pi := ly.Params.PoolIndex(spi)
		nsi := PoolsInt[PoolNeurSt, pi, di]
		nei := PoolsInt[PoolNeurEd, pi, di]
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NrnIsOff(ni) {
				continue
			}
			ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		}
		PoolInhibDecay(pi, di, decay)
	}
}

// DecayStateNeuronsAll decays neural activation state by given proportion
// (default decay values are ly.Params.Acts.Decay.Act, Glong, AHP)
// for all data parallel indexes. Does not decay pool or layer state.
// This is used for minus phase of Pulvinar layers to clear state in prep
// for driver plus phase.
func (ly *Layer) DecayStateNeuronsAll(ctx *Context, decay, glong, ahp float32) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		}
	}
}

// AvgMaxVarByPool returns the average and maximum value of given variable
// for given pool index (0 = entire layer, 1.. are subpools for 4D only).
// Uses fast index-based variable access.
func (ly *Layer) AvgMaxVarByPool(ctx *Context, varNm string, poolIndex, di int) minmax.AvgMax32 {
	var am minmax.AvgMax32
	vidx, err := ly.UnitVarIndex(varNm)
	if err != nil {
		log.Printf("axon.Layer.AvgMaxVar: %s\n", err)
		return am
	}
	pi := ly.Params.PoolIndex(uint32(poolIndex))
	nsi := PoolsInt[PoolNeurSt, pi, di]
	nei := PoolsInt[PoolNeurEd, pi, di]
	am.Init()
	for lni := nsi; lni < nei; lni++ {
		ni := ly.NeurStIndex + uint32(lni)
		if NrnIsOff(ni) {
			continue
		}
		vl := ly.UnitValue1D(vidx, int(ni), di)
		am.UpdateValue(vl, int32(ni))
	}
	am.CalcAvg()
	return am
}

// MinusPhasePost does special algorithm processing at end of minus
func (ly *Layer) MinusPhasePost(ctx *Context) {
	switch ly.Type {
	case MatrixLayer:
		ly.MatrixGated(ctx) // need gated state for decisions about action processing, so do in minus too
	case PulvinarLayer:
		ly.DecayStateNeuronsAll(ctx, 1, 1, 0)
	}
}

// PlusPhaseStart does updating at the start of the plus phase:
// applies Target inputs as External inputs.
func (ly *Layer) PlusPhaseStart(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			ly.Params.PlusPhaseStartNeuron(ctx, ni, di)
		}
	}
}

// PlusPhase does updating at end of the plus phase
func (ly *Layer) PlusPhase(ctx *Context) {
	// todo: see if it is faster to just grab pool info now, then do everything below on CPU
	np := ly.NPools
	for spi := uint32(0); spi < np; spi++ { // gpu_cycletoplus
		for di := uint32(0); di < ctx.NData; di++ {
			pi := ly.Params.PoolIndex(spi)
			ly.Params.PlusPhasePool(ctx, pi, di)
		}
	}
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			lpi := ly.Params.PoolIndex(0)
			pi := ly.Params.PoolIndex(NeuronIxs[NrnSubPool, ni])
			ly.Params.PlusPhaseNeuron(ctx, lpi, pi, ni, di)
		}
	}
}

// PlusPhasePost does special algorithm processing at end of plus
func (ly *Layer) PlusPhasePost(ctx *Context) {
	ly.PlusPhaseActAvg(ctx)
	ly.PhaseDiffFromActs(ctx) // GPU syncs down the state before this
	np := ly.NPools
	if ly.Type == PTMaintLayer && ly.Name == "OFCposPT" {
		for spi := uint32(1); spi < np; spi++ {
			for di := uint32(0); di < ctx.NData; di++ {
				pi := ly.Params.PoolIndex(spi)
				val := PoolAvgMax(AMCaSpkD, AMCycle, Avg, pi, di)
				GlobalVectors[GvOFCposPTMaint, uint32(pi-1), di] = val
			}
		}
	}

	if ly.Params.Acts.Decay.OnRew.IsTrue() {
		for di := uint32(0); di < ctx.NData; di++ {
			hasRew := (GlobalScalars[GvHasRew, di] > 0)
			giveUp := (GlobalScalars[GvGiveUp, di] > 0)
			if hasRew || giveUp {
				ly.DecayState(ctx, di, 1, 1, 1)      // note: GPU will get, and GBuf are auto-cleared in NewState
				for spi := uint32(0); spi < np; spi++ { // also clear the pool stats: GoalMaint depends on these..
					pi := ly.Params.PoolIndex(spi)
					PoolAvgMaxZero(pi, di)
				}
			}
		}
	}
	switch ly.Type {
	case MatrixLayer:
		ly.MatrixGated(ctx)
	}
}

// PlusPhaseActAvg updates ActAvg and DTrgAvg at the plus phase
// Note: could be done on GPU but not worth it at this point..
func (ly *Layer) PlusPhaseActAvg(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		dTrgSum := float32(0)
		avgSum := float32(0)
		for di := uint32(0); di < ctx.NData; di++ {
			dTrgSum += ly.Params.LearnTrgAvgErrLRate() * (Neurons[CaSpkP, ni, di] - Neurons[CaSpkD, ni, di])
			avgSum += ly.Params.Acts.Dt.LongAvgDt * (Neurons[ActM, ni, di] - NeuronAvgs[ActAvg, ni])
		}
		NeuronAvgs[DTrgAvg, ni] += dTrgSum
		NeuronAvgs[ActAvg, ni] += avgSum
	}
}

// TargToExt sets external input Ext from target values Target
// This is done at end of MinusPhase to allow targets to drive activity in plus phase.
// This can be called separately to simulate alpha cycles within theta cycles, for example.
func (ly *Layer) TargToExt(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			if !NrnHasFlag(ni, di, NeuronHasTarg) { // will be clamped in plus phase
				continue
			}
			Neurons[Ext, ni, di] =  Neurons[Target, ni, di]
			NrnSetFlag(ni, di, NeuronHasExt)
			Neurons[ISI, ni, di] = -1 // get fresh update on plus phase output acts
			Neurons[ISIAvg, ni, di] = -1
		}
	}
}

// ClearTargExt clears external inputs Ext that were set from target values Target.
// This can be called to simulate alpha cycles within theta cycles, for example.
func (ly *Layer) ClearTargExt(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			if !NrnHasFlag(ni, di, NeuronHasTarg) { // will be clamped in plus phase
				continue
			}
			Neurons[Ext, ni, di] = 0
			NrnClearFlag(ni, di, NeuronHasExt)
			Neurons[ISI, ni, di] = -1 // get fresh update on plus phase output acts
			Neurons[ISIAvg, ni, di] = -1
		}
	}
}

// SpkSt1 saves current activation state in SpkSt1 variables (using CaP)
func (ly *Layer) SpkSt1(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			Neurons[SpkSt1, ni, di] = Neurons[CaSpkP, ni, di]
		}
	}
}

// SpkSt2 saves current activation state in SpkSt2 variables (using CaP)
func (ly *Layer) SpkSt2(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			Neurons[SpkSt2, ni, di] = Neurons[CaSpkP, ni, di]
		}
	}
}

// PhaseDiffFromActs computes the phase-wise difference in the
// activity state between the minus [ActM] and plus [ActP] phases,
// measured using 1 minus the correlation (centered cosine aka
// normalized dot product).  0 = no difference, 2 = maximum difference.
func (ly *Layer) PhaseDiffFromActs(ctx *Context) {
	li := ly.Index
	for di := uint32(0); di < ctx.NData; di++ {
		lpi := ly.Params.PoolIndex(0)
		avgM := PoolAvgMax(AMAct, AMMinus, Avg, lpi, di)
		avgP := PoolAvgMax(AMAct, AMPlus, Avg, lpi, di)
		cosv := float32(0)
		ssm := float32(0)
		ssp := float32(0)
		nn := ly.NNeurons
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			if NrnIsOff(ni) {
				continue
			}
			ap := Neurons[ActP, ni, di] - avgP // zero mean = correl
			am := Neurons[ActM, ni, di] - avgM
			cosv += ap * am
			ssm += am * am
			ssp += ap * ap
		}
		dist := math32.Sqrt(ssm * ssp)
		if dist != 0 {
			cosv /= dist
		}
		LayerStates[LayerPhaseDiff, li, di] = 1 - cosv
		avg := LayerStates[LayerPhaseDiffAvg, li, di]
		vr := LayerStates[LayerPhaseDiffVar, li, di]
		ly.Params.Acts.Dt.AvgVarUpdate(&avg, &vr, 1-cosv)
		LayerStates[LayerPhaseDiffAvg, li, di] = avg
		LayerStates[LayerPhaseDiffVar, li, di] = vr
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  Learning

// DWt computes the weight change (learning), based on
// synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (ly *Layer) DWt(ctx *Context, si uint32) {
	for _, pj := range ly.SendPaths {
		if pj.Off {
			continue
		}
		pj.DWt(ctx, si)
	}
}

// DWtSubMean computes subtractive normalization of the DWts
func (ly *Layer) DWtSubMean(ctx *Context, ri uint32) {
	for _, pj := range ly.RecvPaths {
		if pj.Off {
			continue
		}
		pj.DWtSubMean(ctx, ri)
	}
}

// WtFromDWt updates weight values from delta weight changes
func (ly *Layer) WtFromDWt(ctx *Context, si uint32) {
	for _, pj := range ly.SendPaths {
		if pj.Off {
			continue
		}
		pj.WtFromDWt(ctx, si)
	}
}

// DTrgSubMean subtracts the mean from DTrgAvg values
// Called by TrgAvgFromD
func (ly *Layer) DTrgSubMean(ctx *Context) {
	submean := ly.Params.Learn.TrgAvgAct.SubMean
	if submean == 0 {
		return
	}
	if ly.HasPoolInhib() && ly.Params.Learn.TrgAvgAct.Pool.IsTrue() {
		np := ly.NPools
		for spi := uint32(1); spi < np; spi++ {
			pi := ly.Params.PoolIndex(spi) // only for idxs
			nsi := PoolsInt[PoolNeurSt, pi, 0]
			nei := PoolsInt[PoolNeurEd, pi, 0]
			nn := 0
			avg := float32(0)
			for lni := nsi; lni < nei; lni++ {
				ni := ly.NeurStIndex + uint32(lni)
				if NrnIsOff(ni) {
					continue
				}
				avg += NeuronAvgs[DTrgAvg, ni]
				nn++
			}
			if nn == 0 {
				continue
			}
			avg /= float32(nn)
			avg *= submean
			for lni := nsi; lni < nei; lni++ {
				ni := ly.NeurStIndex + uint32(lni)
				if NrnIsOff(ni) {
					continue
				}
				NeuronAvgs[DTrgAvg, ni] -= avg
			}
		}
	} else {
		nn := 0
		avg := float32(0)
		for lni := uint32(0); lni < ly.NNeurons; lni++ {
			ni := ly.NeurStIndex + lni
			if NrnIsOff(ni) {
				continue
			}
			avg += NeuronAvgs[DTrgAvg, ni]
			nn++
		}
		if nn == 0 {
			return
		}
		avg /= float32(nn)
		avg *= submean
		for lni := uint32(0); lni < ly.NNeurons; lni++ {
			ni := ly.NeurStIndex + lni
			if NrnIsOff(ni) {
				continue
			}
			NeuronAvgs[DTrgAvg, ni] -= avg
		}
	}
}

// TrgAvgFromD updates TrgAvg from DTrgAvg -- called in PlusPhasePost
func (ly *Layer) TrgAvgFromD(ctx *Context) {
	lr := ly.Params.LearnTrgAvgErrLRate()
	if lr == 0 {
		return
	}
	ly.DTrgSubMean(ctx)
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		ntrg := NeuronAvgs[TrgAvg, ni] + NeuronAvgs[DTrgAvg, ni]
		ntrg = ly.Params.Learn.TrgAvgAct.TrgRange.ClipValue(ntrg)
		NeuronAvgs[TrgAvg, ni] = ntrg
		NeuronAvgs[DTrgAvg, ni] = 0
	}
}

// WtFromDWtLayer does weight update at the layer level.
// does NOT call main pathway-level WtFromDWt method.
// in base, only calls TrgAvgFromD
func (ly *Layer) WtFromDWtLayer(ctx *Context) {
	ly.TrgAvgFromD(ctx)
}

// SlowAdapt is the layer-level slow adaptation functions.
// Calls AdaptInhib and AvgDifFromTrgAvg for Synaptic Scaling.
// Does NOT call pathway-level methods.
func (ly *Layer) SlowAdapt(ctx *Context) {
	ly.AdaptInhib(ctx)
	ly.AvgDifFromTrgAvg(ctx)
	// note: path level call happens at network level
}

// AdaptInhib adapts inhibition
func (ly *Layer) AdaptInhib(ctx *Context) {
	if ly.Params.Inhib.ActAvg.AdaptGi.IsFalse() || ly.Params.IsInput() {
		return
	}
	for di := uint32(0); di < ctx.NData; di++ {
		giMult := LayerStates[LayerGiMult, ly.Index, di]
		avg := LayerStates[LayerActMAvg, ly.Index, di]
		ly.Params.Inhib.ActAvg.Adapt(&giMult, avg)
		LayerStates[LayerGiMult, ly.Index, di] = giMult
	}
}

// AvgDifFromTrgAvg updates neuron-level AvgDif values from AvgPct - TrgAvg
// which is then used for synaptic scaling of LWt values in Path SynScale.
func (ly *Layer) AvgDifFromTrgAvg(ctx *Context) {
	sp := uint32(0)
	if ly.NPools > 1 {
		sp = 1
	}
	np := ly.NPools
	for spi := sp; spi < np; spi++ {
		pi := ly.Params.PoolIndex(spi)
		nsi := PoolsInt[PoolNeurSt, pi, 0]
		nei := PoolsInt[PoolNeurEd, pi, 0]
		plavg := float32(0)
		nn := 0
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NrnIsOff(ni) {
				continue
			}
			plavg += NeuronAvgs[ActAvg, ni]
			nn++
		}
		if nn == 0 {
			continue
		}
		plavg /= float32(nn)
		if plavg < 0.0001 { // gets unstable below here
			continue
		}
		PoolAvgDifInit(pi, 0)
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NrnIsOff(ni) {
				continue
			}
			apct := NeuronAvgs[ActAvg, ni] / plavg
			adif := apct - NeuronAvgs[TrgAvg, ni]
			NeuronAvgs[AvgPct, ni] = apct
			NeuronAvgs[AvgDif, ni] = adif
			PoolAvgDifUpdate(pi, 0, math32.Abs(adif))
		}
		PoolAvgDifCalc(pi, 0)
		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Avg), pi, di] = Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Avg), pi, 0]
			Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Max), pi, di] = Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Max), pi, 0]
		}
	}
	if sp == 1 { // update layer pool
		lpi := ly.Params.PoolIndex(0)
		PoolAvgDifInit(lpi, 0)
		nsi := PoolsInt[PoolNeurSt, lpi, 0]
		nei := PoolsInt[PoolNeurEd, lpi, 0]
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NrnIsOff(ni) {
				continue
			}
			PoolAvgDifUpdate(lpi, 0, math32.Abs(NeuronAvgs[AvgDif, ni]))
		}
		PoolAvgDifCalc(lpi, 0)

		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Avg), lpi, di] = Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Avg), lpi, 0]
			Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Max), lpi, di] = Pools[AvgMaxVarIdx(AMAvgDif, AMCycle, Max), lpi, 0]
		}
	}
}

// SynFail updates synaptic weight failure only -- normally done as part of DWt
// and WtFromDWt, but this call can be used during testing to update failing synapses.
func (ly *Layer) SynFail(ctx *Context) {
	for _, pj := range ly.SendPaths {
		if pj.Off {
			continue
		}
		pj.SynFail(ctx)
	}
}

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateMod(mod float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.LRateMod(mod)
	}
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateSched(sched float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.LRateSched(sched)
	}
}

// SetSubMean sets the SubMean parameters in all the layers in the network
// trgAvg is for Learn.TrgAvgAct.SubMean
// path is for the paths Learn.Trace.SubMean
// in both cases, it is generally best to have both parameters set to 0
// at the start of learning
func (ly *Layer) SetSubMean(trgAvg, path float32) {
	ly.Params.Learn.TrgAvgAct.SubMean = trgAvg
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.Params.Learn.Trace.SubMean = path
	}
}


