// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import 	"cogentcore.org/core/math32"

//gosl:start

// DTrgSubMean subtracts the mean from DTrgAvg values
// Called by TrgAvgFromD
func (ly *LayerParams) DTrgSubMean(ctx *Context) {
	submean := ly.Learn.TrgAvgAct.SubMean
	if submean == 0 {
		return
	}
	if ly.HasPoolInhib() && ly.Learn.TrgAvgAct.Pool.IsTrue() {
		np := ly.Indexes.NPools
		for spi := uint32(1); spi < np; spi++ {
			pi := ly.PoolIndex(spi)
			nsi := PoolsInt[pi, 0, PoolNeurSt]
			nei := PoolsInt[pi, 0, PoolNeurEd]
			nn := 0
			avg := float32(0)
			for lni := nsi; lni < nei; lni++ {
				ni := ly.Indexes.NeurSt + uint32(lni)
				if NeuronIsOff(ni) {
					continue
				}
				avg += NeuronAvgs[ni, DTrgAvg]
				nn++
			}
			if nn == 0 {
				continue
			}
			avg /= float32(nn)
			avg *= submean
			for lni := nsi; lni < nei; lni++ {
				ni := ly.Indexes.NeurSt + uint32(lni)
				if NeuronIsOff(ni) {
					continue
				}
				NeuronAvgs[ni, DTrgAvg] -= avg
			}
		}
	} else {
		nn := 0
		avg := float32(0)
		tn := ly.Indexes.NNeurons
		for lni := uint32(0); lni < tn; lni++ {
			ni := ly.Indexes.NeurSt + lni
			if NeuronIsOff(ni) {
				continue
			}
			avg += NeuronAvgs[ni, DTrgAvg]
			nn++
		}
		if nn == 0 {
			return
		}
		avg /= float32(nn)
		avg *= submean
		for lni := uint32(0); lni < tn; lni++ {
			ni := ly.Indexes.NeurSt + lni
			if NeuronIsOff(ni) {
				continue
			}
			NeuronAvgs[ni, DTrgAvg] -= avg
		}
	}
}

// TrgAvgFromD updates TrgAvg from DTrgAvg -- called in PlusPhasePost
func (ly *LayerParams) TrgAvgFromD(ctx *Context) {
	lr := ly.LearnTrgAvgErrLRate()
	if lr == 0 {
		return
	}
	ly.DTrgSubMean(ctx)
	nn := ly.Indexes.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.Indexes.NeurSt + lni
		if NeuronIsOff(ni) {
			continue
		}
		ntrg := NeuronAvgs[ni, TrgAvg] + NeuronAvgs[ni, DTrgAvg]
		ntrg = ly.Learn.TrgAvgAct.TrgRange.ClipValue(ntrg)
		NeuronAvgs[ni, TrgAvg] = ntrg
		NeuronAvgs[ni, DTrgAvg] = 0.0
	}
}

// WtFromDWtLayer does weight update at the layer level.
// does NOT call main pathway-level WtFromDWt method.
// in base, only calls TrgAvgFromD
func (ly *LayerParams) WtFromDWtLayer(ctx *Context) {
	ly.TrgAvgFromD(ctx)
}

//gosl:end

// SlowAdapt is the layer-level slow adaptation functions.
// Calls AdaptInhib and AvgDifFromTrgAvg for Synaptic Scaling.
// Does NOT call pathway-level methods.
func (ly *Layer) SlowAdapt(ctx *Context) {
	ly.AdaptInhib(ctx)
	ly.AvgDifFromTrgAvg(ctx)
	// note: path level call happens at network level
}

// AdaptInhib adapts inhibition
func (ly *Layer) AdaptInhib(ctx *Context) {
	if ly.Params.Inhib.ActAvg.AdaptGi.IsFalse() || ly.Params.IsInput() {
		return
	}
	for di := uint32(0); di < ctx.NData; di++ {
		giMult := LayerStates[ly.Index, di, LayerGiMult]
		avg := LayerStates[ly.Index, di, LayerActMAvg]
		ly.Params.Inhib.ActAvg.Adapt(&giMult, avg)
		LayerStates[ly.Index, di, LayerGiMult] = giMult
	}
}

// AvgDifFromTrgAvg updates neuron-level AvgDif values from AvgPct - TrgAvg
// which is then used for synaptic scaling of LWt values in Path SynScale.
func (ly *Layer) AvgDifFromTrgAvg(ctx *Context) {
	sp := uint32(0)
	if ly.NPools > 1 {
		sp = 1
	}
	np := ly.NPools
	for spi := sp; spi < np; spi++ {
		pi := ly.Params.PoolIndex(spi)
		nsi := PoolsInt[pi, 0, PoolNeurSt]
		nei := PoolsInt[pi, 0, PoolNeurEd]
		plavg := float32(0)
		nn := 0
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			plavg += NeuronAvgs[ni, ActAvg]
			nn++
		}
		if nn == 0 {
			continue
		}
		plavg /= float32(nn)
		if plavg < 0.0001 { // gets unstable below here
			continue
		}
		PoolAvgDifInit(pi, 0)
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			apct := NeuronAvgs[ni, ActAvg] / plavg
			adif := apct - NeuronAvgs[ni, TrgAvg]
			NeuronAvgs[ni, AvgPct] = apct
			NeuronAvgs[ni, AvgDif] = adif
			PoolAvgDifUpdate(pi, 0, math32.Abs(adif))
		}
		PoolAvgDifCalc(pi, 0)
		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools[pi, di, AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)] = Pools[pi, 0, AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)]
			Pools[pi, di, AvgMaxVarIndex(AMAvgDif, AMCycle, Max)] = Pools[pi, 0, AvgMaxVarIndex(AMAvgDif, AMCycle, Max)]
		}
	}
	if sp == 1 { // update layer pool
		lpi := ly.Params.PoolIndex(0)
		PoolAvgDifInit(lpi, 0)
		nsi := PoolsInt[lpi, 0, PoolNeurSt]
		nei := PoolsInt[lpi, 0, PoolNeurEd]
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			PoolAvgDifUpdate(lpi, 0, math32.Abs(NeuronAvgs[ni, AvgDif]))
		}
		PoolAvgDifCalc(lpi, 0)

		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools[lpi, di, AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)] = Pools[lpi, 0, AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)]
			Pools[lpi, di, AvgMaxVarIndex(AMAvgDif, AMCycle, Max)] = Pools[lpi, 0, AvgMaxVarIndex(AMAvgDif, AMCycle, Max)]
		}
	}
}

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateMod(mod float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.LRateMod(mod)
	}
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateSched(sched float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.LRateSched(sched)
	}
}

// SetSubMean sets the SubMean parameters in all the layers in the network
// trgAvg is for Learn.TrgAvgAct.SubMean
// path is for the paths Learn.Trace.SubMean
// in both cases, it is generally best to have both parameters set to 0
// at the start of learning
func (ly *Layer) SetSubMean(trgAvg, path float32) {
	ly.Params.Learn.TrgAvgAct.SubMean = trgAvg
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		// 	continue
		// }
		pj.Params.Learn.Trace.SubMean = path
	}
}


