// Code generated by "goal build"; DO NOT EDIT.
//line path-algo.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

// path-algo.go has the core computational methods.

//////////////////////////////////////////////////////////////////////////////////////
//  Learn methods

// DWtSubMean subtracts the mean from any pathways that have SubMean > 0.
// This is called on *receiving* pathways, prior to WtFromDwt.
func (pj *Path) DWtSubMean(ctx *Context, ri uint32) {
	if pj.Params.Learn.Learn.IsFalse() {
		return
	}
	sm := pj.Params.Learn.Trace.SubMean
	if sm == 0 { // note default is now 0, so don't exclude Target layers, which should be 0
		return
	}
	syIndexes := pj.RecvSynIxs(ri - pj.Recv.NeurStIndex)
	if len(syIndexes) < 1 {
		return
	}
	sumDWt := float32(0)
	nnz := 0 // non-zero
	for _, syi := range syIndexes {
		syni := pj.SynStIndex + syi
		dw := Synapses.Value(int(DWt), int(syni))
		if dw != 0 {
			sumDWt += dw
			nnz++
		}
	}
	if nnz <= 1 {
		return
	}
	sumDWt /= float32(nnz)
	for _, syi := range syIndexes {
		syni := pj.SynStIndex + syi
		if Synapses.Value(int(DWt), int(syni)) != 0 {
			Synapses.SetAdd(-sm*sumDWt, int(DWt), int(syni))
		}
	}
}

// WtFromDWt computes the weight change (learning), based on
// synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pj *Path) WtFromDWt(ctx *Context, ni uint32) {
	if pj.Params.Learn.Learn.IsFalse() {
		return
	}
	scon := pj.SendCon[ni-pj.Send.NeurStIndex]
	for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
		syni := pj.SynStIndex + syi
		pj.Params.WtFromDWtSyn(ctx, syni)
	}
}

// SlowAdapt does the slow adaptation: SWt learning and SynScale
func (pj *Path) SlowAdapt(ctx *Context) {
	pj.SWtFromWt(ctx)
	pj.SynScale(ctx)
}

// SWtFromWt updates structural, slowly adapting SWt value based on
// accumulated DSWt values, which are zero-summed with additional soft bounding
// relative to SWt limits.
func (pj *Path) SWtFromWt(ctx *Context) {
	if pj.Params.Learn.Learn.IsFalse() || pj.Params.SWts.Adapt.On.IsFalse() {
		return
	}
	rlay := pj.Recv
	if rlay.Params.IsTarget() {
		return
	}
	mx := pj.Params.SWts.Limit.Max
	mn := pj.Params.SWts.Limit.Min
	lr := pj.Params.SWts.Adapt.LRate
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		syIndexes := pj.RecvSynIxs(lni)
		nCons := len(syIndexes)
		if nCons < 1 {
			continue
		}
		avgDWt := float32(0)
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			swt := Synapses.Value(int(SWt), int(syni))
			// softbound for SWt
			if Synapses.Value(int(DSWt), int(syni)) >= 0 {
				Synapses.SetMul((mx - swt), int(DSWt), int(syni))
			} else {
				Synapses.SetMul((swt - mn), int(DSWt), int(syni))
			}
			avgDWt += Synapses.Value(int(DSWt), int(syni))
		}
		avgDWt /= float32(nCons)
		avgDWt *= pj.Params.SWts.Adapt.SubMean
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			Synapses.SetAdd(lr*(Synapses.Value(int(DSWt), int(syni))-avgDWt), int(SWt), int(syni))
			swt := Synapses.Value(int(SWt), int(syni))
			Synapses.Set(0, int(DSWt), int(syni))
			if Synapses.Value(int(Wt), int(syni)) == 0 { // restore failed wts
				wt := pj.Params.SWts.WtValue(swt, Synapses.Value(int(LWt), int(syni)))
				Synapses.Set(wt, int(Wt), int(syni))
			}
			// + pj.Params.SWts.Adapt.RandVar(
			Synapses.Set(pj.Params.SWts.LWtFromWts(Synapses.Value(int(Wt), int(syni)), swt), int(LWt), int(syni))
			Synapses.Set(pj.Params.SWts.WtValue(swt, Synapses.Value(int(LWt), int(syni))), int(Wt), int(syni))
		}
	}
}

// SynScale performs synaptic scaling based on running average activation vs. targets.
// Layer-level AvgDifFromTrgAvg function must be called first.
func (pj *Path) SynScale(ctx *Context) {
	if pj.Params.Learn.Learn.IsFalse() || pj.Params.IsInhib() {
		return
	}
	rlay := pj.Recv
	if !rlay.Params.IsLearnTrgAvg() {
		return
	}
	tp := &rlay.Params.Learn.TrgAvgAct
	lr := tp.SynScaleRate
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		ri := rlay.NeurStIndex + lni
		if NrnIsOff(ri) {
			continue
		}
		adif := -lr * NeuronAvgs.Value(int(AvgDif), int(ri))
		syIndexes := pj.RecvSynIxs(lni)
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			lwt := Synapses.Value(int(LWt), int(syni))
			swt := Synapses.Value(int(SWt), int(syni))
			if adif >= 0 { // key to have soft bounding on lwt here!
				Synapses.SetAdd((1-lwt)*adif*swt, int(LWt), int(syni))
			} else {
				Synapses.SetAdd(lwt*adif*swt, int(LWt), int(syni))
			}
			Synapses.Set(pj.Params.SWts.WtValue(swt, Synapses.Value(int(LWt), int(syni))), int(Wt), int(syni))
		}
	}
}

// SynFail updates synaptic weight failure only -- normally done as part of DWt
// and WtFromDWt, but this call can be used during testing to update failing synapses.
func (pj *Path) SynFail(ctx *Context) {
	slay := pj.Send
	for lni := uint32(0); lni < slay.NNeurons; lni++ {
		scon := pj.SendCon[lni]
		for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
			syni := pj.SynStIndex + syi
			swt := Synapses.Value(int(SWt), int(syni))
			if Synapses.Value(int(Wt), int(syni)) == 0 { // restore failed wts
				Synapses.Set(pj.Params.SWts.WtValue(swt, Synapses.Value(int(LWt), int(syni))), int(Wt), int(syni))
			}
			pj.Params.Com.Fail(ctx, syni, swt)
		}
	}
}

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (pj *Path) LRateMod(mod float32) {
	pj.Params.Learn.LRate.Mod = mod
	pj.Params.Learn.LRate.Update()
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (pj *Path) LRateSched(sched float32) {
	pj.Params.Learn.LRate.Sched = sched
	pj.Params.Learn.LRate.Update()
}
