// Code generated by "goal build"; DO NOT EDIT.
//line learn-layer.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import "cogentcore.org/core/math32"

//gosl:start

// DTrgSubMean subtracts the mean from DTrgAvg values.
// Called by TrgAvgFromD
func (ly *LayerParams) DTrgSubMean(ctx *Context) {
	submean := ly.Learn.TrgAvgAct.SubMean
	if submean == 0 {
		return
	}
	if ly.HasPoolInhib() && ly.Learn.TrgAvgAct.Pool.IsTrue() {
		np := ly.Indexes.NPools
		for spi := uint32(1); spi < np; spi++ {
			pi := ly.PoolIndex(spi)
			nsi := PoolIxs.Value(int(pi), int(PoolNeurSt))
			nei := PoolIxs.Value(int(pi), int(PoolNeurEd))
			nn := 0
			avg := float32(0)
			for lni := nsi; lni < nei; lni++ {
				ni := ly.Indexes.NeurSt + uint32(lni)
				if NeuronIsOff(ni) {
					continue
				}
				avg += NeuronAvgs.Value(int(ni), int(DTrgAvg))
				nn++
			}
			if nn == 0 {
				continue
			}
			avg /= float32(nn)
			avg *= submean
			for lni := nsi; lni < nei; lni++ {
				ni := ly.Indexes.NeurSt + uint32(lni)
				if NeuronIsOff(ni) {
					continue
				}
				NeuronAvgs.SetSub(avg, int(ni), int(DTrgAvg))
			}
		}
	} else {
		nn := 0
		avg := float32(0)
		tn := ly.Indexes.NNeurons
		for lni := uint32(0); lni < tn; lni++ {
			ni := ly.Indexes.NeurSt + lni
			if NeuronIsOff(ni) {
				continue
			}
			avg += NeuronAvgs.Value(int(ni), int(DTrgAvg))
			nn++
		}
		if nn == 0 {
			return
		}
		avg /= float32(nn)
		avg *= submean
		for lni := uint32(0); lni < tn; lni++ {
			ni := ly.Indexes.NeurSt + lni
			if NeuronIsOff(ni) {
				continue
			}
			NeuronAvgs.SetSub(avg, int(ni), int(DTrgAvg))
		}
	}
}

// TrgAvgFromD updates TrgAvg from DTrgAvg, called in PlusPhasePost.
func (ly *LayerParams) TrgAvgFromD(ctx *Context) {
	lr := ly.LearnTrgAvgErrLRate()
	if lr == 0 {
		return
	}
	ly.DTrgSubMean(ctx)
	nn := ly.Indexes.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.Indexes.NeurSt + lni
		if NeuronIsOff(ni) {
			continue
		}
		ntrg := NeuronAvgs.Value(int(ni), int(TrgAvg)) + NeuronAvgs.Value(int(ni), int(DTrgAvg))
		ntrg = ly.Learn.TrgAvgAct.TrgRange.ClampValue(ntrg)
		NeuronAvgs.Set(ntrg, int(ni), int(TrgAvg))
		NeuronAvgs.Set(0.0, int(ni), int(DTrgAvg))
	}
}

// WtFromDWtLayer does weight update at the layer level.
// does NOT call main pathway-level WtFromDWt method.
// in base, only calls TrgAvgFromD
func (ly *LayerParams) WtFromDWtLayer(ctx *Context) {
	ly.TrgAvgFromD(ctx)
}

// DWtSubMean subtracts the mean DWt for each recv neuron.
func (ly *LayerParams) DWtSubMean(ctx *Context, ri uint32) {
	lni := ri - ly.Indexes.NeurSt
	rn := ly.Indexes.RecvN
	for pi := uint32(0); pi < rn; pi++ {
		pti := RecvPathIxs.Value(int(ly.Indexes.RecvSt + pi))
		Paths[pti].DWtSubMean(ctx, pti, ri, lni)
	}
}

//////// SlowAdapt

// SlowAdaptLayer is the layer-level slow adaptation functions.
// Calls AdaptInhib and AvgDifFromTrgAvg for Synaptic Scaling.
// Does NOT call pathway-level methods.
func (ly *LayerParams) SlowAdaptLayer(ctx *Context) {
	ly.AdaptInhib(ctx)
	ly.AvgDifFromTrgAvg(ctx)
}

// AdaptInhib adapts inhibition.
func (ly *LayerParams) AdaptInhib(ctx *Context) {
	if ly.Inhib.ActAvg.AdaptGi.IsFalse() || ly.IsInput() {
		return
	}
	for di := uint32(0); di < ctx.NData; di++ {
		giMult := LayerStates.Value(int(ly.Index), int(di), int(LayerGiMult))
		avg := LayerStates.Value(int(ly.Index), int(di), int(LayerActMAvg))
		ly.Inhib.ActAvg.Adapt(&giMult, avg)
		LayerStates.Set(giMult, int(ly.Index), int(di), int(LayerGiMult))
	}
}

// AvgDifFromTrgAvg updates neuron-level AvgDif values from AvgPct - TrgAvg
// which is then used for synaptic scaling of LWt values in Path SynScale.
func (ly *LayerParams) AvgDifFromTrgAvg(ctx *Context) {
	sp := uint32(0)
	if ly.Indexes.NPools > 1 {
		sp = 1
	}
	np := ly.Indexes.NPools
	for spi := sp; spi < np; spi++ {
		pi := ly.PoolIndex(spi)
		nsi := PoolIxs.Value(int(pi), int(PoolNeurSt))
		nei := PoolIxs.Value(int(pi), int(PoolNeurEd))
		plavg := float32(0)
		nn := 0
		for lni := nsi; lni < nei; lni++ {
			ni := ly.Indexes.NeurSt + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			plavg += NeuronAvgs.Value(int(ni), int(ActAvg))
			nn++
		}
		if nn == 0 {
			continue
		}
		plavg /= float32(nn)
		if plavg < 0.0001 { // gets unstable below here
			continue
		}
		PoolAvgDifInit(pi, 0)
		for lni := nsi; lni < nei; lni++ {
			ni := ly.Indexes.NeurSt + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			apct := NeuronAvgs.Value(int(ni), int(ActAvg)) / plavg
			adif := apct - NeuronAvgs.Value(int(ni), int(TrgAvg))
			NeuronAvgs.Set(apct, int(ni), int(AvgPct))
			NeuronAvgs.Set(adif, int(ni), int(AvgDif))
			PoolAvgDifUpdate(pi, 0, math32.Abs(adif))
		}
		PoolAvgDifCalc(pi, 0)
		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools.Set(Pools.Value(int(pi), int(0), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Avg))), int(pi), int(di), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)))
			Pools.Set(Pools.Value(int(pi), int(0), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Max))), int(pi), int(di), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Max)))
		}
	}
	if sp == 1 { // update layer pool
		lpi := ly.PoolIndex(0)
		PoolAvgDifInit(lpi, 0)
		nsi := PoolIxs.Value(int(lpi), int(PoolNeurSt))
		nei := PoolIxs.Value(int(lpi), int(PoolNeurEd))
		for lni := nsi; lni < nei; lni++ {
			ni := ly.Indexes.NeurSt + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			PoolAvgDifUpdate(lpi, 0, math32.Abs(NeuronAvgs.Value(int(ni), int(AvgDif))))
		}
		PoolAvgDifCalc(lpi, 0)

		for di := uint32(1); di < ctx.NData; di++ { // copy to other datas
			Pools.Set(Pools.Value(int(lpi), int(0), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Avg))), int(lpi), int(di), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Avg)))
			Pools.Set(Pools.Value(int(lpi), int(0), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Max))), int(lpi), int(di), int(AvgMaxVarIndex(AMAvgDif, AMCycle, Max)))
		}
	}
}

// SlowAdaptNeuron does path & synapse level slow adaptation on SWt and
// overall synaptic scaling, per each receiving neuron ri.
func (ly *LayerParams) SlowAdaptNeuron(ctx *Context, ri uint32) {
	lni := ri - ly.Indexes.NeurSt
	rn := ly.Indexes.RecvN
	for pi := uint32(0); pi < rn; pi++ {
		pti := RecvPathIxs.Value(int(ly.Indexes.RecvSt + pi))
		Paths[pti].SlowAdapt(ctx, ly, pti, ri, lni)
	}
}

//gosl:end

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateMod(mod float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		//
		//		continue
		//	}
		pj.LRateMod(mod)
	}
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (ly *Layer) LRateSched(sched float32) {
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		//
		//		continue
		//	}
		pj.LRateSched(sched)
	}
}

// SetSubMean sets the SubMean parameters in all the layers in the network
// trgAvg is for Learn.TrgAvgAct.SubMean
// path is for the paths Learn.Trace.SubMean
// in both cases, it is generally best to have both parameters set to 0
// at the start of learning
func (ly *Layer) SetSubMean(trgAvg, path float32) {
	ly.Params.Learn.TrgAvgAct.SubMean = trgAvg
	for _, pj := range ly.RecvPaths {
		// if pj.Off { // keep all sync'd
		//
		//		continue
		//	}
		pj.Params.Learn.Trace.SubMean = path
	}
}
