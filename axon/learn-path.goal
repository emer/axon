// Copyright (c) 2023, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"cogentcore.org/core/math32"
	"github.com/emer/axon/v2/fsfffb"
)

//gosl:start

// DWtSyn is the overall entry point for weight change (learning) at given synapse.
// It selects appropriate function based on pathway type.
// rpl is the receiving layer SubPool
func (pt *PathParams) DWtSyn(ctx *Context, rlay *LayerParams, syni, si, ri, di uint32) {
	if pt.Learn.Learn == 0 {
		return
	}
	isTarget := rlay.IsTarget()
	spi := NeuronIxs[ri, NrnSubPool]
	pi := rlay.PoolIndex(spi)
	lpi := rlay.PoolIndex(0)
	switch pt.Type {
	case CTCtxtPath:
		pt.DWtSynCTCtxt(ctx, syni, si, ri, lpi, pi, di)
	case VSMatrixPath:
		pt.DWtSynVSMatrix(ctx, syni, si, ri, lpi, pi, di)
	case DSMatrixPath:
		pt.DWtSynDSMatrix(ctx, syni, si, ri, lpi, pi, di)
	case VSPatchPath:
		pt.DWtSynVSPatch(ctx, syni, si, ri, lpi, pi, di)
	case DSPatchPath:
		pt.DWtSynDSPatch(ctx, syni, si, ri, lpi, pi, di)
	case CNIOPath:
		pt.DWtCNIO(ctx, rlay, syni, si, ri, lpi, pi, di)
	case RWPath:
		pt.DWtSynRWPred(ctx, syni, si, ri, lpi, pi, di)
	case TDPredPath:
		pt.DWtSynTDPred(ctx, syni, si, ri, lpi, pi, di)
	case BLAPath:
		pt.DWtSynBLA(ctx, syni, si, ri, lpi, pi, di)
	case HipPath:
		pt.DWtSynHip(ctx, syni, si, ri, lpi, pi, di, isTarget) // by default this is the same as DWtSynCortex (w/ unused Hebb component in the algorithm) except that it uses WtFromDWtSynNoLimits
	default:
		if pt.Learn.Hebb.On.IsTrue() {
			pt.DWtSynHebb(ctx, syni, si, ri, lpi, pi, di)
		} else if isTarget {
			pt.DWtSynTarget(ctx, syni, si, ri, lpi, pi, di)
		} else {
			pt.DWtSynCortex(ctx, rlay, syni, si, ri, lpi, pi, di)
		}
	}
}

// SynCa gets the synaptic calcium P (potentiation) and D (depression)
// values, using an optimized integration of neuron-level [CaBins] values,
// and weight factors to capture the different CaP vs. CaD time constants.
func (pt *PathParams) SynCa(ctx *Context, si, ri, di uint32, syCaP, syCaD *float32) {
	edcyc := ctx.CyclesTotal
	stcyc := edcyc - (ctx.MinusCycles + ctx.PlusCycles)
	nbins := (edcyc - stcyc) / CaBinCycles
	cadSt := GvCaBinWts+GlobalScalarVars(nbins)
	
	b0 := CaBinForCycle(stcyc)
	
	// T0
	r0 := Neurons[ri, di, CaBins + NeuronVars(b0)]
	s0 := Neurons[si, di, CaBins + NeuronVars(b0)]
	sp := r0 * s0
	cp := sp * GlobalScalars[GvCaBinWts + GlobalScalarVars(0), 0]
	cd := sp * GlobalScalars[cadSt + GlobalScalarVars(0), 0]

	syn20 := pt.Learn.DWt.SynCa20.IsTrue()

	for i := int32(1); i < nbins; i++ {
		bi := CaBinForCycle(stcyc + i * CaBinCycles)
		rt := Neurons[ri, di, CaBins + NeuronVars(bi)]
		st := Neurons[si, di, CaBins + NeuronVars(bi)]
		sp := float32(0)
		if syn20 {
			bm := CaBinForCycle(stcyc + (i-1) * CaBinCycles)
			rt1 := Neurons[ri, di, CaBins + NeuronVars(bm)]
			st1 := Neurons[si, di, CaBins + NeuronVars(bm)]
			sp = 0.25 * (rt + rt1) * (st + st1)
		} else {
			sp = rt * st
		}
		cp += sp * GlobalScalars[GvCaBinWts + GlobalScalarVars(i), 0]
		cd += sp * GlobalScalars[cadSt + GlobalScalarVars(i), 0]
	}
	*syCaP = pt.Learn.DWt.CaPScale * cp
	*syCaD = cd
}

// SynCaTotal gets the total synaptic calcium coproduct from 
// given ending cycle (total elapsed cycles) and number of cycles prior,
// using an optimized integration of neuron-level [CaBins] values.
func (pt *PathParams) SynCaTotal(ctx *Context, si, ri, di uint32, edcyc, ncyc int32) float32 {
	nbins := ncyc / CaBinCycles
	stcyc := edcyc - ncyc

	sum := float32(0)
	for i := range nbins {
		bi := CaBinForCycle(stcyc + i * CaBinCycles)
		rc := Neurons[ri, di, CaBins + NeuronVars(bi)]
		sc := Neurons[si, di, CaBins + NeuronVars(bi)]
		sum += rc * sc
	}
	return sum * (8.0/float32(nbins)) // original 150/50 weights sum to 8
}

// IMPORTANT: all DWt routines MUST set DiDWt to _something_, otherwise the
// previous value will persist! i.e., set it to 0 if no learning.

// DWtSynSoftBound does the standard soft weight bounding for given 
// dwt weight change value. SoftBound must be done in the DWt step
// and not later, because it is learning-rule specific and enters
// into the zero-sum computation.
func (pt *PathParams) DWtSynSoftBound(ctx *Context, syni, di uint32, dwt float32) {
	if dwt == 0 {
		SynapseTraces[syni, di, DiDWt] = 0.0
	} else {
		lwt := Synapses[syni, LWt] // linear weight
		edw := dwt
		if edw > 0 {
			edw *= (1 - lwt)
		} else {
			edw *= lwt
		}
		SynapseTraces[syni, di, DiDWt] = pt.Learn.LRate.Eff * edw
	}
}

// DWtSynCortex computes the weight change (learning) at given synapse, using the
// kinase error-driven learning rule for cortical neurons. The error delta is
// based on the receiving neuron's [LearnCaP] - [LearnCaD], multiplied by a separate
// synaptic activation credit assignment factor computed from synaptic co-product CaD values.
func (pt *PathParams) DWtSynCortex(ctx *Context, rlay *LayerParams, syni, si, ri, lpi, pi, di uint32) {
	learnNow := int32(Neurons[ri, di, LearnNow])
	if learnNow - (ctx.CyclesTotal - ctx.ThetaCycles) < 0 { // not in this time window
		SynapseTraces[syni, di, DTr] = 0.0
		SynapseTraces[syni, di, DiDWt] = 0.0
		return
	}
	syCa := pt.SynCaTotal(ctx, si, ri, di, learnNow, rlay.Learn.Timing.SynCaCycles)

	// integrate synaptic trace over time: this is actually beneficial in certain cases,
	// in addition to the ETrLearn factor.
	SynapseTraces[syni, di, DTr] = syCa
	tr := pt.Learn.DWt.SynTrace(SynapseTraces[syni, di, Tr], syCa)
	SynapseTraces[syni, di, Tr] = tr
	
	dwt := float32(0)
	if syCa > pt.Learn.DWt.LearnThr { // todo: elminate?
		dwt = tr * Neurons[ri, di, RLRate] * Neurons[ri, di, LearnDiff] * Neurons[ri, di, ETrLearn] 
	}
	pt.DWtSynSoftBound(ctx, syni, di, dwt)
}

// DWtSynTarget computes the weight change (learning) at given synapse, 
// for a target layer (i.e., Pulvinar in predictive error-driven learning).
func (pt *PathParams) DWtSynTarget(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	var caP, caD float32
	pt.SynCa(ctx, si, ri, di, &caP, &caD)
	// caP := Neurons[ri, di, CaP] // significantly worse!
	// caD := Neurons[ri, di, CaD]

	SynapseTraces[syni, di, DTr] = caD
	tr := pt.Learn.DWt.SynTrace(SynapseTraces[syni, di, Tr], caD)
	SynapseTraces[syni, di, Tr] = tr
	
	dwt := Neurons[ri, di, RLRate] * (caP - caD)
	pt.DWtSynSoftBound(ctx, syni, di, dwt)
}

// DWtSynCTCtxt computes the weight change (learning) at given synapse, using the
// kinase error-driven learning rule for cortical neurons, for CT context paths.
// The error delta is based on the receiving neuron's [LearnCaP] - [LearnCaD],
// multiplied by a separate synaptic activation credit assignment factor computed
// from synaptic co-product CaD values.
func (pt *PathParams) DWtSynCTCtxt(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	syn := Neurons[si, di, BurstPrv] // previous burst, not synCa

	SynapseTraces[syni, di, DTr] = syn
	tr := pt.Learn.DWt.SynTrace(SynapseTraces[syni, di, Tr], syn)
	SynapseTraces[syni, di, Tr] = tr

	// note: not including RLRate here!
	dwt := tr * (Neurons[ri, di, LearnCaP] - Neurons[ri, di, LearnCaD]) * Neurons[ri, di, ETrLearn] 
	pt.DWtSynSoftBound(ctx, syni, di, dwt)
}

// DWtSynHebb computes the weight change (learning) at given synapse for cortex.
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pt *PathParams) DWtSynHebb(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	rLearnCaP := Neurons[ri, di, LearnCaP]
	sNrnCap := Neurons[si, di, LearnCaP]
	lwt := Synapses[syni, LWt] // linear weight
	hebb := rLearnCaP * (pt.Learn.Hebb.Up*sNrnCap*(1-lwt) - pt.Learn.Hebb.Down*(1-sNrnCap)*lwt)
	// not: Neurons[ri, di, RLRate]*
	SynapseTraces[syni, di, DiDWt] = pt.Learn.LRate.Eff * hebb
}

// DWtSynHip computes the weight change (learning) at given synapse for cortex + Hip (CPCA Hebb learning).
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
// Adds proportional CPCA learning rule for hip-specific paths
func (pt *PathParams) DWtSynHip(ctx *Context, syni, si, ri, lpi, pi, di uint32, isTarget bool) {
	// todo:
	// var syCaP, syCaD float32
	// pt.SynCa(ctx, si, ri, di, &syCaP, &syCaD)
// 
	// syn := syCaD               // synaptic activity co-product factor.
	// // integrate synaptic trace over time: this is actually beneficial in certain cases,
	// // in addition to the ETrLearn factor.
	// SynapseTraces[syni, di, DTr] = syn
	// tr := pt.Learn.DWt.SynTrace(SynapseTraces[syni, di, Tr], syn)
	// SynapseTraces[syni, di, Tr] = tr
	// 
	// // error-driven learning part
	// rLearnCaP := Neurons[ri, di, LearnCaP]
	// rLearnCaD := Neurons[ri, di, LearnCaD]
	// err := float32(0)
	// if isTarget {
	// 	err = syCaP - syCaD // for target layers, syn Ca drives error signal directly
	// } else {
	// 	err = tr * (rLearnCaP - rLearnCaD) * Neurons[ri, di, ETrLearn] 
	// }
	// 
	// // softbound immediately -- enters into zero sum.
	// // also other types might not use, so need to do this per learning rule
	// lwt := Synapses[syni, LWt] // linear weight
	// if err > 0 {
	// 	err *= (1 - lwt)
	// } else {
	// 	err *= lwt
	// }
// 
	// // hebbian-learning part
	// sNrnCap := Neurons[si, di, LearnCaP]
	// savg := 0.5 + pt.Hip.SAvgCor*(pt.Hip.SNominal-0.5)
	// savg = 0.5 / math32.Max(pt.Hip.SAvgThr, savg) // keep this Sending Average Correction term within bounds (SAvgThr)
	// hebb := rLearnCaP * (sNrnCap*(savg-lwt) - (1-sNrnCap)*lwt)
// 
	// // setting delta weight (note: impossible to be CTCtxtPath)
	// dwt := Neurons[ri, di, RLRate] * pt.Learn.LRate.Eff * (pt.Hip.Hebb*hebb + pt.Hip.Err*err)
	// SynapseTraces[syni, di, DiDWt] = dwt
}

// DWtSynBLA computes the weight change (learning) at given synapse for BLAPath type.
// Like the BG Matrix learning rule, a synaptic tag "trace" is established at CS onset (ACh)
// and learning at US / extinction is a function of trace * delta from US activity
// (temporal difference), which limits learning.
func (pt *PathParams) DWtSynBLA(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	ach := GlobalScalars[GvACh, di]
	dwt := float32(0)
	if GlobalScalars[GvHasRew, di] > 0 { // learn and reset
		ract := Neurons[ri, di, CaD]
		if ract < pt.Learn.DWt.LearnThr {
			ract = 0
		}
		tr := SynapseTraces[syni, di, Tr]
		ustr := pt.BLA.USTrace
		tr = ustr*Neurons[si, di, Burst] + (1.0-ustr)*tr
		delta := Neurons[ri, di, CaP] - Neurons[ri, di, CaDPrev]
		if delta < 0 { // neg delta learns slower in Acq, not Ext
			delta *= pt.BLA.NegDeltaLRate
		}
		dwt = Neurons[ri, di, RLRate] * tr * delta * ract
		SynapseTraces[syni, di, Tr] = 0.0
	} else if ach > pt.BLA.AChThr {
		// note: the former NonUSLRate parameter is not used -- Trace update Tau replaces it..  elegant
		dtr := ach * Neurons[si, di, Burst]
		SynapseTraces[syni, di, DTr] = dtr
		tr := pt.Learn.DWt.SynTrace(SynapseTraces[syni, di, Tr], dtr)
		SynapseTraces[syni, di, Tr] = tr
	} else {
		SynapseTraces[syni, di, DTr] = 0.0
	}
	pt.DWtSynSoftBound(ctx, syni, di, dwt)
}

// DWtSynRWPred computes the weight change (learning) at given synapse,
// for the RWPredPath type
func (pt *PathParams) DWtSynRWPred(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars[GvDA, di]
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	if NeuronIxs[ri, NrnNeurIndex] == 0 {
		if Neurons[ri, di, Ge] > Neurons[ri, di, Act] && da > 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons[ri, di, Ge] < Neurons[ri, di, Act] && da < 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr                                          // negative case
		if Neurons[ri, di, Ge] > Neurons[ri, di, Act] && da < 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons[ri, di, Ge] < Neurons[ri, di, Act] && da > 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons[si, di, CaP] // no recv unit activation
	SynapseTraces[syni, di, DiDWt] = eff_lr * dwt
}

// DWtSynTDPred computes the weight change (learning) at given synapse,
// for the TDPredPath type
func (pt *PathParams) DWtSynTDPred(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars[GvDA, di]
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	ni := NeuronIxs[ri, NrnNeurIndex]
	if ni == 0 {
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons[si, di, CaDPrev] // no recv unit activation, prior trial act
	SynapseTraces[syni, di, DiDWt] = eff_lr * dwt
}

// DWtSynVSMatrix computes the weight change (learning) at given synapse,
// for the VSMatrixPath type.
func (pt *PathParams) DWtSynVSMatrix(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// note: rn.RLRate already has BurstGain * ACh * DA * (D1 vs. D2 sign reversal) factored in.

	hasRew := GlobalScalars[GvHasRew, di] > 0
	ach := GlobalScalars[GvACh, di]
	if !hasRew && ach < 0.1 {
		SynapseTraces[syni, di, DTr] = 0.0
		return
	}
	rlr := Neurons[ri, di, RLRate]

	rplus := Neurons[ri, di, CaP]
	rminus := Neurons[ri, di, CaD]
	sact := Neurons[si, di, CaD]
	dtr := ach * (pt.VSMatrix.Delta * sact * (rplus - rminus))
	if rminus > pt.Learn.DWt.LearnThr { // key: prevents learning if < threshold
		dtr += ach * (pt.VSMatrix.Credit * sact * rminus)
	}
	dwt := float32(0)
	if hasRew {
		tr := SynapseTraces[syni, di, Tr]
		if pt.VSMatrix.RewActLearn.IsTrue() {
			tr += (1 - GlobalScalars[GvGoalMaint, di]) * dtr
		}
		dtr = 0
		dwt = rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces[syni, di, Tr] = 0.0
	} else {
		dtr *= rlr
		SynapseTraces[syni, di, Tr] += dtr
	}
	SynapseTraces[syni, di, DiDWt] = dwt
	SynapseTraces[syni, di, DTr] = dtr
}

// DWtSynDSMatrix computes the weight change (learning) at given synapse,
// for the DSMatrixPath type.
func (pt *PathParams) DWtSynDSMatrix(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in,
	// at time of reward, and otherwise is just the sig deriv mod.

	rlr := Neurons[ri, di, RLRate]
	dwt := float32(0)
	dtr := float32(0)
	if GlobalScalars[GvHasRew, di] > 0 { // US time -- use DA and current recv activity
		tr := SynapseTraces[syni, di, Tr]
		dwt = rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces[syni, di, Tr] = 0.0
	} else {
		// pfmod := Pools[pi, di, fsfffb.ModAct]
		pfmod := Neurons[ri, di, GModSyn] // syn value is always better
		patchDAD1 := Pools[pi, di, fsfffb.DAD1]
		patchDAD2 := pt.DSMatrix.D2Scale * Pools[pi, di, fsfffb.DAD2]
		rplus := Neurons[ri, di, CaP]
		rminus := Neurons[ri, di, CaD]
		sact := Neurons[si, di, CaD]
		dtr = rlr * (pt.DSMatrix.Delta * sact * (rplus - rminus)) // always delta
		if rminus > pt.Learn.DWt.LearnThr { // key: prevents learning if < threshold
			act := pt.DSMatrix.Credit * rlr * sact * rminus // rlr is sig deriv -- todo: CaSyn??
			dtr += (1.0 - pt.DSMatrix.PatchDA) * pfmod * act // std credit
			if pfmod > pt.Learn.DWt.LearnThr { // we were active in output
				// D1 dopamine discounts to the extent we are the correct action at this time: shunting
				// if reward is positive at end, this doesn't overtrain; if reward is negative because
				// _other_ actions were bad, this insulates the correct one.
				// if reward is negative because this action is bad, patchD2 adds to get more blame,
				dtr += pfmod * pt.DSMatrix.PatchDA * ((1.0 - patchDAD1) + patchDAD2) * act 
			} else { // not active; we have no role in the outcome
				// if the actual outcome is good, it is good for us to stay off
				// but if it is bad, then we should actually turn on.
				// so the sign should flip.
				// how does patch factor into that? If it thinks this is good,
				// but it wasn't activated, then go up, and vice-versa..
				// note: despite similarities with active case above, neither eq works 
				// as well as the one eq: modulation by PF = much better learning
				dtr += pt.DSMatrix.OffTrace * pt.DSMatrix.PatchDA * (patchDAD2 - patchDAD1) * act 
			}
		}
		SynapseTraces[syni, di, Tr] += dtr
	}
	SynapseTraces[syni, di, DiDWt] = dwt
	SynapseTraces[syni, di, DTr] = dtr
}

// DWtSynVSPatch computes the weight change (learning) at given synapse,
// for the VSPatchPath type.
func (pt *PathParams) DWtSynVSPatch(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	ract := Neurons[ri, di, CaDPrev] // t-1
	if ract < pt.Learn.DWt.LearnThr {
		ract = 0
	}
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in.
	// and also the logic that non-positive DA leads to weight decreases.
	sact := Neurons[si, di, CaDPrev] // t-1
	dwt := Neurons[ri, di, RLRate] * pt.Learn.LRate.Eff * sact * ract
	SynapseTraces[syni, di, DiDWt] = dwt
}

// DWtSynDSPatch computes the weight change (learning) at given synapse,
// for the DSPatchPath type. Conditioned on PF modulatory inputs.
func (pt *PathParams) DWtSynDSPatch(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	ract := Neurons[ri, di, CaD]
	if ract < pt.Learn.DWt.LearnThr {
		ract = 0
	}
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in,
	// at time of reward; otherwise is just sig deriv.

	rlr := Neurons[ri, di, RLRate]
	dwt := float32(0)
	dtr := float32(0)
	if GlobalScalars[GvHasRew, di] > 0 { // US time -- use DA * tr
		tr := SynapseTraces[syni, di, Tr]
		dwt = rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces[syni, di, Tr] = 0.0
	} else {
		pfmod := Neurons[ri, di, GModSyn] // so much better! todo: why!?
		// pfmod := Pools[pi, di, fsfffb.ModAct]
		sact := Neurons[si, di, CaD] // todo: use CaSyn instead of sact * ract? But BG is transient, so no?
		dtr = pfmod * rlr * sact * ract // rlr is just sig deriv
		SynapseTraces[syni, di, Tr] += dtr
	}
	SynapseTraces[syni, di, DiDWt] = dwt
	SynapseTraces[syni, di, DTr] = dtr
}

// DWtCNIO computes the weight change (learning) at given synapse,
// for cerebellar neurons that learn from IO LearnNow signals.
func (pt *PathParams) DWtCNIO(ctx *Context, rlay *LayerParams, syni, si, ri, lpi, pi, di uint32) {
	learnNow := int32(Neurons[ri, di, LearnNow])
	if learnNow - (ctx.CyclesTotal - ctx.ThetaCycles) < 0 { // not in this time window
		SynapseTraces[syni, di, DTr] = 0.0
		SynapseTraces[syni, di, DiDWt] = 0.0
		return
	}
	bi := CaBinForCycle(learnNow - rlay.Nuclear.SendTimeOff)
	sact := Neurons[si, di, CaBins + NeuronVars(bi)] // sending activity
	// todo: rlrate? Neurons[ri, di, RLRate]
	dwt := sact 
	if Neurons[ri, di, TimePeak] > 0 { // means that we got to end of cycle with no err: decay
		dwt = -dwt * rlay.Nuclear.Decay
	}
	// todo: softbound?
	SynapseTraces[syni, di, DiDWt] = pt.Learn.LRate.Eff * dwt
}

//////// WtFromDWt

// DWtFromDi updates DWt from data parallel DiDWt values
func (pt *PathParams) DWtFromDi(ctx *Context, syni uint32) {
	dwt := float32(0)
	for di := uint32(0); di < ctx.NData; di++ {
		dwt += SynapseTraces[syni, di, DiDWt]
	}
	Synapses[syni, DWt] += dwt
}

// DWtSubMean subtracts the mean for given recv neuron ri,
// for pathways that have SubMean > 0.
// This is called on *receiving* pathways, prior to WtFromDwt.
func (pt *PathParams) DWtSubMean(ctx *Context, pti, ri, lni uint32) {
	if pt.Learn.Learn.IsFalse() {
		return
	}
	sm := pt.Learn.DWt.SubMean
	if sm == 0 { // note default is now 0, so don't exclude Target layers, which should be 0
		return
	}
	cni := pt.Indexes.RecvConSt + lni
	synn := PathRecvCon[cni, Nitems]

	if synn < 1 {
		return
	}
	synst := pt.Indexes.RecvSynSt + PathRecvCon[cni, StartOff]

	sumDWt := float32(0)
	nnz := 0 // non-zero
	for ci := uint32(0); ci < synn; ci++ {
		syni := RecvSynIxs.Value(int(synst + ci))
		dw := Synapses[syni, DWt]
		if dw != 0 {
			sumDWt += dw
			nnz++
		}
	}
	if nnz <= 1 {
		return
	}
	sumDWt /= float32(nnz)
	for ci := uint32(0); ci < synn; ci++ {
		syni := RecvSynIxs.Value(int(synst + ci))
		if Synapses[syni, DWt] != 0 {
			Synapses[syni, DWt] += -sm * sumDWt
		}
	}
}

// WtFromDWtSyn is the overall entry point for updating weights from weight changes.
func (pt *PathParams) WtFromDWtSyn(ctx *Context, syni uint32) {
	switch pt.Type {
	case RWPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case TDPredPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case BLAPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case HipPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	default:
		pt.WtFromDWtSynCortex(ctx, syni)
	}
}

// WtFromDWtSynCortex updates weights from dwt changes
func (pt *PathParams) WtFromDWtSynCortex(ctx *Context, syni uint32) {
	dwt := Synapses[syni, DWt]
	Synapses[syni, DSWt] += dwt
	wt := Synapses[syni, Wt]
	lwt := Synapses[syni, LWt]

	pt.SWts.WtFromDWt(&wt, &lwt, dwt, Synapses[syni, SWt])
	Synapses[syni, DWt] = 0.0
	Synapses[syni, Wt] = wt
	Synapses[syni, LWt] = lwt
	// pj.Com.Fail(&sy.Wt, sy.SWt) // skipping for now -- not useful actually
}

// WtFromDWtSynNoLimits -- weight update without limits
func (pt *PathParams) WtFromDWtSynNoLimits(ctx *Context, syni uint32) {
	dwt := Synapses[syni, DWt]
	if dwt == 0 {
		return
	}
	Synapses[syni, Wt] += dwt
	if Synapses[syni, Wt] < 0 {
		Synapses[syni, Wt] = 0.0
	}
	Synapses[syni, LWt] = Synapses[syni, Wt]
	Synapses[syni, DWt] = 0.0
}

// SlowAdapt does the slow adaptation: SWt learning and SynScale
func (pt *PathParams) SlowAdapt(ctx *Context, rlay *LayerParams, pti, ri, lni uint32) {
	pt.SWtFromWt(ctx, rlay, pti, ri, lni)
	pt.SynScale(ctx, rlay, pti, ri, lni)
}

// SWtFromWt updates structural, slowly adapting SWt value based on
// accumulated DSWt values, which are zero-summed with additional soft bounding
// relative to SWt limits.
func (pt *PathParams) SWtFromWt(ctx *Context, rlay *LayerParams, pti, ri, lni uint32) {
	if pt.Learn.Learn.IsFalse() || pt.SWts.Adapt.On.IsFalse() {
		return
	}
	if rlay.IsTarget() {
		return
	}
	mx := pt.SWts.Limit.Max
	mn := pt.SWts.Limit.Min
	lr := pt.SWts.Adapt.LRate

	cni := pt.Indexes.RecvConSt + lni
	synn := PathRecvCon[cni, Nitems]
	synst := pt.Indexes.RecvSynSt + PathRecvCon[cni, StartOff]

	avgDWt := float32(0)
	avgWt := float32(0)
	for ci := uint32(0); ci < synn; ci++ {
		syni := RecvSynIxs.Value(int(synst + ci))
		swt := Synapses[syni, SWt]
		// softbound for SWt
		if Synapses[syni, DSWt] >= 0 {
			Synapses[syni, DSWt] *= (mx - swt)
		} else {
			Synapses[syni, DSWt] *= (swt - mn)
		}
		avgDWt += Synapses[syni, DSWt]
		avgWt += Synapses[syni, Wt]
	}
	avgDWt /= float32(synn)
	avgWt /= float32(synn)
	hiDk := math32.Clamp(pt.SWts.Adapt.HiMeanDecay * (avgWt - pt.SWts.Adapt.HiMeanThr), 0.0, pt.SWts.Adapt.HiMeanDecay)
	avgDWt *= pt.SWts.Adapt.SubMean
	for ci := uint32(0); ci < synn; ci++ {
		syni := RecvSynIxs.Value(int(synst + ci))
		Synapses[syni, SWt] += lr * (Synapses[syni, DSWt] - avgDWt)
		swt := Synapses[syni, SWt]
		Synapses[syni, DSWt] = 0.0
		wt := Synapses[syni, Wt]
		lwt := pt.SWts.LWtFromWts(wt, swt)
		lwt -= hiDk * lwt
		Synapses[syni, LWt] = lwt
		Synapses[syni, Wt] = pt.SWts.WtValue(swt, lwt)
	}
}

// SynScale performs synaptic scaling based on running average activation vs. targets.
// Layer-level AvgDifFromTrgAvg function must be called first.
func (pt *PathParams) SynScale(ctx *Context, rlay *LayerParams, pti, ri, lni uint32) {
	if pt.Learn.Learn.IsFalse() || pt.IsInhib() {
		return
	}
	if !rlay.IsLearnTrgAvg() {
		return
	}
	lr := rlay.Learn.TrgAvgAct.SynScaleRate

	cni := pt.Indexes.RecvConSt + lni
	synn := PathRecvCon[cni, Nitems]
	synst := pt.Indexes.RecvSynSt + PathRecvCon[cni, StartOff]
	adif := -lr * NeuronAvgs[ri, AvgDif]
	for ci := uint32(0); ci < synn; ci++ {
		syni := RecvSynIxs.Value(int(synst + ci))
		lwt := Synapses[syni, LWt]
		swt := Synapses[syni, SWt]
		if adif >= 0 { // key to have soft bounding on lwt here!
			Synapses[syni, LWt] += (1 - lwt) * adif * swt
		} else {
			Synapses[syni, LWt] += lwt * adif * swt
		}
		Synapses[syni, Wt] = pt.SWts.WtValue(swt, Synapses[syni, LWt])
	}
}

//gosl:end

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (pt *Path) LRateMod(mod float32) {
	pt.Params.Learn.LRate.Mod = mod
	pt.Params.Learn.LRate.Update()
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (pt *Path) LRateSched(sched float32) {
	pt.Params.Learn.LRate.Sched = sched
	pt.Params.Learn.LRate.Update()
}
