// Copyright (c) 2023, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import "cogentcore.org/core/math32"

//gosl:start

// DWtSyn is the overall entry point for weight change (learning) at given synapse.
// It selects appropriate function based on pathway type.
// rpl is the receiving layer SubPool
func (pt *PathParams) DWtSyn(ctx *Context, rlay *LayerParams, syni, si, ri, di uint32) {
	if pt.Learn.Learn == 0 {
		return
	}
	isTarget := rlay.Acts.Clamp.IsTarget > 0
	spi := NeuronIxs[NrnSubPool, ri]
	pi := rlay.PoolIndex(spi)
	lpi := rlay.PoolIndex(0)
	switch pt.PathType {
	case RWPath:
		pt.DWtSynRWPred(ctx, syni, si, ri, lpi, pi, di)
	case TDPredPath:
		pt.DWtSynTDPred(ctx, syni, si, ri, lpi, pi, di)
	case VSMatrixPath:
		pt.DWtSynVSMatrix(ctx, syni, si, ri, lpi, pi, di)
	case DSMatrixPath:
		pt.DWtSynDSMatrix(ctx, syni, si, ri, lpi, pi, di)
	case VSPatchPath:
		pt.DWtSynVSPatch(ctx, syni, si, ri, lpi, pi, di)
	case BLAPath:
		pt.DWtSynBLA(ctx, syni, si, ri, lpi, pi, di)
	case HipPath:
		pt.DWtSynHip(ctx, syni, si, ri, lpi, pi, di, isTarget) // by default this is the same as DWtSynCortex (w/ unused Hebb component in the algorithm) except that it uses WtFromDWtSynNoLimits
	default:
		if pt.Learn.Hebb.On.IsTrue() {
			pt.DWtSynHebb(ctx, syni, si, ri, lpi, pi, di)
		} else {
			pt.DWtSynCortex(ctx, syni, si, ri, lpi, pi, di, isTarget)
		}
	}
}

// SynCa gets the synaptic calcium P (potentiation) and D (depression)
// values, using optimized computation.
func (pt *PathParams) SynCa(ctx *Context, si, ri, di uint32, syCaP, syCaD *float32) {
	rb0 := Neurons[SpkBin0, ri, di]
	sb0 := Neurons[SpkBin0, si, di]
	rb1 := Neurons[SpkBin1, ri, di]
	sb1 := Neurons[SpkBin1, si, di]
	rb2 := Neurons[SpkBin2, ri, di]
	sb2 := Neurons[SpkBin2, si, di]
	rb3 := Neurons[SpkBin3, ri, di]
	sb3 := Neurons[SpkBin3, si, di]
	rb4 := Neurons[SpkBin4, ri, di]
	sb4 := Neurons[SpkBin4, si, di]
	rb5 := Neurons[SpkBin5, ri, di]
	sb5 := Neurons[SpkBin5, si, di]
	rb6 := Neurons[SpkBin6, ri, di]
	sb6 := Neurons[SpkBin6, si, di]
	rb7 := Neurons[SpkBin7, ri, di]
	sb7 := Neurons[SpkBin7, si, di]

	b0 := 0.1 * (rb0 * sb0)
	b1 := 0.1 * (rb1 * sb1)
	b2 := 0.1 * (rb2 * sb2)
	b3 := 0.1 * (rb3 * sb3)
	b4 := 0.1 * (rb4 * sb4)
	b5 := 0.1 * (rb5 * sb5)
	b6 := 0.1 * (rb6 * sb6)
	b7 := 0.1 * (rb7 * sb7)

	pt.Learn.KinaseCa.FinalCa(b0, b1, b2, b3, b4, b5, b6, b7, syCaP, syCaD)
}

// DWtSynCortex computes the weight change (learning) at given synapse for cortex.
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pt *PathParams) DWtSynCortex(ctx *Context, syni, si, ri, lpi, pi, di uint32, isTarget bool) {
	var syCaP, syCaD float32
	pt.SynCa(ctx, si, ri, di, &syCaP, &syCaD)

	dtr := syCaD                   // delta trace, caD reflects entire window
	if pt.PathType == CTCtxtPath { // layer 6 CT pathway
		dtr = Neurons[BurstPrv, si, di]
	}
	// save delta trace for GUI
	SynapseTraces[DTr, syni, di] = dtr
	// TrFromCa(prev-multiTrial Integrated Trace, deltaTrace), as a mixing func
	tr := pt.Learn.Trace.TrFromCa(SynapseTraces[Tr, syni, di], dtr)
	// save new trace, updated w/ credit assignment (dependent on Tau in the TrFromCa function
	SynapseTraces[Tr, syni, di] = tr
	// failed con, no learn
	if Synapses[Wt, syni] == 0 {
		return
	}

	// error-driven learning
	var err float32
	if isTarget {
		err = syCaP - syCaD // for target layers, syn Ca drives error signal directly
	} else {
		err = tr * (Neurons[NrnCaP, ri, di] - Neurons[NrnCaD, ri, di]) // hiddens: recv NMDA Ca drives error signal w/ trace credit
	}
	// note: trace ensures that nothing changes for inactive synapses..
	// sb immediately -- enters into zero sum.
	// also other types might not use, so need to do this per learning rule
	lwt := Synapses[LWt, syni] // linear weight
	if err > 0 {
		err *= (1 - lwt)
	} else {
		err *= lwt
	}
	if pt.PathType == CTCtxtPath { // rn.RLRate IS needed for other pathways, just not the context one
		SynapseTraces[DiDWt, syni, di] = pt.Learn.LRate.Eff * err
	} else {
		SynapseTraces[DiDWt, syni, di] = Neurons[RLRate, ri, di] * pt.Learn.LRate.Eff * err
	}
}

// DWtSynHebb computes the weight change (learning) at given synapse for cortex.
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
func (pt *PathParams) DWtSynHebb(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	rNrnCaP := Neurons[NrnCaP, ri, di]
	sNrnCap := Neurons[NrnCaP, si, di]
	lwt := Synapses[LWt, syni] // linear weight
	hebb := rNrnCaP * (pt.Learn.Hebb.Up*sNrnCap*(1-lwt) - pt.Learn.Hebb.Down*(1-sNrnCap)*lwt)
	// not: Neurons[RLRate, ri, di]*
	SynapseTraces[DiDWt, syni, di] = pt.Learn.LRate.Eff * hebb
}

// DWtSynHip computes the weight change (learning) at given synapse for cortex + Hip (CPCA Hebb learning).
// Uses synaptically integrated spiking, computed at the Theta cycle interval.
// This is the trace version for hidden units, and uses syn CaP - CaD for targets.
// Adds proportional CPCA learning rule for hip-specific paths
func (pt *PathParams) DWtSynHip(ctx *Context, syni, si, ri, lpi, pi, di uint32, isTarget bool) {
	var syCaP, syCaD float32
	pt.SynCa(ctx, si, ri, di, &syCaP, &syCaD)
	dtr := syCaD // delta trace, caD reflects entire window
	// save delta trace for GUI
	SynapseTraces[DTr, syni, di] = dtr
	// TrFromCa(prev-multiTrial Integrated Trace, deltaTrace), as a mixing func
	tr := pt.Learn.Trace.TrFromCa(SynapseTraces[Tr, syni, di], dtr)
	// save new trace, updated w/ credit assignment (dependent on Tau in the TrFromCa function
	SynapseTraces[Tr, syni, di] = tr
	// failed con, no learn
	if Synapses[Wt, syni] == 0 {
		return
	}

	// error-driven learning part
	rNrnCaP := Neurons[NrnCaP, ri, di]
	rNrnCaD := Neurons[NrnCaD, ri, di]
	var err float32
	if isTarget {
		err = syCaP - syCaD // for target layers, syn Ca drives error signal directly
	} else {
		err = tr * (rNrnCaP - rNrnCaD) // hiddens: recv NMDA Ca drives error signal w/ trace credit
	}
	// note: trace ensures that nothing changes for inactive synapses..
	// sb immediately -- enters into zero sum.
	// also other types might not use, so need to do this per learning rule
	lwt := Synapses[LWt, syni] // linear weight
	if err > 0 {
		err *= (1 - lwt)
	} else {
		err *= lwt
	}

	// hebbian-learning part
	sNrnCap := Neurons[NrnCaP, si, di]
	savg := 0.5 + pt.Hip.SAvgCor*(pt.Hip.SNominal-0.5)
	savg = 0.5 / math32.Max(pt.Hip.SAvgThr, savg) // keep this Sending Average Correction term within bounds (SAvgThr)
	hebb := rNrnCaP * (sNrnCap*(savg-lwt) - (1-sNrnCap)*lwt)

	// setting delta weight (note: impossible to be CTCtxtPath)
	dwt := Neurons[RLRate, ri, di] * pt.Learn.LRate.Eff * (pt.Hip.Hebb*hebb + pt.Hip.Err*err)
	SynapseTraces[DiDWt, syni, di] = dwt
}

// DWtSynBLA computes the weight change (learning) at given synapse for BLAPath type.
// Like the BG Matrix learning rule, a synaptic tag "trace" is established at CS onset (ACh)
// and learning at US / extinction is a function of trace * delta from US activity
// (temporal difference), which limits learning.
func (pt *PathParams) DWtSynBLA(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	dwt := float32(0)
	ach := GlobalScalars[GvACh, di]
	if GlobalScalars[GvHasRew, di] > 0 { // learn and reset
		ract := Neurons[CaSpkD, ri, di]
		if ract < pt.Learn.Trace.LearnThr {
			ract = 0
		}
		tr := SynapseTraces[Tr, syni, di]
		ustr := pt.BLA.USTrace
		tr = ustr*Neurons[Burst, si, di] + (1.0-ustr)*tr
		delta := Neurons[CaSpkP, ri, di] - Neurons[SpkPrv, ri, di]
		if delta < 0 { // neg delta learns slower in Acq, not Ext
			delta *= pt.BLA.NegDeltaLRate
		}
		dwt = tr * delta * ract
		SynapseTraces[Tr, syni, di] = 0.0
	} else if ach > pt.BLA.AChThr {
		// note: the former NonUSLRate parameter is not used -- Trace update Tau replaces it..  elegant
		dtr := ach * Neurons[Burst, si, di]
		SynapseTraces[DTr, syni, di] = dtr
		tr := pt.Learn.Trace.TrFromCa(SynapseTraces[Tr, syni, di], dtr)
		SynapseTraces[Tr, syni, di] = tr
	} else {
		SynapseTraces[DTr, syni, di] = 0.0
	}
	lwt := Synapses[LWt, syni]
	if dwt > 0 {
		dwt *= (1 - lwt)
	} else {
		dwt *= lwt
	}
	SynapseTraces[DiDWt, syni, di] = Neurons[RLRate, ri, di] * pt.Learn.LRate.Eff * dwt
}

// DWtSynRWPred computes the weight change (learning) at given synapse,
// for the RWPredPath type
func (pt *PathParams) DWtSynRWPred(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars[GvDA, di]
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	if NeuronIxs[NrnNeurIndex, ri] == 0 {
		if Neurons[Ge, ri, di] > Neurons[Act, ri, di] && da > 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons[Ge, ri, di] < Neurons[Act, ri, di] && da < 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr                                          // negative case
		if Neurons[Ge, ri, di] > Neurons[Act, ri, di] && da < 0 { // clipped at top, saturate up
			da = 0
		}
		if Neurons[Ge, ri, di] < Neurons[Act, ri, di] && da > 0 { // clipped at bottom, saturate down
			da = 0
		}
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons[CaSpkP, si, di] // no recv unit activation
	SynapseTraces[DiDWt, syni, di] = eff_lr * dwt
}

// DWtSynTDPred computes the weight change (learning) at given synapse,
// for the TDPredPath type
func (pt *PathParams) DWtSynTDPred(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// todo: move all of this into rn.RLRate
	lda := GlobalScalars[GvDA, di]
	da := lda
	lr := pt.Learn.LRate.Eff
	eff_lr := lr
	ni := NeuronIxs[NrnNeurIndex, ri]
	if ni == 0 {
		if da < 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	} else {
		eff_lr = -eff_lr
		if da >= 0 {
			eff_lr *= pt.RLPred.OppSignLRate
		}
	}

	dwt := da * Neurons[SpkPrv, si, di] // no recv unit activation, prior trial act
	SynapseTraces[DiDWt, syni, di] = eff_lr * dwt
}

// DWtSynVSMatrix computes the weight change (learning) at given synapse,
// for the VSMatrixPath type.
func (pt *PathParams) DWtSynVSMatrix(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// note: rn.RLRate already has BurstGain * ACh * DA * (D1 vs. D2 sign reversal) factored in.

	hasRew := GlobalScalars[GvHasRew, di] > 0
	ach := GlobalScalars[GvACh, di]
	if !hasRew && ach < 0.1 {
		SynapseTraces[DTr, syni, di] = 0.0
		return
	}
	rlr := Neurons[RLRate, ri, di]

	rplus := Neurons[CaSpkP, ri, di]
	rminus := Neurons[CaSpkD, ri, di]
	sact := Neurons[CaSpkD, si, di]
	dtr := ach * (pt.Matrix.Delta * sact * (rplus - rminus))
	if rminus > pt.Learn.Trace.LearnThr { // key: prevents learning if < threshold
		dtr += ach * (pt.Matrix.Credit * sact * rminus)
	}
	if hasRew {
		tr := SynapseTraces[Tr, syni, di]
		if pt.Matrix.VSRewLearn.IsTrue() {
			tr += (1 - GlobalScalars[GvGoalMaint, di]) * dtr
		}
		dwt := rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces[DiDWt, syni, di] = dwt
		SynapseTraces[Tr, syni, di] = 0.0
		SynapseTraces[DTr, syni, di] = 0.0
	} else {
		dtr *= rlr
		SynapseTraces[DTr, syni, di] = dtr
		SynapseTraces[Tr, syni, di] += dtr
	}
}

// DWtSynDSMatrix computes the weight change (learning) at given synapse,
// for the DSMatrixPath type.
func (pt *PathParams) DWtSynDSMatrix(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in.

	rlr := Neurons[RLRate, ri, di]
	if GlobalScalars[GvHasRew, di] > 0 { // US time -- use DA and current recv activity
		tr := SynapseTraces[Tr, syni, di]
		dwt := rlr * pt.Learn.LRate.Eff * tr
		SynapseTraces[DiDWt, syni, di] = dwt
		SynapseTraces[Tr, syni, di] = 0.0
		SynapseTraces[DTr, syni, di] = 0.0
	} else {
		pfmod := pt.Matrix.BasePF + Neurons[GModSyn, ri, di]
		rplus := Neurons[CaSpkP, ri, di]
		rminus := Neurons[CaSpkD, ri, di]
		sact := Neurons[CaSpkD, si, di]
		dtr := rlr * (pt.Matrix.Delta * sact * (rplus - rminus))
		if rminus > pt.Learn.Trace.LearnThr { // key: prevents learning if < threshold
			dtr += rlr * (pt.Matrix.Credit * pfmod * sact * rminus)
		}
		SynapseTraces[DTr, syni, di] = dtr
		SynapseTraces[Tr, syni, di] += dtr
	}
}

// DWtSynVSPatch computes the weight change (learning) at given synapse,
// for the VSPatchPath type.
func (pt *PathParams) DWtSynVSPatch(ctx *Context, syni, si, ri, lpi, pi, di uint32) {
	ract := Neurons[SpkPrv, ri, di] // t-1
	if ract < pt.Learn.Trace.LearnThr {
		ract = 0
	}
	// note: rn.RLRate already has ACh * DA * (D1 vs. D2 sign reversal) factored in.
	// and also the logic that non-positive DA leads to weight decreases.
	sact := Neurons[SpkPrv, si, di] // t-1
	dwt := Neurons[RLRate, ri, di] * pt.Learn.LRate.Eff * sact * ract
	SynapseTraces[DiDWt, syni, di] = dwt
}

//////// WtFromDWt

// DWtFromDi updates DWt from data parallel DiDWt values
func (pt *PathParams) DWtFromDi(ctx *Context, syni uint32) {
	dwt := float32(0)
	for di := uint32(0); di < ctx.NData; di++ {
		dwt += SynapseTraces[DiDWt, syni, di]
	}
	Synapses[DWt, syni] += dwt
}

// DWtSubMean subtracts the mean from any pathways that have SubMean > 0.
// This is called on *receiving* pathways, prior to WtFromDwt.
func (pt *PathParams) DWtSubMean(ctx *Context, pti uint32) {
	if pt.Learn.Learn.IsFalse() {
		return
	}
	sm := pt.Learn.Trace.SubMean
	if sm == 0 { // note default is now 0, so don't exclude Target layers, which should be 0
		return
	}
	ri := pt.Indexes.RecvLayer
	lni := ri - pt.Indexes.RecvNeurSt
	cni := pt.Indexes.RecvConSt + lni
	synn := int(pt.Indexes.RecvSynSt + PathRecvCon[StartNN, cni])
	if synn < 1 {
		return
	}
	synst := pt.Indexes.RecvSynSt + PathRecvCon[StartOff, cni]
	sumDWt := float32(0)
	nnz := 0 // non-zero
	for ci := range synn {
		syni := RecvSynIxs.Value(int(synst) + ci)
		dw := Synapses[DWt, syni]
		if dw != 0 {
			sumDWt += dw
			nnz++
		}
	}
	if nnz <= 1 {
		return
	}
	sumDWt /= float32(nnz)
	for ci := range synn {
		syni := RecvSynIxs.Value(int(synst) + ci)
		if Synapses[DWt, syni] != 0 {
			Synapses[DWt, syni] += -sm * sumDWt
		}
	}
}

// WtFromDWtSyn is the overall entry point for updating weights from weight changes.
func (pt *PathParams) WtFromDWtSyn(ctx *Context, syni uint32) {
	switch pt.PathType {
	case RWPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case TDPredPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case BLAPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	case HipPath:
		pt.WtFromDWtSynNoLimits(ctx, syni)
	default:
		pt.WtFromDWtSynCortex(ctx, syni)
	}
}

// WtFromDWtSynCortex updates weights from dwt changes
func (pt *PathParams) WtFromDWtSynCortex(ctx *Context, syni uint32) {
	dwt := Synapses[DWt, syni]
	Synapses[DSWt, syni] += dwt
	wt := Synapses[Wt, syni]
	lwt := Synapses[LWt, syni]

	pt.SWts.WtFromDWt(&wt, &lwt, dwt, Synapses[SWt, syni])
	Synapses[DWt, syni] = 0.0
	Synapses[Wt, syni] = wt
	Synapses[LWt, syni] = lwt
	// pj.Com.Fail(&sy.Wt, sy.SWt) // skipping for now -- not useful actually
}

// WtFromDWtSynNoLimits -- weight update without limits
func (pt *PathParams) WtFromDWtSynNoLimits(ctx *Context, syni uint32) {
	dwt := Synapses[DWt, syni]
	if dwt == 0 {
		return
	}
	Synapses[Wt, syni] += dwt
	if Synapses[Wt, syni] < 0 {
		Synapses[Wt, syni] = 0.0
	}
	Synapses[LWt, syni] = Synapses[Wt, syni]
	Synapses[DWt, syni] = 0.0
}

//gosl:end

// todo: rewrite below for PathParams target

// SlowAdapt does the slow adaptation: SWt learning and SynScale
func (pj *Path) SlowAdapt(ctx *Context) {
	pj.SWtFromWt(ctx)
	pj.SynScale(ctx)
}

// SWtFromWt updates structural, slowly adapting SWt value based on
// accumulated DSWt values, which are zero-summed with additional soft bounding
// relative to SWt limits.
func (pj *Path) SWtFromWt(ctx *Context) {
	if pj.Params.Learn.Learn.IsFalse() || pj.Params.SWts.Adapt.On.IsFalse() {
		return
	}
	rlay := pj.Recv
	if rlay.Params.IsTarget() {
		return
	}
	mx := pj.Params.SWts.Limit.Max
	mn := pj.Params.SWts.Limit.Min
	lr := pj.Params.SWts.Adapt.LRate
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		syIndexes := pj.RecvSynIxs(lni)
		nCons := len(syIndexes)
		if nCons < 1 {
			continue
		}
		avgDWt := float32(0)
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			swt := Synapses[SWt, syni]
			// softbound for SWt
			if Synapses[DSWt, syni] >= 0 {
				Synapses[DSWt, syni] *= (mx - swt)
			} else {
				Synapses[DSWt, syni] *= (swt - mn)
			}
			avgDWt += Synapses[DSWt, syni]
		}
		avgDWt /= float32(nCons)
		avgDWt *= pj.Params.SWts.Adapt.SubMean
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			Synapses[SWt, syni] += lr * (Synapses[DSWt, syni] - avgDWt)
			swt := Synapses[SWt, syni]
			Synapses[DSWt, syni] = 0
			Synapses[LWt, syni] = pj.Params.SWts.LWtFromWts(Synapses[Wt, syni], swt)
			Synapses[Wt, syni] = pj.Params.SWts.WtValue(swt, Synapses[LWt, syni])
		}
	}
}

// SynScale performs synaptic scaling based on running average activation vs. targets.
// Layer-level AvgDifFromTrgAvg function must be called first.
func (pj *Path) SynScale(ctx *Context) {
	if pj.Params.Learn.Learn.IsFalse() || pj.Params.IsInhib() {
		return
	}
	rlay := pj.Recv
	if !rlay.Params.IsLearnTrgAvg() {
		return
	}
	tp := &rlay.Params.Learn.TrgAvgAct
	lr := tp.SynScaleRate
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		ri := rlay.NeurStIndex + lni
		if NrnIsOff(ri) {
			continue
		}
		adif := -lr * NeuronAvgs[AvgDif, ri]
		syIndexes := pj.RecvSynIxs(lni)
		for _, syi := range syIndexes {
			syni := pj.SynStIndex + syi
			lwt := Synapses[LWt, syni]
			swt := Synapses[SWt, syni]
			if adif >= 0 { // key to have soft bounding on lwt here!
				Synapses[LWt, syni] += (1 - lwt) * adif * swt
			} else {
				Synapses[LWt, syni] += lwt * adif * swt
			}
			Synapses[Wt, syni] = pj.Params.SWts.WtValue(swt, Synapses[LWt, syni])
		}
	}
}

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
func (pj *Path) LRateMod(mod float32) {
	pj.Params.Learn.LRate.Mod = mod
	pj.Params.Learn.LRate.Update()
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
func (pj *Path) LRateSched(sched float32) {
	pj.Params.Learn.LRate.Sched = sched
	pj.Params.Learn.LRate.Update()
}
