// Code generated by "goal build"; DO NOT EDIT.
//line learn-net.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

// DWt computes the weight change (learning) based on current
// running-average activation values. Copies synapses back from GPU,
// for case where viewing the synapses.
func (nt *Network) DWt() {
	nix := nt.NetIxs()
	ctx := nt.Context()
	sd := int(nix.NSyns * ctx.NData)
	RunDWtSyn(sd)
	RunDWtFromDiSyn(int(nix.NSyns))
	RunDoneSynapsesTrace()
}

// WtFromDWt updates the weights from delta-weight changes,
// after having done DWt previously.
// Also does SlowUpdate.
func (nt *Network) WtFromDWt() {
	nix := nt.NetIxs()
	RunWtFromDWtLayer(int(nix.NLayers))
	RunDWtSubMeanNeuron(int(nix.NNeurons))
	RunWtFromDWtSyn(int(nix.NSyns))
	nt.SlowUpdate()
	RunDoneSynapses()
}

// DWtToWt computes the weight change (learning) based on current
// running-average activation values, and then WtFromDWt,
// without syncing any synapse-level state.
// This should be used when not viewing the weights.
// Also does SlowUpdate.
func (nt *Network) DWtToWt() {
	nix := nt.NetIxs()
	ctx := nt.Context()
	sd := int(nix.NSyns * ctx.NData)
	RunDWtSyn(sd)
	RunDWtFromDiSyn(int(nix.NSyns))
	RunWtFromDWtLayer(int(nix.NLayers))
	RunDWtSubMeanNeuron(int(nix.NNeurons))
	RunWtFromDWtSyn(int(nix.NSyns))
	nt.SlowUpdate()
	RunDone()
}

// SlowUpdate does ctx.SlowInc() and calls SlowAdapt at SlowInterval
// and AdaptGi at AdaptGiInterval.
func (nt *Network) SlowUpdate() {
	ctx := nt.Context()
	slow, adaptgi := ctx.SlowInc()
	if slow {
		nt.SlowAdapt()
	}
	if adaptgi {
		nt.AdaptGi()
	}
}

// SlowAdapt runs slow adaptation functions associated with sleep,
// including synaptic scaling associated with overall neural activity.
func (nt *Network) SlowAdapt() {
	nix := nt.NetIxs()
	RunSlowAdaptLayer(int(nix.NLayers))
	RunSlowAdaptNeuron(int(nix.NNeurons))
}

// AdaptGi does adapting inhibition at a slower interval.
func (nt *Network) AdaptGi() {
	nix := nt.NetIxs()
	RunAdaptGiLayer(int(nix.NLayers))
}

// LRateMod sets the LRate modulation parameter for Paths, which is
// for dynamic modulation of learning rate (see also LRateSched).
// Updates the effective learning rate factor accordingly.
// Must call ToGPUParams() after once done changing all params.
func (nt *Network) LRateMod(mod float32) {
	for _, ly := range nt.Layers {
		// if ly.Off { // keep all sync'd
		//
		//		continue
		//	}
		ly.LRateMod(mod)
	}
}

// LRateSched sets the schedule-based learning rate multiplier.
// See also LRateMod.
// Updates the effective learning rate factor accordingly.
// Must call ToGPUParams() after once done changing all params.
func (nt *Network) LRateSched(sched float32) {
	for _, ly := range nt.Layers {
		// if ly.Off { // keep all sync'd
		//
		//		continue
		//	}
		ly.LRateSched(sched)
	}
}

// SetSubMean sets the SubMean parameters in all the layers in the network
// trgAvg is for Learn.TrgAvgAct.SubMean
// path is for the paths Learn.DWt.SubMean
// in both cases, it is generally best to have both parameters set to 0
// at the start of learning
func (nt *Network) SetSubMean(trgAvg, path float32) {
	for _, ly := range nt.Layers {
		// if ly.Off { // keep all sync'd
		//
		//		continue
		//	}
		ly.SetSubMean(trgAvg, path)
	}
}

////////  Methods used in MPI computation, which don't depend on MPI specifically

// CollectDWts writes all of the synaptic DWt values to given dwts slice
// which is pre-allocated to given nwts size if dwts is nil,
// in which case the method returns true so that the actual length of
// dwts can be passed next time around.
// Used for MPI sharing of weight changes across processors.
// This Sync's Layers and Synapses from GPU first (nop if not using).
func (nt *Network) CollectDWts(dwts *[]float32) bool {
	RunGPUSync()
	RunDoneLayersSynapses()
	idx := 0
	made := false
	if *dwts == nil {
		nwts := 0
		for _, ly := range nt.Layers {
			nwts += 5                // ActAvgValues
			nwts += int(ly.NNeurons) // ActAvg
			if ly.Params.IsLearnTrgAvg() {
				nwts += int(ly.NNeurons)
			}
			for _, pj := range ly.SendPaths {
				nwts += int(pj.NSyns) + 3 // Scale, AvgAvg, MaxAvg
			}
		}
		*dwts = make([]float32, nwts)
		made = true
	}
	for li, ly := range nt.Layers {
		nn := ly.NNeurons
		(*dwts)[idx+0] = LayerStates.Value(int(li), int(0), int(LayerActMAvg))
		(*dwts)[idx+1] = LayerStates.Value(int(li), int(0), int(LayerActPAvg))
		(*dwts)[idx+2] = LayerStates.Value(int(li), int(0), int(LayerAvgMaxGeM))
		(*dwts)[idx+3] = LayerStates.Value(int(li), int(0), int(LayerAvgMaxGiM))
		(*dwts)[idx+4] = LayerStates.Value(int(li), int(0), int(LayerGiMult))
		idx += 5
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			(*dwts)[idx+int(lni)] = NeuronAvgs.Value(int(ni), int(ActAvg))
		}
		idx += int(nn)
		if ly.Params.IsLearnTrgAvg() {
			for lni := uint32(0); lni < nn; lni++ {
				ni := ly.NeurStIndex + lni
				(*dwts)[idx+int(lni)] = NeuronAvgs.Value(int(ni), int(DTrgAvg))
			}
			idx += int(nn)
		}
		for _, pj := range ly.SendPaths {
			for lni := range pj.SendCon {
				scon := pj.SendCon[lni]
				for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
					syni := pj.SynStIndex + syi
					(*dwts)[idx+int(syi)] = Synapses.Value(int(syni), int(DWt))
					//	if syni < 100 {
					//		fmt.Printf("%d: %d = %g\n", syni, syi, (*dwts)[idx+int(syi)])
					//	}
				}
			}
			idx += int(pj.NSyns)
		}
	}
	return made
}

// SetDWts sets the DWt weight changes from given array of floats, which must be correct size
// navg is the number of processors aggregated in these dwts -- some variables need to be
// averaged instead of summed (e.g., ActAvg)
// This Sync's Layers and Synapses to the GPU after (nop if not using).
func (nt *Network) SetDWts(dwts []float32, navg int) {
	idx := 0
	davg := 1 / float32(navg)
	for li, ly := range nt.Layers {
		nn := ly.NNeurons
		LayerStates.Set(davg*dwts[idx+0], int(li), int(0), int(LayerActMAvg))
		LayerStates.Set(davg*dwts[idx+1], int(li), int(0), int(LayerActPAvg))
		LayerStates.Set(davg*dwts[idx+2], int(li), int(0), int(LayerAvgMaxGeM))
		LayerStates.Set(davg*dwts[idx+3], int(li), int(0), int(LayerAvgMaxGiM))
		LayerStates.Set(davg*dwts[idx+4], int(li), int(0), int(LayerGiMult))
		idx += 5
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			NeuronAvgs.Set(davg*dwts[idx+int(lni)], int(ni), int(ActAvg))
		}
		idx += int(nn)
		if ly.Params.IsLearnTrgAvg() {
			for lni := uint32(0); lni < nn; lni++ {
				ni := ly.NeurStIndex + lni
				NeuronAvgs.Set(dwts[idx+int(lni)], int(ni), int(DTrgAvg))
			}
			idx += int(nn)
		}
		for _, pj := range ly.SendPaths {
			for lni := range pj.SendCon {
				scon := pj.SendCon[lni]
				for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
					syni := pj.SynStIndex + syi
					Synapses.Set(dwts[idx+int(syi)], int(syni), int(DWt))
					//	if syni < 100 {
					//		fmt.Printf("%d: %d = %g = %g\n", syni, syi, dwts[idx+int(syi)], Synapses[syni, DWt])
					//	}
				}
			}
			idx += int(pj.NSyns)
		}
	}
	ToGPULayersSynapses()
	RunGPUSync()
	RunDone()
}

//gosl:start

// DWtSyn is the kernel over Synapses * Data to
// compute weight changes (learning).
func DWtSyn(i uint32) { //gosl:kernel
	ctx := GetCtx(0)
	syni := ctx.ItemIndex(i)
	if syni >= NetworkIxs[0].NSyns {
		return
	}
	di := ctx.DataIndex(i)
	pti := SynapseIxs.Value(int(syni), int(SynPathIndex))
	si := SynapseIxs.Value(int(syni), int(SynSendIndex))
	ri := SynapseIxs.Value(int(syni), int(SynRecvIndex))
	Paths[pti].DWtSyn(ctx, &Layers[Paths[pti].Indexes.RecvLayer], syni, si, ri, di)
}

// DWtFromDiSyn is the kernel over Synapses (not * Data) to
// integrate DWt over Di data parallel values.
func DWtFromDiSyn(syni uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if syni >= NetworkIxs[0].NSyns {
		return
	}
	pti := SynapseIxs.Value(int(syni), int(SynPathIndex))
	Paths[pti].DWtFromDi(ctx, syni)
}

// WtFromDWtLayer is the kernel over Layers for layer-level Wt update.
// Does TrgAvg updating.
func WtFromDWtLayer(li uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if li >= NetworkIxs[0].NLayers {
		return
	}
	Layers[li].WtFromDWtLayer(ctx)
}

// DWtSubMeanNeuron is the kernel over Paths to
// compute DWt - mean(DWt) for each recv neuron.
func DWtSubMeanNeuron(ni uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if ni >= NetworkIxs[0].NNeurons {
		return
	}
	li := NeuronIxs.Value(int(ni), int(NrnLayIndex))
	Layers[li].DWtSubMean(ctx, ni)
}

// WtFromDWtSyn is the kernel over Synapses (not * Data) to
// compute Wt from DWt weight changes.
func WtFromDWtSyn(syni uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if syni >= NetworkIxs[0].NSyns {
		return
	}
	pti := SynapseIxs.Value(int(syni), int(SynPathIndex))
	Paths[pti].WtFromDWtSyn(ctx, syni)
}

// SlowAdaptLayer is the kernel over Layers (not * Data) to
// run slow adaptation functions.
// Calls AvgDifFromTrgAvg for Synaptic Scaling.
func SlowAdaptLayer(li uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if li >= NetworkIxs[0].NLayers {
		return
	}
	Layers[li].SlowAdaptLayer(ctx)
}

// SlowAdaptNeuron is the kernel over receiving Neurons to
// compute slow adaptation in receiving pathways.
func SlowAdaptNeuron(ni uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if ni >= NetworkIxs[0].NNeurons {
		return
	}
	li := NeuronIxs.Value(int(ni), int(NrnLayIndex))
	Layers[li].SlowAdaptNeuron(ctx, ni)
}

// AdaptGiLayer is the kernel over Layers (not * Data) to
// run adaptating inhibition function.
func AdaptGiLayer(li uint32) { //gosl:kernel
	ctx := GetCtx(0)
	if li >= NetworkIxs[0].NLayers {
		return
	}
	Layers[li].AdaptGi(ctx)
}

//gosl:end
