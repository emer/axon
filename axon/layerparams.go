// Code generated by "goal build"; DO NOT EDIT.

// Copyright (c) 2023, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"encoding/json"

	"cogentcore.org/core/math32"
)

//gosl:start

// LayerIndexes contains index access into network global arrays for GPU.
type LayerIndexes struct {

	// layer index
	LayIndex uint32 `edit:"-"`

	// maximum number of data parallel elements
	MaxData uint32 `edit:"-"`

	// start of pools for this layer -- first one is always the layer-wide pool
	PoolSt uint32 `edit:"-"`

	// start of neurons for this layer in global array (same as Layer.NeurStIndex)
	NeurSt uint32 `edit:"-"`

	// number of neurons in layer
	NeurN uint32 `edit:"-"`

	// start index into RecvPaths global array
	RecvSt uint32 `edit:"-"`

	// number of recv pathways
	RecvN uint32 `edit:"-"`

	// start index into RecvPaths global array
	SendSt uint32 `edit:"-"`

	// number of recv pathways
	SendN uint32 `edit:"-"`

	// starting index in network global Exts list of external input for this layer -- only for Input / Target / Compare layer types
	ExtsSt uint32 `edit:"-"`

	// layer shape Pools Y dimension -- 1 for 2D
	ShpPlY int32 `edit:"-"`

	// layer shape Pools X dimension -- 1 for 2D
	ShpPlX int32 `edit:"-"`

	// layer shape Units Y dimension
	ShpUnY int32 `edit:"-"`

	// layer shape Units X dimension
	ShpUnX int32 `edit:"-"`

	pad, pad1 uint32
}

// PoolIndex returns the global network index for pool with given
// pool (0 = layer pool, 1+ = subpools) and data parallel indexes
func (lx *LayerIndexes) PoolIndex(pi, di uint32) uint32 {
	return lx.PoolSt + pi*lx.MaxData + di
}

// ValuesIndex returns the global network index for LayerValues with given
// data parallel index.
func (lx *LayerIndexes) ValuesIndex(di uint32) uint32 {
	return lx.LayIndex*lx.MaxData + di
}

// ExtIndex returns the index for accessing Exts values: [Neuron][Data]
// Neuron is *layer-relative* lni index -- add the ExtsSt for network level access.
func (lx *LayerIndexes) ExtIndex(ni, di uint32) uint32 {
	return ni*lx.MaxData + di
}

// LayerInhibIndexes contains indexes of layers for between-layer inhibition
type LayerInhibIndexes struct {

	// idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib1Name if present -- -1 if not used
	Index1 int32 `edit:"-"`

	// idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib2Name if present -- -1 if not used
	Index2 int32 `edit:"-"`

	// idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib3Name if present -- -1 if not used
	Index3 int32 `edit:"-"`

	// idx of Layer to geta layer-level inhibition from -- set during Build from BuildConfig LayInhib4Name if present -- -1 if not used
	Index4 int32 `edit:"-"`
}

// note: the following must appear above LayerParams for GPU usage which is order sensitive

// SetNeuronExtPosNeg sets neuron Ext value based on neuron index
// with positive values going in first unit, negative values rectified
// to positive in 2nd unit
func SetNeuronExtPosNeg(ctx *Context, ni, di uint32, val float32) {
	if ni == 0 {
		if val >= 0 {
			Neurons.Set(val, int(Ext), int(ni), int(di))
		} else {
			Neurons.Set(0, int(Ext), int(ni), int(di))
		}
	} else {
		if val >= 0 {
			Neurons.Set(0, int(Ext), int(ni), int(di))
		} else {
			Neurons.Set(-val, int(Ext), int(ni), int(di))
		}
	}
}

// LayerParams contains all of the layer parameters.
// These values must remain constant over the course of computation.
// On the GPU, they are loaded into a uniform.
type LayerParams struct {

	// functional type of layer -- determines functional code path for specialized layer types, and is synchronized with the Layer.Type value
	LayType LayerTypes

	pad, pad1, pad2 int32

	// Activation parameters and methods for computing activations
	Acts ActParams `display:"add-fields"`

	// Inhibition parameters and methods for computing layer-level inhibition
	Inhib InhibParams `display:"add-fields"`

	// indexes of layers that contribute between-layer inhibition to this layer -- set these indexes via BuildConfig LayInhibXName (X = 1, 2...)
	LayInhib LayerInhibIndexes `display:"inline"`

	// Learning parameters and methods that operate at the neuron level
	Learn LearnNeurParams `display:"add-fields"`

	// BurstParams determine how the 5IB Burst activation is computed from CaSpkP integrated spiking values in Super layers -- thresholded.
	Bursts BurstParams `display:"inline"`

	// ] params for the CT corticothalamic layer and PTPred layer that generates predictions over the Pulvinar using context -- uses the CtxtGe excitatory input plus stronger NMDA channels to maintain context trace
	CT CTParams `display:"inline"`

	// provides parameters for how the plus-phase (outcome) state of Pulvinar thalamic relay cell neurons is computed from the corresponding driver neuron Burst activation (or CaSpkP if not Super)
	Pulv PulvParams `display:"inline"`

	// parameters for BG Striatum Matrix MSN layers, which are the main Go / NoGo gating units in BG.
	Matrix MatrixParams `display:"inline"`

	// type of GP Layer.
	GP GPParams `display:"inline"`

	// parameterizes laterodorsal tegmentum ACh salience neuromodulatory signal, driven by superior colliculus stimulus novelty, US input / absence, and OFC / ACC inhibition
	LDT LDTParams `display:"inline"`

	// parameterizes computing overall VTA DA based on LHb PVDA (primary value -- at US time, computed at start of each trial and stored in LHbPVDA global value) and Amygdala (CeM) CS / learned value (LV) activations, which update every cycle.
	VTA VTAParams `display:"inline"`

	// parameterizes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework).
	RWPred RWPredParams `display:"inline"`

	// parameterizes reward prediction dopamine for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework).
	RWDa RWDaParams `display:"inline"`

	// parameterizes TD reward integration layer
	TDInteg TDIntegParams `display:"inline"`

	// parameterizes dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase.
	TDDa TDDaParams `display:"inline"`

	// recv and send pathway array access info
	Indexes LayerIndexes
}

func (ly *LayerParams) Update() {
	ly.Acts.Update()
	ly.Inhib.Update()
	ly.Learn.Update()

	ly.Bursts.Update()
	ly.CT.Update()
	ly.Pulv.Update()

	ly.Matrix.Update()
	ly.GP.Update()

	ly.LDT.Update()
	ly.VTA.Update()

	ly.RWPred.Update()
	ly.RWDa.Update()
	ly.TDInteg.Update()
	ly.TDDa.Update()
}

func (ly *LayerParams) Defaults() {
	ly.Acts.Defaults()
	ly.Inhib.Defaults()
	ly.Learn.Defaults()
	ly.Inhib.Layer.On.SetBool(true)
	ly.Inhib.Layer.Gi = 1.0
	ly.Inhib.Pool.Gi = 1.0

	ly.Bursts.Defaults()
	ly.CT.Defaults()
	ly.Pulv.Defaults()

	ly.Matrix.Defaults()
	ly.GP.Defaults()

	ly.LDT.Defaults()
	ly.VTA.Defaults()

	ly.RWPred.Defaults()
	ly.RWDa.Defaults()
	ly.TDInteg.Defaults()
	ly.TDDa.Defaults()
}

func (ly *LayerParams) ShouldDisplay(field string) bool {
	switch field {
	case "Bursts":
		return ly.LayType == SuperLayer
	case "CT":
		return ly.LayType == CTLayer || ly.LayType == PTPredLayer || ly.LayType == BLALayer
	case "Pulv":
		return ly.LayType == PulvinarLayer
	case "Matrix":
		return ly.LayType == MatrixLayer
	case "GP":
		return ly.LayType == GPLayer
	case "LDT":
		return ly.LayType == LDTLayer
	case "VTA":
		return ly.LayType == VTALayer
	case "RWPred":
		return ly.LayType == RWPredLayer
	case "RWDa":
		return ly.LayType == RWDaLayer
	case "TDInteg":
		return ly.LayType == TDIntegLayer
	case "TDDa":
		return ly.LayType == TDDaLayer
	default:
		return true
	}
}

// AllParams returns a listing of all parameters in the Layer
func (ly *LayerParams) AllParams() string {
	str := ""
	// todo: replace with a custom reflection crawler that generates
	// the right output directly and filters based on LayType etc.

	b, _ := json.MarshalIndent(&ly.Acts, "", " ")
	str += "Act: {\n " + JsonToParams(b)
	b, _ = json.MarshalIndent(&ly.Inhib, "", " ")
	str += "Inhib: {\n " + JsonToParams(b)
	b, _ = json.MarshalIndent(&ly.Learn, "", " ")
	str += "Learn: {\n " + JsonToParams(b)

	switch ly.LayType {
	case SuperLayer:
		b, _ = json.MarshalIndent(&ly.Bursts, "", " ")
		str += "Burst:   {\n " + JsonToParams(b)
	case CTLayer, PTPredLayer, BLALayer:
		b, _ = json.MarshalIndent(&ly.CT, "", " ")
		str += "CT:      {\n " + JsonToParams(b)
	case PulvinarLayer:
		b, _ = json.MarshalIndent(&ly.Pulv, "", " ")
		str += "Pulv:    {\n " + JsonToParams(b)

	case MatrixLayer:
		b, _ = json.MarshalIndent(&ly.Matrix, "", " ")
		str += "Matrix:  {\n " + JsonToParams(b)
	case GPLayer:
		b, _ = json.MarshalIndent(&ly.GP, "", " ")
		str += "GP:      {\n " + JsonToParams(b)

	case LDTLayer:
		b, _ = json.MarshalIndent(&ly.LDT, "", " ")
		str += "LDT: {\n " + JsonToParams(b)
	case VTALayer:
		b, _ = json.MarshalIndent(&ly.VTA, "", " ")
		str += "VTA: {\n " + JsonToParams(b)

	case RWPredLayer:
		b, _ = json.MarshalIndent(&ly.RWPred, "", " ")
		str += "RWPred:  {\n " + JsonToParams(b)
	case RWDaLayer:
		b, _ = json.MarshalIndent(&ly.RWDa, "", " ")
		str += "RWDa:    {\n " + JsonToParams(b)
	case TDIntegLayer:
		b, _ = json.MarshalIndent(&ly.TDInteg, "", " ")
		str += "TDInteg: {\n " + JsonToParams(b)
	case TDDaLayer:
		b, _ = json.MarshalIndent(&ly.TDDa, "", " ")
		str += "TDDa:    {\n " + JsonToParams(b)
	}
	return str
}

//////////////////////////////////////////////////////////////////////////////////////
//  ApplyExt

// ApplyExtFlags gets the clear mask and set mask for updating neuron flags
// based on layer type, and whether input should be applied to Target (else Ext)
func (ly *LayerParams) ApplyExtFlags(clearMask, setMask *NeuronFlags, toTarg *bool) {
	*clearMask = NeuronHasExt | NeuronHasTarg | NeuronHasCmpr
	*toTarg = false
	switch ly.LayType {
	case TargetLayer:
		*setMask = NeuronHasTarg
		*toTarg = true
	case CompareLayer:
		*setMask = NeuronHasCmpr
		*toTarg = true
	default:
		*setMask = NeuronHasExt
	}
	return
}

// InitExt initializes external input state for given neuron
func (ly *LayerParams) InitExt(ctx *Context, ni, di uint32) {
	Neurons.Set(0, int(Ext), int(ni), int(di))
	Neurons.Set(0, int(Target), int(ni), int(di))
	NrnClearFlag(ctx, ni, di, NeuronHasExt|NeuronHasTarg|NeuronHasCmpr)
}

// ApplyExtVal applies given external value to given neuron,
// setting flags based on type of layer.
// Should only be called on Input, Target, Compare layers.
// Negative values are not valid, and will be interpreted as missing inputs.
func (ly *LayerParams) ApplyExtValue(ctx *Context, ni, di uint32, val float32) {
	if val < 0 {
		return
	}
	var clearMask, setMask NeuronFlags
	var toTarg bool
	ly.ApplyExtFlags(&clearMask, &setMask, &toTarg)
	if toTarg {
		Neurons.Set(val, int(Target), int(ni), int(di))
	} else {
		Neurons.Set(val, int(Ext), int(ni), int(di))
	}
	NrnClearFlag(ctx, ni, di, clearMask)
	NrnSetFlag(ctx, ni, di, setMask)
}

// IsTarget returns true if this layer is a Target layer.
// By default, returns true for layers of Type == TargetLayer
// Other Target layers include the TRCLayer in deep predictive learning.
// It is used in SynScale to not apply it to target layers.
// In both cases, Target layers are purely error-driven.
func (ly *LayerParams) IsTarget() bool {
	switch ly.LayType {
	case TargetLayer:
		return true
	case PulvinarLayer:
		return true
	default:
		return false
	}
}

// IsInput returns true if this layer is an Input layer.
// By default, returns true for layers of Type == axon.InputLayer
// Used to prevent adapting of inhibition or TrgAvg values.
func (ly *LayerParams) IsInput() bool {
	switch ly.LayType {
	case InputLayer:
		return true
	default:
		return false
	}
}

// IsInputOrTarget returns true if this layer is either an Input
// or a Target layer.
func (ly *LayerParams) IsInputOrTarget() bool {
	return (ly.IsTarget() || ly.IsInput())
}

// IsLearnTrgAvg returns true if this layer has Learn.TrgAvgAct.RescaleOn set for learning
// adjustments based on target average activity levels, and the layer is not an
// input or target layer.
func (ly *LayerParams) IsLearnTrgAvg() bool {
	if ly.Acts.Clamp.IsInput.IsTrue() || ly.Acts.Clamp.IsTarget.IsTrue() || ly.Learn.TrgAvgAct.RescaleOn.IsFalse() {
		return false
	}
	return true
}

// LearnTrgAvgErrLRate returns the effective error-driven learning rate for adjusting
// target average activity levels.  This is 0 if !IsLearnTrgAvg() and otherwise
// is Learn.TrgAvgAct.ErrLRate
func (ly *LayerParams) LearnTrgAvgErrLRate() float32 {
	if !ly.IsLearnTrgAvg() {
		return 0
	}
	return ly.Learn.TrgAvgAct.ErrLRate
}

//////////////////////////////////////////////////////////////////////////////////////
//  Cycle methods

// LayPoolGiFromSpikes computes inhibition Gi from Spikes for layer-level pool.
// Also grabs updated Context NeuroMod values into LayerValues
func (ly *LayerParams) LayPoolGiFromSpikes(ctx *Context, lpl *Pool, vals *LayerValues) {
	lpl.Inhib.SpikesFromRaw(lpl.NNeurons())
	ly.Inhib.Layer.Inhib(&lpl.Inhib, vals.ActAvg.GiMult)
}

// SubPoolGiFromSpikes computes inhibition Gi from Spikes within a sub-pool
// pl is guaranteed not to be the overall layer pool
func (ly *LayerParams) SubPoolGiFromSpikes(ctx *Context, di uint32, pl *Pool, lpl *Pool, lyInhib bool, giMult float32) {
	pl.Inhib.SpikesFromRaw(pl.NNeurons())
	ly.Inhib.Pool.Inhib(&pl.Inhib, giMult)
	if lyInhib {
		pl.Inhib.LayerMax(lpl.Inhib.Gi) // note: this requires lpl inhib to have been computed before!
	} else {
		lpl.Inhib.PoolMax(pl.Inhib.Gi) // display only
		lpl.Inhib.SaveOrig()           // effective GiOrig
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  CycleNeuron methods

// GatherSpikesInit initializes G*Raw and G*Syn values for given neuron
// prior to integration
func (ly *LayerParams) GatherSpikesInit(ctx *Context, ni, di uint32) {
	Neurons.Set(0, int(GeRaw), int(ni), int(di))
	Neurons.Set(0, int(GiRaw), int(ni), int(di))
	Neurons.Set(0, int(GModRaw), int(ni), int(di))
	Neurons.Set(0, int(GModSyn), int(ni), int(di))
	Neurons.Set(0, int(GMaintRaw), int(ni), int(di))
	Neurons.Set(0, int(CtxtGeRaw), int(ni), int(di))
	Neurons.Set(NeuronAvgs[GeBase, ni], int(GeSyn), int(ni), int(di))
	Neurons.Set(NeuronAvgs[GiBase, ni], int(GiSyn), int(ni), int(di))
}

////////////////////////
//  GInteg

// SpecialPreGs is used for special layer types to do things to the
// conductance values prior to doing the standard updates in GFromRawSyn
// drvAct is for Pulvinar layers, activation of driving neuron
func (ly *LayerParams) SpecialPreGs(ctx *Context, ni, di uint32, pl *Pool, vals *LayerValues, drvGe float32, nonDrivePct float32) float32 {
	saveVal := float32(0)               // sometimes we need to use a value computed here, for the post Gs step
	pi := NrnI(ctx, ni, NrnSubPool) - 1 // 0-n pool index
	pni := NrnI(ctx, ni, NrnNeurIndex) - pl.StIndex
	nrnCtxtGe := Neurons.Value(int(CtxtGe), int(ni), int(di))
	nrnGeRaw := Neurons.Value(int(GeRaw), int(ni), int(di))
	hasRew := GlbV(ctx, di, GvHasRew) > 0
	switch ly.LayType {
	case CTLayer:
		fallthrough
	case PTPredLayer:
		geCtxt := ly.CT.GeGain * nrnCtxtGe
		Neurons.SetAdd(geCtxt, int(GeRaw), int(ni), int(di))
		if ly.CT.DecayDt > 0 {
			Neurons.SetSub(ly.CT.DecayDt*nrnCtxtGe, int(CtxtGe), int(ni), int(di))
		}
		ctxExt := ly.Acts.Dt.GeSynFromRawSteady(geCtxt)
		Neurons.SetAdd(ctxExt, int(GeSyn), int(ni), int(di))
		saveVal = ctxExt // used In PostGs to set nrn.GeExt
	case PTMaintLayer:
		if ly.Acts.SMaint.On.IsTrue() {
			saveVal = ly.Acts.SMaint.Inhib * Neurons.Value(int(GMaintRaw), int(ni), int(di)) // used In PostGs to set nrn.GeExt
		}
	case PulvinarLayer:
		if ctx.PlusPhase.IsFalse() {
			break
		}
		// geSyn, goes into nrn.GeExt in PostGs, so inhibition gets it
		saveVal = nonDrivePct*Neurons.Value(int(GeSyn), int(ni), int(di)) + ly.Acts.Dt.GeSynFromRawSteady(drvGe)
		Neurons.Set(nonDrivePct*nrnGeRaw+drvGe, int(GeRaw), int(ni), int(di))
		Neurons.Set(saveVal, int(GeSyn), int(ni), int(di))
	case VSGatedLayer:
		dr := float32(0)
		if pi == 0 {
			dr = GlbV(ctx, di, GvVSMatrixJustGated)
		} else {
			dr = GlbV(ctx, di, GvVSMatrixHasGated)
		}
		dr = math32.Abs(dr)
		Neurons.Set(dr, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(dr), int(GeSyn), int(ni), int(di))

	case BLALayer:
		if ly.Learn.NeuroMod.IsBLAExt() {
			mod := max(-GlbV(ctx, di, GvDA), 0) // ext is modulated by negative da
			geCtxt := mod * ly.CT.GeGain * Neurons.Value(int(CtxtGeOrig), int(ni), int(di))
			Neurons.SetAdd(geCtxt, int(GeRaw), int(ni), int(di))
			ctxExt := ly.Acts.Dt.GeSynFromRawSteady(geCtxt)
			Neurons.SetAdd(ctxExt, int(GeSyn), int(ni), int(di))
			saveVal = ctxExt // used In PostGs to set nrn.GeExt
		}
	case LHbLayer:
		geRaw := float32(0)
		if ni == 0 {
			geRaw = 0.2 * math32.Abs(GlbV(ctx, di, GvLHbDip))
		} else {
			geRaw = 0.2 * math32.Abs(GlbV(ctx, di, GvLHbBurst))
		}
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case DrivesLayer:
		dr := GlbUSposV(ctx, di, GvDrives, uint32(pi))
		geRaw := dr
		if dr > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, uint32(pl.NNeurons()), dr)
		}
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case UrgencyLayer:
		ur := GlbV(ctx, di, GvUrgency)
		geRaw := ur
		if ur > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, uint32(pl.NNeurons()), ur)
		}
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case USLayer:
		us := RubiconUSStimValue(ctx, di, pi, ly.Learn.NeuroMod.Valence)
		geRaw := us
		if us > 0 {
			geRaw = ly.Acts.PopCode.EncodeGe(pni, uint32(pl.NNeurons()), us)
		}
		// D2Mod = final
		if ly.Learn.NeuroMod.DAMod == D1Mod || (ly.Learn.NeuroMod.DAMod == D2Mod && hasRew && ctx.PlusPhase.IsTrue()) {
			Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
			Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
		}
	case PVLayer:
		if hasRew && ctx.PlusPhase.IsTrue() {
			pv := float32(0)
			if ly.Learn.NeuroMod.Valence == Positive {
				pv = GlbV(ctx, di, GvPVpos)
			} else {
				pv = GlbV(ctx, di, GvPVneg)
			}
			pc := ly.Acts.PopCode.EncodeGe(pni, ly.Indexes.NeurN, pv)
			Neurons.Set(pc, int(GeRaw), int(ni), int(di))
			Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(pc), int(GeSyn), int(ni), int(di))
		}
	case LDTLayer:
		geRaw := 0.4 * GlbV(ctx, di, GvACh)
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case VTALayer:
		geRaw := ly.RWDa.GeFromDA(GlbV(ctx, di, GvVtaDA))
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))

	case RewLayer:
		NrnSetFlag(ctx, ni, di, NeuronHasExt)
		SetNeuronExtPosNeg(ctx, ni, di, GlbV(ctx, di, GvRew)) // Rew must be set in Context!
	case RWDaLayer:
		geRaw := ly.RWDa.GeFromDA(GlbV(ctx, di, GvDA))
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case TDDaLayer:
		geRaw := ly.TDDa.GeFromDA(GlbV(ctx, di, GvDA))
		Neurons.Set(geRaw, int(GeRaw), int(ni), int(di))
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(geRaw), int(GeSyn), int(ni), int(di))
	case TDIntegLayer:
		NrnSetFlag(ctx, ni, di, NeuronHasExt)
		SetNeuronExtPosNeg(ctx, ni, di, GlbV(ctx, di, GvRewPred))
	}
	return saveVal
}

// SpecialPostGs is used for special layer types to do things
// after the standard updates in GFromRawSyn.
// It is passed the saveVal from SpecialPreGs
func (ly *LayerParams) SpecialPostGs(ctx *Context, ni, di uint32, saveVal float32) {
	switch ly.LayType {
	case BLALayer:
		fallthrough
	case CTLayer:
		fallthrough
	case PTMaintLayer:
		fallthrough
	case PulvinarLayer:
		Neurons.Set(saveVal, int(GeExt), int(ni), int(di))
	case PTPredLayer:
		Neurons.Set(saveVal, int(GeExt), int(ni), int(di))
		orig := Neurons.Value(int(CtxtGeOrig), int(ni), int(di))
		if orig < 0.05 {
			Neurons.Set(0, int(Ge), int(ni), int(di))
		}
	}
}

// GFromRawSyn computes overall Ge and GiSyn conductances for neuron
// from GeRaw and GeSyn values, including NMDA, VGCC, AMPA, and GABA-A channels.
// drvAct is for Pulvinar layers, activation of driving neuron
func (ly *LayerParams) GFromRawSyn(ctx *Context, ni, di uint32) {
	extraRaw := float32(0)
	extraSyn := float32(0)
	nrnGModRaw := Neurons.Value(int(GModRaw), int(ni), int(di))
	nrnGModSyn := Neurons.Value(int(GModSyn), int(ni), int(di))
	ach := GlbV(ctx, di, GvACh)
	switch ly.LayType {
	case PTMaintLayer:
		mod := ly.Acts.Dend.ModGain * nrnGModSyn
		if ly.Acts.Dend.ModACh.IsTrue() {
			mod *= ach
		}
		mod += ly.Acts.Dend.ModBase
		// key: excluding GModMaint here, so active maintenance can persist
		Neurons.SetMul(mod, int(GeRaw), int(ni), int(di))
		Neurons.SetMul(mod, int(GeSyn), int(ni), int(di))
		extraRaw = ly.Acts.Dend.ModGain * nrnGModRaw
		if ly.Acts.Dend.ModACh.IsTrue() {
			extraRaw *= ach
		}
		extraSyn = mod
	case BLALayer:
		// modulatory pathway from PTp is only used so we can modulate by da
		mod := max(-GlbV(ctx, di, GvDA), 0) // ext is modulated by negative da
		extraRaw = mod * nrnGModRaw * ly.Acts.Dend.ModGain
		extraSyn = mod * nrnGModSyn * ly.Acts.Dend.ModGain
	default:
		if ly.Acts.Dend.HasMod.IsTrue() {
			mod := ly.Acts.Dend.ModBase + ly.Acts.Dend.ModGain*nrnGModSyn
			if mod > 1 {
				mod = 1
			}
			Neurons.SetMul(mod, int(GeRaw), int(ni), int(di))
			Neurons.SetMul(mod, int(GeSyn), int(ni), int(di))
		}
	}
	geRaw := Neurons.Value(int(GeRaw), int(ni), int(di))
	geSyn := Neurons.Value(int(GeSyn), int(ni), int(di))
	ly.Acts.NMDAFromRaw(ctx, ni, di, geRaw+extraRaw)
	ly.Acts.MaintNMDAFromRaw(ctx, ni, di) // uses GMaintRaw directly
	ly.Learn.LrnNMDAFromRaw(ctx, ni, di, geRaw)
	ly.Acts.GvgccFromVm(ctx, ni, di)
	ege := Neurons.Value(int(Gnmda), int(ni), int(di)) + Neurons.Value(int(GnmdaMaint), int(ni), int(di)) + Neurons.Value(int(Gvgcc), int(ni), int(di)) + extraSyn
	ly.Acts.GeFromSyn(ctx, ni, di, geSyn, ege) // sets nrn.GeExt too
	ly.Acts.GkFromVm(ctx, ni, di)
	ly.Acts.GSkCaFromCa(ctx, ni, di)
	Neurons.Set(ly.Acts.GiFromSyn(ctx, ni, di, Neurons[GiSyn, ni, di]), int(GiSyn), int(ni), int(di))
}

// GiInteg adds Gi values from all sources including SubPool computed inhib
// and updates GABAB as well
func (ly *LayerParams) GiInteg(ctx *Context, ni, di uint32, pl *Pool, vals *LayerValues) {
	gi := vals.ActAvg.GiMult*pl.Inhib.Gi + Neurons.Value(int(GiSyn), int(ni), int(di)) + Neurons.Value(int(GiNoise), int(ni), int(di)) + ly.Learn.NeuroMod.GiFromACh(GlbV(ctx, di, GvACh))
	Neurons.Set(gi, int(Gi), int(ni), int(di))
	Neurons.Set(pl.Inhib.SSGi, int(SSGi), int(ni), int(di))
	Neurons.Set(0, int(SSGiDend), int(ni), int(di))
	if ctx.PlusPhase.IsTrue() && ly.LayType == PulvinarLayer {
		ext := Neurons.Value(int(Ext), int(ni), int(di)) // nonDrivePct
		Neurons.Set(ext*ly.Acts.Dend.SSGi*pl.Inhib.SSGi, int(SSGiDend), int(ni), int(di))
	} else {
		if !(ly.Acts.Clamp.IsInput.IsTrue() || ly.Acts.Clamp.IsTarget.IsTrue()) {
			Neurons.Set(ly.Acts.Dend.SSGi*pl.Inhib.SSGi, int(SSGiDend), int(ni), int(di))
		}
	}
	vm := Neurons.Value(int(VmDend), int(ni), int(di))
	nrnGABAB := Neurons.Value(int(GABAB), int(ni), int(di))
	nrnGABABx := Neurons.Value(int(GABABx), int(ni), int(di))
	ly.Acts.GabaB.GABAB(gi, &nrnGABAB, &nrnGABABx)
	Neurons.Set(nrnGABAB, int(GABAB), int(ni), int(di))
	Neurons.Set(nrnGABABx, int(GABABx), int(ni), int(di))
	nrnGgabaB := ly.Acts.GabaB.GgabaB(nrnGABAB, vm)
	Neurons.Set(nrnGgabaB, int(GgabaB), int(ni), int(di))
	// Gk was already init
	Neurons.SetAdd(nrnGgabaB, int(Gk), int(ni), int(di))
}

// GNeuroMod does neuromodulation of conductances
func (ly *LayerParams) GNeuroMod(ctx *Context, ni, di uint32, vals *LayerValues) {
	ggain := ly.Learn.NeuroMod.GGain(GlbV(ctx, di, GvDA) + GlbV(ctx, di, GvDAtonic))
	Neurons.SetMul(ggain, int(Ge), int(ni), int(di))
	Neurons.SetMul(ggain, int(Gi), int(ni), int(di))
}

////////////////////////
//  SpikeFromG

// SpikeFromG computes Vm from Ge, Gi, Gl conductances and then Spike from that
func (ly *LayerParams) SpikeFromG(ctx *Context, ni, di uint32, lpl *Pool) {
	ly.Acts.VmFromG(ctx, ni, di)
	ly.Acts.SpikeFromVm(ctx, ni, di)
	ly.Learn.CaFromSpike(ctx, ni, di)
	lmax := lpl.AvgMax.GeInt.Cycle.Max
	if lmax > 0 {
		Neurons.Set(Neurons[GeInt, ni, di]/lmax, int(GeIntNorm), int(ni), int(di))
	} else {
		Neurons.Set(Neurons[GeInt, ni, di], int(GeIntNorm), int(ni), int(di))
	}
	if ctx.Cycle >= ly.Acts.Dt.MaxCycStart {
		Neurons.SetAdd(ly.Learn.CaSpk.Dt.PDt*(Neurons[CaSpkM, ni, di]-Neurons[SpkMaxCa, ni, di]), int(SpkMaxCa), int(ni), int(di))
		spkmax := Neurons.Value(int(SpkMaxCa), int(ni), int(di))
		if spkmax > Neurons.Value(int(SpkMax), int(ni), int(di)) {
			Neurons.Set(spkmax, int(SpkMax), int(ni), int(di))
		}
	}
	spksper := ctx.ThetaCycles / 8
	bin := ctx.Cycle / spksper
	spk := Neurons.Value(int(Spike), int(ni), int(di))
	switch bin {
	case 0:
		Neurons.SetAdd(spk, int(SpkBin0), int(ni), int(di))
	case 1:
		Neurons.SetAdd(spk, int(SpkBin1), int(ni), int(di))
	case 2:
		Neurons.SetAdd(spk, int(SpkBin2), int(ni), int(di))
	case 3:
		Neurons.SetAdd(spk, int(SpkBin3), int(ni), int(di))
	case 4:
		Neurons.SetAdd(spk, int(SpkBin4), int(ni), int(di))
	case 5:
		Neurons.SetAdd(spk, int(SpkBin5), int(ni), int(di))
	case 6:
		Neurons.SetAdd(spk, int(SpkBin6), int(ni), int(di))
	default:
		Neurons.SetAdd(spk, int(SpkBin7), int(ni), int(di))
	}
}

// PostSpikeSpecial does updates at neuron level after spiking has been computed.
// This is where special layer types add extra code.
// warning: if more than 1 layer writes to vals, gpu will fail!
func (ly *LayerParams) PostSpikeSpecial(ctx *Context, ni, di uint32, pl *Pool, lpl *Pool, vals *LayerValues) {
	Neurons.Set(Neurons[CaSpkP, ni, di], int(Burst), int(ni), int(di))
	pi := NrnI(ctx, ni, NrnSubPool) - 1 // 0-n pool index
	pni := NrnI(ctx, ni, NrnNeurIndex) - pl.StIndex
	hasRew := GlbV(ctx, di, GvHasRew) > 0
	switch ly.LayType {
	case SuperLayer:
		if ctx.PlusPhase.IsTrue() {
			actMax := lpl.AvgMax.CaSpkP.Cycle.Max
			actAvg := lpl.AvgMax.CaSpkP.Cycle.Avg
			thr := ly.Bursts.ThrFromAvgMax(actAvg, actMax)
			if Neurons.Value(int(CaSpkP), int(ni), int(di)) < thr {
				Neurons.Set(0, int(Burst), int(ni), int(di))
			}
		}
	case CTLayer:
		fallthrough
	case PTPredLayer:
		if ctx.Cycle == ctx.ThetaCycles-1 {
			if ly.CT.DecayTau == 0 {
				Neurons.Set(Neurons[CtxtGeRaw, ni, di], int(CtxtGe), int(ni), int(di))
			} else {
				Neurons.SetAdd(Neurons[CtxtGeRaw, ni, di], int(CtxtGe), int(ni), int(di))
			}
			Neurons.Set(Neurons[CtxtGe, ni, di], int(CtxtGeOrig), int(ni), int(di))
		}
	case VSGatedLayer:
		dr := float32(0)
		if pi == 0 {
			dr = GlbV(ctx, di, GvVSMatrixJustGated)
		} else {
			dr = GlbV(ctx, di, GvVSMatrixHasGated)
		}
		Neurons.Set(dr, int(Act), int(ni), int(di))

	case BLALayer:
		if ctx.Cycle == ctx.ThetaCycles-1 {
			if hasRew {
				Neurons.Set(0, int(CtxtGe), int(ni), int(di))
				Neurons.Set(0, int(CtxtGeOrig), int(ni), int(di))
			} else if GlbV(ctx, di, GvACh) > 0.1 {
				Neurons.Set(Neurons[CtxtGeRaw, ni, di], int(CtxtGe), int(ni), int(di))
				Neurons.Set(Neurons[CtxtGe, ni, di], int(CtxtGeOrig), int(ni), int(di))
			}
		}
	case LHbLayer:
		if pni == 0 {
			Neurons.Set(GlbV(ctx, di, GvLHbDip), int(Act), int(ni), int(di))
		} else {
			Neurons.Set(GlbV(ctx, di, GvLHbBurst), int(Act), int(ni), int(di))
		}
		Neurons.Set(ly.Acts.Dt.GeSynFromRawSteady(Neurons[GeRaw, ni, di]), int(GeSyn), int(ni), int(di))
	case DrivesLayer:
		dr := GlbUSposV(ctx, di, GvDrives, uint32(pi))
		act := dr
		if dr > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, uint32(pl.NNeurons()), dr)
		}
		Neurons.Set(act, int(Act), int(ni), int(di))
	case UrgencyLayer:
		ur := GlbV(ctx, di, GvUrgency)
		act := ur
		if ur > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, uint32(pl.NNeurons()), ur)
		}
		Neurons.Set(act, int(Act), int(ni), int(di))
	case USLayer:
		us := RubiconUSStimValue(ctx, di, pi, ly.Learn.NeuroMod.Valence)
		act := us
		if us > 0 {
			act = ly.Acts.PopCode.EncodeValue(pni, uint32(pl.NNeurons()), us)
		}
		// D2Mod = final
		if ly.Learn.NeuroMod.DAMod == D1Mod || (ly.Learn.NeuroMod.DAMod == D2Mod && hasRew && ctx.PlusPhase.IsTrue()) {
			Neurons.Set(act, int(Act), int(ni), int(di))
		}
	case PVLayer:
		if hasRew {
			pv := float32(0)
			if ly.Learn.NeuroMod.Valence == Positive {
				pv = GlbV(ctx, di, GvPVpos)
			} else {
				pv = GlbV(ctx, di, GvPVneg)
			}
			act := ly.Acts.PopCode.EncodeValue(pni, ly.Indexes.NeurN, pv)
			Neurons.Set(act, int(Act), int(ni), int(di))
		}
	case LDTLayer:
		// I set this in CyclePost
		Neurons.Set(GlbV(ctx, di, GvAChRaw), int(Act), int(ni), int(di))
	case VTALayer:
		// I set this in CyclePost
		Neurons.Set(GlbV(ctx, di, GvVtaDA), int(Act), int(ni), int(di))

	case RewLayer:
		Neurons.Set(GlbV(ctx, di, GvRew), int(Act), int(ni), int(di))
	case RWPredLayer:
		// clipped linear
		Neurons.Set(ly.RWPred.PredRange.ClipValue(Neurons[Ge, ni, di]), int(Act), int(ni), int(di))
		if pni == 0 {
			vals.Special.V1 = Neurons.Value(int(ActInt), int(ni), int(di)) // warning: if more than 1 layer writes to vals, gpu will fail!
		} else {
			vals.Special.V2 = Neurons.Value(int(ActInt), int(ni), int(di))
		}
	case RWDaLayer:
		// I set this in CyclePost
		Neurons.Set(GlbV(ctx, di, GvDA), int(Act), int(ni), int(di))
	case TDPredLayer:
		// linear
		Neurons.Set(Neurons[Ge, ni, di], int(Act), int(ni), int(di))
		if pni == 0 {
			vals.Special.V1 = Neurons.Value(int(ActInt), int(ni), int(di)) // warning: if more than 1 layer writes to vals, gpu will fail!
		} else {
			vals.Special.V2 = Neurons.Value(int(ActInt), int(ni), int(di))
		}
	case TDIntegLayer:
		Neurons.Set(GlbV(ctx, di, GvRewPred), int(Act), int(ni), int(di))
	case TDDaLayer:
		// I set this in CyclePost
		Neurons.Set(GlbV(ctx, di, GvDA), int(Act), int(ni), int(di))
	}
}

// PostSpike does updates at neuron level after spiking has been computed.
// it is called *after* PostSpikeSpecial.
// It also updates the CaSpkPCyc stats.
func (ly *LayerParams) PostSpike(ctx *Context, ni, di uint32, pl *Pool, vals *LayerValues) {
	intdt := ly.Acts.Dt.IntDt
	Neurons.SetAdd(intdt*(Neurons[Ge, ni, di]-Neurons[GeInt, ni, di]), int(GeInt), int(ni), int(di))
	Neurons.SetAdd(intdt*(Neurons[GiSyn, ni, di]-Neurons[GiInt, ni, di]), int(GiInt), int(ni), int(di))
	// act int is reset at start of the plus phase -- needs faster integration:
	if ctx.PlusPhase.IsTrue() {
		intdt *= 3.0
	}
	// using reg act here now
	Neurons.SetAdd(intdt*(Neurons[Act, ni, di]-Neurons[ActInt, ni, di]), int(ActInt), int(ni), int(di))
}

/////////////////////////////////////////////////////////////////////////
//  Special CyclePost methods for different layer types
//  call these in layer_compute.go/CyclePost and
//  gpu_wgsl/gpu_cyclepost.wgsl

// CyclePostLayer is called for all layer types
func (ly *LayerParams) CyclePostLayer(ctx *Context, di uint32, lpl *Pool, vals *LayerValues) {
	if ctx.Cycle >= ly.Acts.Dt.MaxCycStart && lpl.AvgMax.CaSpkP.Cycle.Max > 0.5 { // todo: ly.Acts.AttnMod.RTThr {
		if vals.RT <= 0 {
			vals.RT = float32(ctx.Cycle)
		}
	}
}

func (ly *LayerParams) CyclePostLDTLayer(ctx *Context, di uint32, vals *LayerValues, srcLay1Act, srcLay2Act, srcLay3Act, srcLay4Act float32) {
	ach := ly.LDT.ACh(ctx, di, srcLay1Act, srcLay2Act, srcLay3Act, srcLay4Act)

	SetGlbV(ctx, di, GvAChRaw, ach)
	if ach > GlbV(ctx, di, GvACh) { // instant up
		SetGlbV(ctx, di, GvACh, ach)
	} else {
		AddGlbV(ctx, di, GvACh, ly.Acts.Dt.IntDt*(ach-GlbV(ctx, di, GvACh)))
	}
}

func (ly *LayerParams) CyclePostRWDaLayer(ctx *Context, di uint32, vals *LayerValues, pvals *LayerValues) {
	pred := pvals.Special.V1 - pvals.Special.V2
	SetGlbV(ctx, di, GvRewPred, pred) // record
	da := float32(0)
	if GlbV(ctx, di, GvHasRew) > 0 {
		da = GlbV(ctx, di, GvRew) - pred
	}
	SetGlbV(ctx, di, GvDA, da) // updates global value that will be copied to layers next cycle.
}

func (ly *LayerParams) CyclePostTDPredLayer(ctx *Context, di uint32, vals *LayerValues) {
	if ctx.PlusPhase.IsTrue() {
		pred := vals.Special.V1 - vals.Special.V2
		SetGlbV(ctx, di, GvPrevPred, pred)
	}
}

func (ly *LayerParams) CyclePostTDIntegLayer(ctx *Context, di uint32, vals *LayerValues, pvals *LayerValues) {
	rew := float32(0)
	if GlbV(ctx, di, GvHasRew) > 0 {
		rew = GlbV(ctx, di, GvRew)
	}
	rpval := float32(0)
	if ctx.PlusPhase.IsTrue() {
		pred := pvals.Special.V1 - pvals.Special.V2 // neuron0 (pos) - neuron1 (neg)
		rpval = rew + ly.TDInteg.Discount*ly.TDInteg.PredGain*pred
		vals.Special.V2 = rpval // plus phase
	} else {
		rpval = ly.TDInteg.PredGain * GlbV(ctx, di, GvPrevPred)
		vals.Special.V1 = rpval // minus phase is *previous trial*
	}
	SetGlbV(ctx, di, GvRewPred, rpval) // global value will be copied to layers next cycle
}

func (ly *LayerParams) CyclePostTDDaLayer(ctx *Context, di uint32, vals *LayerValues, ivals *LayerValues) {
	da := ivals.Special.V2 - ivals.Special.V1
	if ctx.PlusPhase.IsFalse() {
		da = 0
	}
	SetGlbV(ctx, di, GvDA, da) // updates global value that will be copied to layers next cycle.
}

func (ly *LayerParams) CyclePostCeMLayer(ctx *Context, di uint32, lpl *Pool) {
	if ly.Learn.NeuroMod.Valence == Positive {
		SetGlbV(ctx, di, GvCeMpos, lpl.AvgMax.CaSpkD.Cycle.Max)
	} else {
		SetGlbV(ctx, di, GvCeMneg, lpl.AvgMax.CaSpkD.Cycle.Max)
	}
}

func (ly *LayerParams) CyclePostVTALayer(ctx *Context, di uint32) {
	ly.VTA.VTADA(ctx, di, GlbV(ctx, di, GvACh), (GlbV(ctx, di, GvHasRew) > 0))
}

// note: needs to iterate over sub-pools in layer!
func (ly *LayerParams) CyclePostVSPatchLayer(ctx *Context, di uint32, pi int32, pl *Pool, vals *LayerValues) {
	val := pl.AvgMax.CaSpkD.Cycle.Avg
	if ly.Learn.NeuroMod.DAMod == D1Mod {
		SetGlbUSposV(ctx, di, GvVSPatchD1, uint32(pi-1), val)
	} else {
		SetGlbUSposV(ctx, di, GvVSPatchD2, uint32(pi-1), val)
	}
}

/////////////////////////////////////////////////////////////////////////
//  Phase timescale

// NewStateLayerActAvg updates ActAvg.ActMAvg and ActPAvg based on current values
// that have been averaged across NData already.
func (ly *LayerParams) NewStateLayerActAvg(ctx *Context, vals *LayerValues, actMinusAvg, actPlusAvg float32) {
	ly.Inhib.ActAvg.AvgFromAct(&vals.ActAvg.ActMAvg, actMinusAvg, ly.Acts.Dt.LongAvgDt)
	ly.Inhib.ActAvg.AvgFromAct(&vals.ActAvg.ActPAvg, actPlusAvg, ly.Acts.Dt.LongAvgDt)
}

func (ly *LayerParams) NewStateLayer(ctx *Context, lpl *Pool, vals *LayerValues) {
	ly.Acts.Clamp.IsInput.SetBool(ly.IsInput())
	ly.Acts.Clamp.IsTarget.SetBool(ly.IsTarget())
	vals.RT = -1
}

func (ly *LayerParams) NewStatePool(ctx *Context, pl *Pool) {
	pl.Inhib.Clamped.SetBool(false)
	if ly.Acts.Clamp.Add.IsFalse() && ly.Acts.Clamp.IsInput.IsTrue() {
		pl.Inhib.Clamped.SetBool(true)
	}
	pl.Inhib.Decay(ly.Acts.Decay.Act)
	pl.Gated.SetBool(false)
}

// NewStateNeuron handles all initialization at start of new input pattern.
// Should already have presented the external input to the network at this point.
func (ly *LayerParams) NewStateNeuron(ctx *Context, ni, di uint32, vals *LayerValues, pl *Pool) {
	Neurons.Set(Neurons[Burst, ni, di], int(BurstPrv), int(ni), int(di))
	Neurons.Set(Neurons[CaSpkD, ni, di], int(SpkPrv), int(ni), int(di))
	Neurons.Set(0, int(SpkMax), int(ni), int(di))
	Neurons.Set(0, int(SpkMaxCa), int(ni), int(di))

	Neurons.Set(0, int(SpkBin0), int(ni), int(di))
	Neurons.Set(0, int(SpkBin1), int(ni), int(di))
	Neurons.Set(0, int(SpkBin2), int(ni), int(di))
	Neurons.Set(0, int(SpkBin3), int(ni), int(di))
	Neurons.Set(0, int(SpkBin4), int(ni), int(di))
	Neurons.Set(0, int(SpkBin5), int(ni), int(di))
	Neurons.Set(0, int(SpkBin6), int(ni), int(di))
	Neurons.Set(0, int(SpkBin7), int(ni), int(di))

	ly.Acts.DecayState(ctx, ni, di, ly.Acts.Decay.Act, ly.Acts.Decay.Glong, ly.Acts.Decay.AHP)
	// Note: synapse-level Ca decay happens in DWt
	ly.Acts.KNaNewState(ctx, ni, di)
}

func (ly *LayerParams) MinusPhasePool(ctx *Context, pl *Pool) {
	pl.AvgMax.CycleToMinus()
	if ly.Acts.Clamp.Add.IsFalse() && ly.Acts.Clamp.IsTarget.IsTrue() {
		pl.Inhib.Clamped.SetBool(true)
	}
}

// AvgGeM computes the average and max GeInt, GiInt in minus phase
// (AvgMaxGeM, AvgMaxGiM) stats, updated in MinusPhase,
// using values that already max across NData.
func (ly *LayerParams) AvgGeM(ctx *Context, vals *LayerValues, geIntMinusMax, giIntMinusMax float32) {
	vals.ActAvg.AvgMaxGeM += ly.Acts.Dt.LongAvgDt * (geIntMinusMax - vals.ActAvg.AvgMaxGeM)
	vals.ActAvg.AvgMaxGiM += ly.Acts.Dt.LongAvgDt * (giIntMinusMax - vals.ActAvg.AvgMaxGiM)
}

// MinusPhaseNeuron does neuron level minus-phase updating
func (ly *LayerParams) MinusPhaseNeuron(ctx *Context, ni, di uint32, pl *Pool, lpl *Pool, vals *LayerValues) {
	Neurons.Set(Neurons[ActInt, ni, di], int(ActM), int(ni), int(di))
	Neurons.Set(Neurons[CaSpkP, ni, di], int(CaSpkPM), int(ni), int(di))
}

// PlusPhaseStartNeuron does neuron level plus-phase start:
// applies Target inputs as External inputs.
func (ly *LayerParams) PlusPhaseStartNeuron(ctx *Context, ni, di uint32, pl *Pool, lpl *Pool, vals *LayerValues) {
	if NrnHasFlag(ctx, ni, di, NeuronHasTarg) { // will be clamped in plus phase
		Neurons.Set(Neurons[Target, ni, di], int(Ext), int(ni), int(di))
		NrnSetFlag(ctx, ni, di, NeuronHasExt)
		// get fresh update on plus phase output acts
		Neurons.Set(-1, int(ISI), int(ni), int(di))
		Neurons.Set(-1, int(ISIAvg), int(ni), int(di))
		// reset for plus phase
		Neurons.Set(ly.Acts.Init.Act, int(ActInt), int(ni), int(di))
	}
}

func (ly *LayerParams) PlusPhasePool(ctx *Context, pl *Pool) {
	pl.AvgMax.CycleToPlus()
}

// PlusPhaseNeuronSpecial does special layer type neuron level plus-phase updating
func (ly *LayerParams) PlusPhaseNeuronSpecial(ctx *Context, ni, di uint32, pl *Pool, lpl *Pool, vals *LayerValues) {
}

// PlusPhaseNeuron does neuron level plus-phase updating
func (ly *LayerParams) PlusPhaseNeuron(ctx *Context, ni, di uint32, pl *Pool, lpl *Pool, vals *LayerValues) {
	Neurons.Set(Neurons[ActInt, ni, di], int(ActP), int(ni), int(di))
	nrnCaSpkP := Neurons.Value(int(CaSpkP), int(ni), int(di))
	nrnCaSpkD := Neurons.Value(int(CaSpkD), int(ni), int(di))
	da := GlbV(ctx, di, GvDA)
	ach := GlbV(ctx, di, GvACh)
	mlr := ly.Learn.RLRate.RLRateSigDeriv(nrnCaSpkD, lpl.AvgMax.CaSpkD.Cycle.Max)
	modlr := ly.Learn.NeuroMod.LRMod(da, ach)
	dlr := float32(1)
	hasRew := (GlbV(ctx, di, GvHasRew) > 0)

	switch ly.LayType {
	case BLALayer:
		dlr = ly.Learn.RLRate.RLRateDiff(nrnCaSpkP, Neurons.Value(int(SpkPrv), int(ni), int(di))) // delta on previous trial
		if !ly.Learn.NeuroMod.IsBLAExt() && pl.StIndex == 0 {                                     // first pool
			dlr = 0 // first pool is novelty / curiosity -- no learn
		}
	case VSPatchLayer:
		da = GlbV(ctx, di, GvVSPatchPosRPE) // our own personal
		modlr = ly.Learn.NeuroMod.LRMod(da, ach)
		mlr = ly.Learn.RLRate.RLRateSigDeriv(Neurons.Value(int(SpkPrv), int(ni), int(di)), 1) // note: don't have proper max here
	case MatrixLayer:
		if hasRew { // reward time
			mlr = 1 // don't use dig deriv
		} else {
			modlr = 1 // don't use mod
		}
	default:
		dlr = ly.Learn.RLRate.RLRateDiff(nrnCaSpkP, nrnCaSpkD)
	}
	Neurons.Set(mlr*dlr*modlr, int(RLRate), int(ni), int(di))
	var tau float32
	sahpN := Neurons.Value(int(SahpN), int(ni), int(di))
	nrnSaphCa := Neurons.Value(int(SahpCa), int(ni), int(di))
	ly.Acts.Sahp.NinfTauFromCa(nrnSaphCa, &sahpN, &tau)
	nrnSaphCa = ly.Acts.Sahp.CaInt(nrnSaphCa, nrnCaSpkD)
	Neurons.Set(sahpN, int(SahpN), int(ni), int(di))
	Neurons.Set(nrnSaphCa, int(SahpCa), int(ni), int(di))
	Neurons.Set(ly.Acts.Sahp.GsAHP(sahpN), int(Gsahp), int(ni), int(di))
}

//gosl:end
