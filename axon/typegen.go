// Code generated by "core generate -add-types -gosl"; DO NOT EDIT.

package axon

import (
	"cogentcore.org/core/types"
	"cogentcore.org/lab/gosl/slbool"
	"cogentcore.org/lab/gosl/slrand"
)

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathGTypes", IDName: "path-g-types", Doc: "PathGTypes represents the conductance (G) effects of a given pathway,\nincluding excitatory, inhibitory, and modulatory."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynComParams", IDName: "syn-com-params", Doc: "SynComParams are synaptic communication parameters:\nused in the Path parameters.  Includes delay and\nprobability of failure, and Inhib for inhibitory connections,\nand modulatory pathways that have multiplicative-like effects.", Fields: []types.Field{{Name: "GType", Doc: "type of conductance (G) communicated by this pathway"}, {Name: "Delay", Doc: "additional synaptic delay in msec for inputs arriving at this pathway.\nMust be <= MaxDelay which is set during network building based on MaxDelay\nof any existing Path in the network. Delay = 0 means a spike reaches\nreceivers in the next Cycle, which is the minimum time (1 msec).\nBiologically, subtract 1 from biological synaptic delay values to set\ncorresponding Delay value."}, {Name: "MaxDelay", Doc: "maximum value of Delay, based on MaxDelay values when the BuildGBuf\nfunction was called during [Network.Build]. Cannot set it longer than this,\nexcept by calling BuildGBuf on network after changing MaxDelay to a larger\nvalue in any pathway in the network."}, {Name: "DelLen", Doc: "delay length = actual length of the GBuf buffer per neuron = Delay+1; just for speed"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathScaleParams", IDName: "path-scale-params", Doc: "PathScaleParams are pathway scaling parameters: modulates overall strength of pathway,\nusing both absolute and relative factors.", Fields: []types.Field{{Name: "Rel", Doc: "relative scaling that shifts balance between different pathways -- this is subject to normalization across all other pathways into receiving neuron, and determines the GScale.Target for adapting scaling"}, {Name: "Abs", Doc: "absolute multiplier adjustment factor for the path scaling -- can be used to adjust for idiosyncrasies not accommodated by the standard scaling based on initial target activation level and relative scaling factors -- any adaptation operates by directly adjusting scaling factor from the initially computed value"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeParams", IDName: "spike-params", Doc: "SpikeParams contains spiking activation function params.\nImplements a basic thresholded Vm model, and optionally\nthe AdEx adaptive exponential function.", Fields: []types.Field{{Name: "Thr", Doc: "Thr is the spiking threshold value Theta (Î˜) for firing output activation,\nin mV (millivolts). See also ExpThr for the AdEx implementation,\nin which case this threshold is the V_t parameters for the exponential function."}, {Name: "VmR", Doc: "VmR is the post-spiking membrane potential to reset to, in mV.\nThis produces refractory effect if lower than VmInit.\n-70 is appropriate biologically based value for AdEx (Brette & Gurstner, 2005)\nparameters. See also RTau."}, {Name: "Tr", Doc: "Tr is the post-spiking explicit refractory period, in cycles.\nPrevents Vm updating for this number of cycles post firing.\nVm is reduced in exponential steps over this period according to RTau,\nbeing fixed at Tr to VmR exactly."}, {Name: "RTau", Doc: "RTau is the time constant for decaying Vm down to VmR. At end of Tr it is set\nto VmR exactly. This provides a more realistic shape of the post-spiking\nVm which is only relevant for more realistic channels that key off of Vm.\nDoes not otherwise affect standard computation."}, {Name: "Exp", Doc: "Exp turns on the AdEx exponential excitatory current that drives Vm rapidly\nupward for spiking as it gets past its nominal firing threshold (Thr).\nEfficiently captures the Hodgkin Huxley dynamics of Na and K channels\n(Brette & Gurstner 2005)."}, {Name: "ExpSlope", Doc: "ExpSlope is the slope in mV for extra exponential excitatory current in AdEx."}, {Name: "ExpThr", Doc: "ExpThr is the membrane potential threshold (mV) for actually triggering\na spike when using the exponential mechanism. Due to 1 ms time integration,\nthis doesn't have much impact as long as it is above nominal spike threshold,\nand inside the VmRange for clipping Vm."}, {Name: "MaxHz", Doc: "MaxHz is for translating spiking interval (rate) into rate-code activation\nequivalent, as the maximum firing rate associated with a maximum\nactivation value of 1."}, {Name: "ISITau", Doc: "ISITau is the time constant for integrating the spiking interval in\nestimating spiking rate."}, {Name: "ISIDt", Doc: "ISIDt = 1 / tau"}, {Name: "RDt", Doc: "RDt = 1 / tau"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DendParams", IDName: "dend-params", Doc: "DendParams are the parameters for updating dendrite-specific dynamics", Fields: []types.Field{{Name: "GExp", Doc: "GExp is the dendrite-specific strength multiplier of the exponential\nspiking drive on Vm. E.g., .5 makes it half as strong as at the soma."}, {Name: "GR", Doc: "GR is the dendrite-specific additional conductance of Kdr delayed\nrectifier currents, used to reset membrane potential for dendrite.\nApplied for Tr cycles (ms)."}, {Name: "SSGi", Doc: "SSGi is the SST+ somatostatin positive slow spiking inhibition level\nspecifically affecting dendritic Vm (VmDend). This is important for countering\na positive feedback loop from NMDA getting stronger over the course\nof learning. Also typically requires SubMean = 1 for TrgAvgAct and\nlearning to fully counter this feedback loop."}, {Name: "HasMod", Doc: "HasMod is set automatically based on whether this layer has any recv pathways\nthat have a GType conductance type of Modulatory.\nIf so, then multiply GeSyn etc by GModSyn."}, {Name: "ModGain", Doc: "ModGain is a multiplicative gain factor on the total modulatory input.\nThis can also be controlled by the PathScale.Abs factor on\nModulatoryG inputs, but it is convenient to be able to control\non the layer as well."}, {Name: "ModACh", Doc: "ModACh if true, modulatory signal also includes ACh multiplicative factor."}, {Name: "ModBase", Doc: "ModBase is the baseline modulatory level for modulatory effects.\nNet modulation is ModBase + ModGain * GModSyn"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActInitParams", IDName: "act-init-params", Doc: "ActInitParams are initial values for key network state variables.\nInitialized in InitActs called by InitWeights, and provides target values\nfor DecayState.", Fields: []types.Field{{Name: "Vm", Doc: "Vm initial membrane potential in mV (millivolts).\nSee Erev.L for the resting potential, typically -70."}, {Name: "Act", Doc: "Act is the initial activation value. Typically 0."}, {Name: "GeBase", Doc: "GeBase is the baseline level of excitatory conductance (net input).\nGe is initialized to this value, and it is added in as a constant\nbackground level of excitatory input, to capture all the other\ninputs not represented in the model, and intrinsic excitability, etc."}, {Name: "GiBase", Doc: "GiBase baseline level of inhibitory conductance (net input)\nGi is initialized to this value, and it is added in as a constant\nbackground level of inhibitory input. Captures all the other inputs\nnot represented in the model."}, {Name: "GeVar", Doc: "GeVar is the variance (sigma) of gaussian distribution around baseline\nGe values, per neuron, to establish variability in intrinsic excitability.\nValue never goes < 0."}, {Name: "GiVar", Doc: "GiVar is the variance (sigma) of gaussian distribution around baseline\nGi values, per neuron, to establish variability in intrinsic excitability.\nValue never goes < 0"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DecayParams", IDName: "decay-params", Doc: "DecayParams control the decay of activation state in the DecayState function\ncalled in NewState when a new state is to be processed.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Act", Doc: "Act is proportion to decay most activation state variables toward initial\nvalues at start of every ThetaCycle (except those controlled separately below).\nIf 1 it is effectively equivalent to full clear, resetting other derived values.\nISI is reset every AlphaCycle to get a fresh sample of activations\n(doesn't affect direct computation -- only readout)."}, {Name: "Glong", Doc: "Glong is proportion to decay long-lasting conductances, NMDA and GABA,\nand also the dendritic membrane potential -- when using random stimulus\norder, it is important to decay this significantly to allow a fresh start,\nbut set Act to 0 to enable ongoing activity to keep neurons in their\nsensitive regime."}, {Name: "AHP", Doc: "AHP is decay of afterhyperpolarization currents, including mAHP, sAHP,\nand KNa, Kir. Has a separate decay because often useful to have this\nnot decay at all even if decay is on."}, {Name: "LearnCa", Doc: "LearnCa is decay of Ca variables driven by spiking activity used in learning:\nCaSpike* and Ca* variables. These are typically not decayed but may\nneed to be in some situations."}, {Name: "OnRew", Doc: "OnRew means decay layer at end of ThetaCycle when there is a global reward.\ntrue by default for PTPred, PTMaint and PFC Super layers."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DtParams", IDName: "dt-params", Doc: "DtParams are time and rate constants for temporal derivatives in Axon (Vm, G)", Fields: []types.Field{{Name: "Integ", Doc: "Integ is the overall rate constant for numerical integration, for all equations\nat the neuron level. All time constants are specified in ms millisecond units,\nwith one cycle = 1 ms. If you instead want to make one cycle = 2 ms, you can do\nthis globally by setting this integ value to 2 (etc).\nHowever, stability issues will likely arise if you go too high.\nFor improved numerical stability, you may even need to reduce this value\nto 0.5 or possibly even lower (typically however this is not necessary)."}, {Name: "VmC", Doc: "VmC is the membrane potential capacitance in pF (picofarads), which\ndetermines the rate of Vm updating over time."}, {Name: "VmDendC", Doc: "VmDendC is the effective dendritic membrane capacitance in pF (picofarads),\nwhich is typically slower than VmC (also reflecting other dendritic dynamics)."}, {Name: "VmSteps", Doc: "VmSteps are the number of integration steps to take in computing new Vm value.\nThis is the one computation that can be most numerically unstable\nso taking multiple steps with proportionally smaller dt is beneficial."}, {Name: "GeTau", Doc: "GeTau is the time constant for decay of excitatory AMPA receptor\nconductance in ms (milliseconds)."}, {Name: "GiTau", Doc: "GiTau is the time constant for decay of inhibitory GABA-A receptor\nconductance in ms (milliseconds)."}, {Name: "IntTau", Doc: "IntTau is a time constant for integrating values over timescale of an\nindividual input state (e.g., roughly the 200 msec theta cycle),\nused in computing ActInt, GeInt from Ge, and GiInt from GiSyn.\nThis is used for scoring performance, not for learning, in cycles,\nwhich should be milliseconds typically\n(Tau is roughly 2/3 of the way to asymptote)."}, {Name: "LongAvgTau", Doc: "LongAvgTau is a time constant for integrating slower long-time-scale averages,\nsuch as ActAvg, Pool.ActsMAvg, ActsPAvg. Computed in NewState\nwhen a new input state is present (i.e., not msec but in units\nof a theta cycle) (Tau is roughly 2/3 of the way to asymptote).\nSet lower for smaller models."}, {Name: "MaxCycStart", Doc: "maxCycStart is the cycle to start updating the CaPMaxCa, CaPMax values\nwithin a theta cycle. Early cycles often reflect prior state."}, {Name: "VmDt", Doc: "VmDT = Integ / VmC"}, {Name: "VmDendDt", Doc: "VmDendDt = Integ / VmDendC"}, {Name: "DtStep", Doc: "DtStep = 1 / VmSteps"}, {Name: "GeDt", Doc: "GeDt = Integ / GeTau"}, {Name: "GiDt", Doc: "GiDt = Integ / GiTau"}, {Name: "IntDt", Doc: "IntDt = Integ / IntTau"}, {Name: "LongAvgDt", Doc: "LongAvgDt = 1 / LongAvgTau"}, {Name: "MaxI", Doc: "MaxI = VmC * 100 nS nominal max conductance = maximum I current step."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeNoiseParams", IDName: "spike-noise-params", Doc: "SpikeNoiseParams parameterizes background spiking activity impinging on the neuron,\nsimulated using a poisson spiking process.", Fields: []types.Field{{Name: "On", Doc: "On switch to add noise simulating background spiking levels."}, {Name: "GeHz", Doc: "GeHz is the mean frequency of excitatory spikes. Typically 50Hz but multiple\ninputs increase rate. This is a poisson lambda parameter, also the variance."}, {Name: "Ge", Doc: "Ge is the excitatory conductance per spike. 0.001 has minimal impact,\n0.01 can be strong, and .15 is needed to influence timing of clamped inputs."}, {Name: "GiHz", Doc: "GiHz is the mean frequency of inhibitory spikes. Typically 100Hz fast spiking\nbut multiple inputs increase rate. This is a poisson lambda parameter,\nalso the variance."}, {Name: "Gi", Doc: "Gi is the excitatory conductance per spike. 0.001 has minimal impact,\n0.01 can be strong, and .15 is needed to influence timing of clamped inputs."}, {Name: "MaintGe", Doc: "MaintGe adds Ge noise to GeMaintRaw instead of standard Ge.\nused for PTMaintLayer for example."}, {Name: "GeExpInt", Doc: "GeExpInt = Exp(-Interval) which is the threshold for GeNoiseP as it is updated."}, {Name: "GiExpInt", Doc: "GiExpInt = Exp(-Interval) which is the threshold for GiNoiseP as it is updated."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ClampParams", IDName: "clamp-params", Doc: "ClampParams specify how external inputs drive excitatory conductances\n(like a current clamp) -- either adds or overwrites existing conductances.\nNoise is added in either case.", Fields: []types.Field{{Name: "Ge", Doc: "Ge is the contribution to Ge(t) for clamped external input.\nGenerally use .8 for Target layers, 1.50 for Input layers.\nThis is later multiplied by overall gbar_e which converts to nS units."}, {Name: "Add", Doc: "Add external conductance on top of any existing.\nGenerally this is not a good idea for target layers\n(creates a main effect that learning can never match),\nbut may be ok for input layers."}, {Name: "ErrThr", Doc: "ErrThr is the threshold on neuron Act activity to count as active for\ncomputing the error relative to target in PctErr method."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SMaintParams", IDName: "s-maint-params", Doc: "SMaintParams for self-maintenance simulating a population of\nNMDA-interconnected spiking neurons", Fields: []types.Field{{Name: "On", Doc: "On switch for self maintenance."}, {Name: "NNeurons", Doc: "NNeurons is the number of neurons within the self-maintenance pool,\neach of which is assumed to have the same probability of spiking."}, {Name: "Ge", Doc: "Ge is the excitatory conductance multiplier for self maintenance synapses."}, {Name: "Inhib", Doc: "Inhib controls how much of the extra maintenance conductance goes\nto the GeExt, which drives extra proportional inhibition."}, {Name: "ISI", Doc: "ISI (inter spike interval) range. Min is used as min ISIAvg\nfor poisson spike rate expected from the population,\nand above Max, no additional maintenance conductance is added."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PopCodeParams", IDName: "pop-code-params", Doc: "PopCodeParams provides an encoding of scalar value using population code,\nwhere a single continuous (scalar) value is encoded as a gaussian bump\nacross a population of neurons (1 dimensional).\nIt can also modulate rate code and number of neurons active according to the value.\nThis is for layers that represent values as in the Rubicon system.\nBoth normalized activation values (1 max) and Ge conductance values can be generated.", Fields: []types.Field{{Name: "On", Doc: "On toggles use of popcode encoding of variable(s) that this layer represents."}, {Name: "Ge", Doc: "Ge multiplier for driving excitatory conductance based on PopCode.\nMultiplies normalized activation values and adds to total Ge(t)\nwhich is later multiplied by Gbar.E for pA unit scaling."}, {Name: "Min", Doc: "Min is the minimum value representable. For GaussBump, typically include\nextra to allow mean with activity on either side to represent\nthe lowest value you want to encode."}, {Name: "Max", Doc: "Max is the maximum value representable. For GaussBump, typically include\nextra to allow mean with activity on either side to represent\nthe lowest value you want to encode."}, {Name: "MinAct", Doc: "MinAct is an activation multiplier for values at Min end of range,\nwhere values at Max end have an activation of 1.\nIf this is < 1, then there is a rate code proportional\nto the value in addition to the popcode pattern. See also MinSigma, MaxSigma."}, {Name: "MinSigma", Doc: "MinSigma is the sigma parameter of a gaussian specifying the tuning width\nof the coarse-coded units, in normalized 0-1 range, for values at the Min\nend of the range. If MinSigma < MaxSigma then more units are activated\nfor Max values vs. Min values, proportionally."}, {Name: "MaxSigma", Doc: "MaxSigma is the sigma parameter of a gaussian specifying the tuning width\nof the coarse-coded units, in normalized 0-1 range, for values at the Max\nend of the range. If MinSigma < MaxSigma then more units are activated\nfor Max values vs. Min values, proportionally."}, {Name: "Clip", Doc: "Clip ensures that encoded and decoded value remains within specified range."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActParams", IDName: "act-params", Doc: "ActParams contains all the neural activity computation params and functions\nfor Axon, at the neuron level. This is included in [LayerParams].", Fields: []types.Field{{Name: "Spikes", Doc: "Spikes are spiking function parameter, including the AdEx spiking function."}, {Name: "Dend", Doc: "Dend are dendrite-specific parameters, which more accurately approximate\nthe electrical dynamics present in dendrites vs the soma."}, {Name: "Init", Doc: "Init has initial values for key network state variables.\nInitialized in InitActs called by InitWeights, and provides target\nvalues for DecayState."}, {Name: "Decay", Doc: "Decay is the amount to decay between theta cycles, simulating the passage\nof time and effects of saccades etc. It is especially important for\nenvironments with random temporal structure (e.g., most standard neural net\ntraining corpora)."}, {Name: "Dt", Doc: "Dt has time and rate constants for temporal derivatives / updating of\nactivation state."}, {Name: "Gbar", Doc: "Gbar has maximal conductances levels for channels, in nS (nanosiemens).\nMost other conductances are computed as time-varying proportions of these\nvalues (strict 1 max is not enforced and can be exceeded)."}, {Name: "Erev", Doc: "Erev are reversal / driving potentials for each channel, in mV (millivolts).\nCurrent is a function of the difference between these driving potentials\nand the membrane potential Vm, and goes to 0 (and reverses sign) as it\ncrosses equality."}, {Name: "Clamp", Doc: "Clamp determines how external inputs drive excitatory conductance."}, {Name: "Noise", Doc: "Noise specifies how, where, when, and how much noise to add."}, {Name: "VmRange", Doc: "VmRange constrains the range of the Vm membrane potential,\nwhich helps to prevent numerical instability."}, {Name: "Mahp", Doc: "Mahp is the M-type medium time-scale afterhyperpolarization (mAHP) current.\nThis is the primary form of adaptation on the time scale of\nmultiple sequences of spikes."}, {Name: "Sahp", Doc: "Sahp is the slow time-scale afterhyperpolarization (sAHP) current.\nIt integrates CaD at theta cycle intervals and produces a hard cutoff\non sustained activity for any neuron."}, {Name: "KNa", Doc: "KNa has the sodium-gated potassium channel adaptation parameters.\nIt activates a leak-like current as a function of neural activity\n(firing = Na influx) at two different time-scales (Slick = medium, Slack = slow)."}, {Name: "Kir", Doc: "Kir is the potassium (K) inwardly rectifying (ir) current, which\nis similar to GABA-B (which is a GABA modulated Kir channel).\nThis channel is off by default but plays a critical role in making medium\nspiny neurons (MSNs) relatively quiet in the striatum."}, {Name: "NMDA", Doc: "NMDA has channel parameters used in computing the Gnmda conductance\nthat is maximal for more depolarized neurons (due to unblocking of\nMg++ ions), and thus helps keep active neurons active, thereby promoting\noverall neural stability over time. See also Learn.LearnNMDA for\ndistinct parameters used for Ca++ influx driving learning, and\nMaintNMDA for specialized NMDA driven by maintenance pathways."}, {Name: "MaintNMDA", Doc: "MaintNMDA has channel parameters used in computing the Gnmda conductance\nbased on pathways of the MaintG conductance type, e.g., in the PT PFC neurons.\nThis is typically stronger and longer lasting than standard NMDA."}, {Name: "GabaB", Doc: "GabaB has GABA-B channel parameters for long-lasting inhibition\nthat is inwardly rectified (GIRK coupled) and maximal for more hyperpolarized\nneurons, thus keeping inactive neurons inactive. This is synergistic with\nNMDA for supporting stable activity patterns over the theta cycle."}, {Name: "VGCC", Doc: "VGCC are voltage gated calcium channels, which provide a key additional\nsource of Ca for learning and positive-feedback loop upstate for active\nneurons when they are spiking."}, {Name: "AK", Doc: "AK is the A-type potassium (K) channel that is particularly important\nfor limiting the runaway excitation from VGCC channels."}, {Name: "SKCa", Doc: "SKCa is the small-conductance calcium-activated potassium channel produces\nthe pausing function as a consequence of rapid bursting. These are not active\nby default but are critical for subthalamic nucleus (STN) neurons."}, {Name: "SMaint", Doc: "SMaint provides a simplified self-maintenance current for a population of\nNMDA-interconnected spiking neurons."}, {Name: "PopCode", Doc: "PopCode provides encoding population codes, used to represent a single\ncontinuous (scalar) value, across a population of units / neurons\n(1 dimensional)."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLANovelPath", IDName: "bla-novel-path", Doc: "BLANovelPath connects all other pools to the first, Novelty, pool in a BLA layer.\nThis allows the known US representations to specifically inhibit the novelty pool."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NuclearParams", IDName: "nuclear-params", Doc: "NuclearParams has parameters that apply to all cerebellum Nuclear model neurons.\nNot just cerebellar nuclei neurons: also applies to PC.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "ActTarget", Doc: "ActTarget is the target activity level, as measured by CaD.\nGeBase is adapted, along with excitatory MF inputs in proportion to activity,\nwhich is the source of very slow synaptic decay in these pathways."}, {Name: "IOLayIndex", Doc: "IOLayIndex of IO (inferior olive) layer for sending error signals\nto this layer. Set via SetBuildConfig(IOLayName) setting."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.IOParams", IDName: "io-params", Doc: "IOParams has parameters for the IO inferior olive neurons,\nwhich compute a temporal offset error signal between CNiIO inhibitory\npredictions and excitatory sensory input, contingent on initial\nabove-threshold efferent copy motor trigger input (modulatory).\nNeuron [CaBins] are used to store TimeOff past inhibitory inputs.", Fields: []types.Field{{Name: "TimeOff", Doc: "TimeOff is the time offset for earlier predictive inhibitory inputs to\ncompare against current excitatory inputs to trigger an error,\nin ms (cycles). Determines how far back we store inhib values in [CaBins].\nMust be an even multiple of InhibBin."}, {Name: "ActionEnv", Doc: "ActionEnv is the total time envelope for actions to be tracked,\nin ms (cycles)."}, {Name: "ErrThr", Doc: "ErrThr is the threshold on the GeSyn - GiSyn_(t-TimeOff) difference\nto trigger an error."}, {Name: "EfferentThr", Doc: "EfferentThr is the threshold for modulatory [GModSyn] from efferent copy\ninputs to trigger an activated IO window where error comparison occurs."}, {Name: "InhibBin", Doc: "InhibBin is the bin size in ms (cycles) for storing TimeOff inhib values.\nTimeOff / InhibBin cannot exceed NetworkIxs.NCaBins."}, {Name: "TimeBins", Doc: "TimeBins = TimeOff / InhibBin"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CNeUpParams", IDName: "c-ne-up-params", Doc: "CNeUpParams has parameters for learning in the cerebellar nucleus\nexcitatory upgoing output neurons, which are tonically active and\nlearn to maintain a target activity level in the presence and absence\nof inputs.", Fields: []types.Field{{Name: "ActTarg", Doc: "ActTarg is the target activity level, as measured by CaD.\nGeBase is adapted in the absence of synaptic input, and\ninhibitory input from CNiIO neurons is adapted when\nboth excitatory and inhibitory input is present above threshold."}, {Name: "LearnThr", Doc: "Learning threshold for CNiIO and Sense excitatory input neurons\nto enable synaptic learning in the CNiIO inputs, to learn towards\nthe ActTarg activity level."}, {Name: "GeBaseLRate", Doc: "Learning rate for GeBase baseline excitation level, when no synaptic\ninput is above threshold."}, {Name: "PredLayIndex", Doc: "PredLayIndex of CNiIOLayer for this output layer.\nSet via SetBuildConfig(PredLayName) setting."}, {Name: "SenseLayIndex", Doc: "SenseLayIndex of excitatory sensory input that drives our activity.\nSet via SetBuildConfig(SenseLayName) setting."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Context", IDName: "context", Doc: "Context contains all of the global context state info\nthat is shared across every step of the computation.\nIt is passed around to all relevant computational functions,\nand is updated on the CPU and synced to the GPU after every cycle.\nIt contains timing, Testing vs. Training mode, random number context, etc.\nThere is one canonical instance on the network as Ctx, always get it from\nthe network.Context() method.", Directives: []types.Directive{{Tool: "types", Directive: "add", Args: []string{"-setters"}}}, Fields: []types.Field{{Name: "NData", Doc: "number of data parallel items to process currently."}, {Name: "Mode", Doc: "current running mode, using sim-defined enum, e.g., Train, Test, etc."}, {Name: "Testing", Doc: "Testing is true if the model is being run in a testing mode,\nso no weight changes or other associated computations should be done.\nThis flag should only affect learning-related behavior."}, {Name: "MinusPhase", Doc: "MinusPhase is true if this is the minus phase, when a stimulus is present\nand learning is occuring. Could also be in a non-learning phase when\nno stimulus is present. This affects accumulation of CaBins values only."}, {Name: "PlusPhase", Doc: "PlusPhase is true if this is the plus phase, when the outcome / bursting\nis occurring, driving positive learning; else minus or non-learning phase."}, {Name: "PhaseCycle", Doc: "Cycle within current phase, minus or plus."}, {Name: "Cycle", Doc: "Cycle within Trial: number of iterations of activation updating (settling)\non the current state. This is reset at NewState."}, {Name: "ThetaCycles", Doc: "ThetaCycles is the length of the theta cycle (i.e., Trial),\nin terms of 1 msec Cycles. Some network update steps depend on doing something\nat the end of the theta cycle (e.g., CTCtxtPath).\nShould be ISICycles + MinusCycles + PlusCycles"}, {Name: "ISICycles", Doc: "ISICycles is the number of inter-stimulus-interval cycles,\nwhich happen prior to the minus phase (i.e., after the last plus phase)."}, {Name: "MinusCycles", Doc: "MinusCycles is the number of cycles in the minus phase. Typically 150,\nbut may be set longer if ThetaCycles is above default of 200."}, {Name: "PlusCycles", Doc: "PlusCycles is the number of cycles in the plus phase. Typically 50,\nbut may be set longer if ThetaCycles is above default of 200."}, {Name: "CyclesTotal", Doc: "CyclesTotal is the accumulated cycle count, which increments continuously\nfrom whenever it was last reset. Typically this is the number of milliseconds\nin simulation time."}, {Name: "Time", Doc: "Time is the accumulated amount of time the network has been running,\nin simulation-time (not real world time), in seconds."}, {Name: "TrialsTotal", Doc: "TrialsTotal is the total trial count, which increments continuously in NewState\n_only in Train mode_ from whenever it was last reset. Can be used for synchronizing\nweight updates across nodes."}, {Name: "TimePerCycle", Doc: "TimePerCycle is the amount of Time to increment per cycle."}, {Name: "SlowInterval", Doc: "SlowInterval is how frequently in Trials to perform slow adaptive processes\nsuch as synaptic scaling, associated in the brain with sleep,\nvia the SlowAdapt method.  This should be long enough for meaningful changes\nto accumulate. 100 is default but could easily be longer in larger models.\nBecause SlowCounter is incremented by NData, high NData cases (e.g. 16) likely need to\nincrease this value, e.g., 400 seems to produce overall consistent results in various models."}, {Name: "SlowCounter", Doc: "SlowCounter increments for each training trial, to trigger SlowAdapt at SlowInterval.\nThis is incremented by NData to maintain consistency across different values of this parameter."}, {Name: "AdaptGiInterval", Doc: "AdaptGiInterval is how frequently in Trials to perform inhibition adaptation,\nwhich needs to be even slower than the SlowInterval."}, {Name: "AdaptGiCounter", Doc: "AdaptGiCounter increments for each training trial, to trigger AdaptGi at AdaptGiInterval.\nThis is incremented by NData to maintain consistency across different values of this parameter."}, {Name: "pad"}, {Name: "RandCounter", Doc: "RandCounter is the random counter, incremented by maximum number of\npossible random numbers generated per cycle, regardless of how\nmany are actually used. This is shared across all layers so must\nencompass all possible param settings."}}})

// SetNData sets the [Context.NData]:
// number of data parallel items to process currently.
func (t *Context) SetNData(v uint32) *Context { t.NData = v; return t }

// SetMode sets the [Context.Mode]:
// current running mode, using sim-defined enum, e.g., Train, Test, etc.
func (t *Context) SetMode(v int32) *Context { t.Mode = v; return t }

// SetTesting sets the [Context.Testing]:
// Testing is true if the model is being run in a testing mode,
// so no weight changes or other associated computations should be done.
// This flag should only affect learning-related behavior.
func (t *Context) SetTesting(v slbool.Bool) *Context { t.Testing = v; return t }

// SetMinusPhase sets the [Context.MinusPhase]:
// MinusPhase is true if this is the minus phase, when a stimulus is present
// and learning is occuring. Could also be in a non-learning phase when
// no stimulus is present. This affects accumulation of CaBins values only.
func (t *Context) SetMinusPhase(v slbool.Bool) *Context { t.MinusPhase = v; return t }

// SetPlusPhase sets the [Context.PlusPhase]:
// PlusPhase is true if this is the plus phase, when the outcome / bursting
// is occurring, driving positive learning; else minus or non-learning phase.
func (t *Context) SetPlusPhase(v slbool.Bool) *Context { t.PlusPhase = v; return t }

// SetPhaseCycle sets the [Context.PhaseCycle]:
// Cycle within current phase, minus or plus.
func (t *Context) SetPhaseCycle(v int32) *Context { t.PhaseCycle = v; return t }

// SetCycle sets the [Context.Cycle]:
// Cycle within Trial: number of iterations of activation updating (settling)
// on the current state. This is reset at NewState.
func (t *Context) SetCycle(v int32) *Context { t.Cycle = v; return t }

// SetThetaCycles sets the [Context.ThetaCycles]:
// ThetaCycles is the length of the theta cycle (i.e., Trial),
// in terms of 1 msec Cycles. Some network update steps depend on doing something
// at the end of the theta cycle (e.g., CTCtxtPath).
// Should be ISICycles + MinusCycles + PlusCycles
func (t *Context) SetThetaCycles(v int32) *Context { t.ThetaCycles = v; return t }

// SetISICycles sets the [Context.ISICycles]:
// ISICycles is the number of inter-stimulus-interval cycles,
// which happen prior to the minus phase (i.e., after the last plus phase).
func (t *Context) SetISICycles(v int32) *Context { t.ISICycles = v; return t }

// SetMinusCycles sets the [Context.MinusCycles]:
// MinusCycles is the number of cycles in the minus phase. Typically 150,
// but may be set longer if ThetaCycles is above default of 200.
func (t *Context) SetMinusCycles(v int32) *Context { t.MinusCycles = v; return t }

// SetPlusCycles sets the [Context.PlusCycles]:
// PlusCycles is the number of cycles in the plus phase. Typically 50,
// but may be set longer if ThetaCycles is above default of 200.
func (t *Context) SetPlusCycles(v int32) *Context { t.PlusCycles = v; return t }

// SetCyclesTotal sets the [Context.CyclesTotal]:
// CyclesTotal is the accumulated cycle count, which increments continuously
// from whenever it was last reset. Typically this is the number of milliseconds
// in simulation time.
func (t *Context) SetCyclesTotal(v int32) *Context { t.CyclesTotal = v; return t }

// SetTime sets the [Context.Time]:
// Time is the accumulated amount of time the network has been running,
// in simulation-time (not real world time), in seconds.
func (t *Context) SetTime(v float32) *Context { t.Time = v; return t }

// SetTrialsTotal sets the [Context.TrialsTotal]:
// TrialsTotal is the total trial count, which increments continuously in NewState
// _only in Train mode_ from whenever it was last reset. Can be used for synchronizing
// weight updates across nodes.
func (t *Context) SetTrialsTotal(v int32) *Context { t.TrialsTotal = v; return t }

// SetTimePerCycle sets the [Context.TimePerCycle]:
// TimePerCycle is the amount of Time to increment per cycle.
func (t *Context) SetTimePerCycle(v float32) *Context { t.TimePerCycle = v; return t }

// SetSlowInterval sets the [Context.SlowInterval]:
// SlowInterval is how frequently in Trials to perform slow adaptive processes
// such as synaptic scaling, associated in the brain with sleep,
// via the SlowAdapt method.  This should be long enough for meaningful changes
// to accumulate. 100 is default but could easily be longer in larger models.
// Because SlowCounter is incremented by NData, high NData cases (e.g. 16) likely need to
// increase this value, e.g., 400 seems to produce overall consistent results in various models.
func (t *Context) SetSlowInterval(v int32) *Context { t.SlowInterval = v; return t }

// SetSlowCounter sets the [Context.SlowCounter]:
// SlowCounter increments for each training trial, to trigger SlowAdapt at SlowInterval.
// This is incremented by NData to maintain consistency across different values of this parameter.
func (t *Context) SetSlowCounter(v int32) *Context { t.SlowCounter = v; return t }

// SetAdaptGiInterval sets the [Context.AdaptGiInterval]:
// AdaptGiInterval is how frequently in Trials to perform inhibition adaptation,
// which needs to be even slower than the SlowInterval.
func (t *Context) SetAdaptGiInterval(v int32) *Context { t.AdaptGiInterval = v; return t }

// SetAdaptGiCounter sets the [Context.AdaptGiCounter]:
// AdaptGiCounter increments for each training trial, to trigger AdaptGi at AdaptGiInterval.
// This is incremented by NData to maintain consistency across different values of this parameter.
func (t *Context) SetAdaptGiCounter(v int32) *Context { t.AdaptGiCounter = v; return t }

// SetRandCounter sets the [Context.RandCounter]:
// RandCounter is the random counter, incremented by maximum number of
// possible random numbers generated per cycle, regardless of how
// many are actually used. This is shared across all layers so must
// encompass all possible param settings.
func (t *Context) SetRandCounter(v slrand.Counter) *Context { t.RandCounter = v; return t }

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BurstParams", IDName: "burst-params", Doc: "BurstParams determine how the 5IB Burst activation is computed from\nCaP integrated spiking values in Super layers -- thresholded.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "ThrRel", Doc: "Relative component of threshold on superficial activation value,\nbelow which it does not drive Burst (and above which, Burst = CaP).\nThis is the distance between the average and maximum activation values\nwithin layer (e.g., 0 = average, 1 = max).  Overall effective threshold\nis MAX of relative and absolute thresholds."}, {Name: "ThrAbs", Doc: "Absolute component of threshold on superficial activation value,\nbelow which it does not drive Burst (and above which, Burst = CaP).\nOverall effective threshold is MAX of relative and absolute thresholds."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CTParams", IDName: "ct-params", Doc: "CTParams control the CT corticothalamic neuron special behavior", Fields: []types.Field{{Name: "GeGain", Doc: "GeGain is the gain factor for context excitatory input, which is\nconstant as compared to the spiking input from other pathways, so it\nmust be downscaled accordingly. This can make a difference\nand may need to be scaled up or down."}, {Name: "DecayTau", Doc: "DecayTau is the decay time constant for context Ge input.\nif > 0, decays over time so intrinsic circuit dynamics have to take over.\nFor single-step copy-based cases, set to 0, while longer-time-scale\ndynamics should use ~50 or more."}, {Name: "OFCposPT", Doc: "OFCposPT is set for the OFCposPT PTMaintLayer, which sets the\nGvOFCposPTMaint global variable."}, {Name: "DecayDt", Doc: "1 / tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PulvinarParams", IDName: "pulvinar-params", Doc: "PulvinarParams provides parameters for how the plus-phase (outcome)\nstate of Pulvinar thalamic relay cell neurons is computed from\nthe corresponding driver neuron Burst activation (or CaP if not Super)", Fields: []types.Field{{Name: "DriveScale", Doc: "DriveScale is the multiplier on driver input strength,\nwhich multiplies CaP from driver layer to produce Ge excitatory\ninput to CNiIO unit."}, {Name: "FullDriveAct", Doc: "FullDriveAct is the level of Max driver layer CaP at which the drivers\nfully drive the burst phase activation. If there is weaker driver input,\nthen (Max/FullDriveAct) proportion of the non-driver inputs remain and\nthis critically prevents the network from learning to turn activation\noff, which is difficult and severely degrades learning."}, {Name: "DriveLayIndex", Doc: "DriveLayIndex of layer that generates the driving activity into this one\nset via SetBuildConfig(DriveLayName) setting"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GlobalScalarVars", IDName: "global-scalar-vars", Doc: "GlobalScalarVars are network-wide scalar variables, such as neuromodulators,\nreward, etc including the state for the Rubicon phasic dopamine model.\nThese are stored in the Network.GlobalScalars tensor and corresponding global variable."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GlobalVectorVars", IDName: "global-vector-vars", Doc: "GlobalVectorVars are network-wide vector variables, such as drives,\ncosts, US outcomes, with [MaxGlobalVecN] values per variable.\nThese are stored in the Network.GlobalVectors tensor and\ncorresponding global variable."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPUVars", IDName: "gpu-vars", Doc: "GPUVars is an enum for GPU variables, for specifying what to sync."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipConfig", IDName: "hip-config", Doc: "HipConfig have the hippocampus size and connectivity parameters", Fields: []types.Field{{Name: "EC2Size", Doc: "size of EC2"}, {Name: "EC3NPool", Doc: "number of EC3 pools (outer dimension)"}, {Name: "EC3NNrn", Doc: "number of neurons in one EC3 pool"}, {Name: "CA1NNrn", Doc: "number of neurons in one CA1 pool"}, {Name: "CA3Size", Doc: "size of CA3"}, {Name: "DGRatio", Doc: "size of DG / CA3"}, {Name: "EC3ToEC2PCon", Doc: "percent connectivity from EC3 to EC2"}, {Name: "EC2ToDGPCon", Doc: "percent connectivity from EC2 to DG"}, {Name: "EC2ToCA3PCon", Doc: "percent connectivity from EC2 to CA3"}, {Name: "CA3ToCA1PCon", Doc: "percent connectivity from CA3 to CA1"}, {Name: "DGToCA3PCon", Doc: "percent connectivity into CA3 from DG"}, {Name: "EC2LatRadius", Doc: "lateral radius of connectivity in EC2"}, {Name: "EC2LatSigma", Doc: "lateral gaussian sigma in EC2 for how quickly weights fall off with distance"}, {Name: "MossyDelta", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in training, applied at the start of a trial to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "MossyDeltaTest", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in testing, applied during 2nd-3rd quarters to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "ThetaLow", Doc: "low theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "ThetaHigh", Doc: "high theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "EC5Clamp", Doc: "flag for clamping the EC5 from EC5ClampSrc"}, {Name: "EC5ClampSrc", Doc: "source layer for EC5 clamping activations in the plus phase -- biologically it is EC3 but can use an Input layer if available"}, {Name: "EC5ClampTest", Doc: "clamp the EC5 from EC5ClampSrc during testing as well as training -- this will overwrite any target values that might be used in stats (e.g., in the basic hip example), so it must be turned off there"}, {Name: "EC5ClampThr", Doc: "threshold for binarizing EC5 clamp values -- any value above this is clamped to 1, else 0 -- helps produce a cleaner learning signal.  Set to 0 to not perform any binarization."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipPathParams", IDName: "hip-path-params", Doc: "HipPathParams define behavior of hippocampus paths, which have special learning rules", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Hebb", Doc: "Hebbian learning proportion"}, {Name: "Err", Doc: "EDL proportion"}, {Name: "SAvgCor", Doc: "proportion of correction to apply to sending average activation for hebbian learning component (0=none, 1=all, .5=half, etc)"}, {Name: "SAvgThr", Doc: "threshold of sending average activation below which learning does not occur (prevents learning when there is no input)"}, {Name: "SNominal", Doc: "sending layer Nominal (need to manually set it to be the same as the sending layer)"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActAvgParams", IDName: "act-avg-params", Doc: "ActAvgParams represents the nominal average activity levels in the layer\nand parameters for adapting the computed Gi inhibition levels to maintain\naverage activity within a target range.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}, {Tool: "gosl", Directive: "import", Args: []string{"github.com/emer/axon/v2/fsfffb"}}}, Fields: []types.Field{{Name: "Nominal", Doc: "Nominal is the estimated average activity level in the layer, which is\nused in computing the scaling factor on sending pathways from this layer.\nIn general it should roughly match the layer ActAvg.ActMAvg value, which\ncan be logged using the axon.LogAddDiagnosticItems function.\nIf layers receiving from this layer are not getting enough Ge excitation,\nthen this Nominal level can be lowered to increase pathway strength\n(fewer active neurons means each one contributes more, so scaling factor\n\n\tgoes as the inverse of activity level), or vice-versa if Ge is too high.\n\nIt is also the basis for the target activity level used for the AdaptGi\n\n\toption: see the Offset which is added to this value."}, {Name: "RTThr", Doc: "RTThr is the reaction time (RT) threshold activity level in the layer,\nin terms of the maximum CaP level of any neuron in the layer. The\nLayerStates LayerRT value is recorded for the cycle at which this\nlevel is exceeded within a theta cycle, after Acts.Dt.MaxCycStart cycles."}, {Name: "AdaptGi", Doc: "AdaptGi enables adapting of layer inhibition Gi multiplier factor\n(stored in layer GiMult value) to maintain a target layer level of\nActAvg.Nominal. This generally works well and improves the long-term\nstability of the models. It is not enabled by default because it depends\non having established a reasonable Nominal + Offset target activity level."}, {Name: "Offset", Doc: "Offset is added to Nominal for the target average activity that drives\nadaptation of Gi for this layer.  Typically the Nominal level is good,\nbut sometimes Nominal must be adjusted up or down to achieve desired Ge\nscaling, so this Offset can compensate accordingly."}, {Name: "HiTol", Doc: "HiTol is the tolerance for higher than Target target average activation\nas a proportion of that target value (0 = exactly the target, 0.2 = 20%\nhigher than target). Only once activations move outside this tolerance\n\n\tare inhibitory values adapted."}, {Name: "LoTol", Doc: "LoTol is the tolerance for lower than Target target average activation\nas a proportion of that target value (0 = exactly the target, 0.5 = 50%\nlower than target). Only once activations move outside this tolerance are\n\n\tinhibitory values adapted."}, {Name: "AdaptRate", Doc: "AdaptRate is the rate of Gi adaptation as function of\nAdaptRate * (Target - ActMAvg) / Target. This occurs at spaced intervals\ndetermined by Network.SlowInterval value. Slower values such as 0.05 may\nbe needed for large networks and sparse layers."}, {Name: "AdaptMax", Doc: "AdaptMax is the maximum adaptation step magnitude to take at any point."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.InhibParams", IDName: "inhib-params", Doc: "InhibParams contains all the inhibition computation params and functions for basic Axon.\nThis is included in LayerParams to support computation.\nAlso includes the expected average activation in the layer, which is used for\nG conductance rescaling and potentially for adapting inhibition over time.", Fields: []types.Field{{Name: "ActAvg", Doc: "ActAvg has layer-level and pool-level average activation initial values\nand updating / adaptation thereof.\nInitial values help determine initial scaling factors."}, {Name: "Layer", Doc: "Layer determines inhibition across the entire layer.\nInput layers generally use Gi = 0.8 or 0.9, 1.3 or higher for sparse layers.\nIf the layer has sub-pools (4D shape) then this is effectively between-pool inhibition."}, {Name: "Pool", Doc: "Pool determines inhibition within sub-pools of units, for layers with 4D shape.\nThis is almost always necessary if the layer has sub-pools."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Layer", IDName: "layer", Doc: "Layer implements the basic Axon spiking activation function,\nand manages learning in the pathways.", Methods: []types.Method{{Name: "InitWeights", Doc: "InitWeights initializes the weight values in the network, i.e., resetting learning\nAlso calls InitActs", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx", "nt"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- only called automatically during InitWeights", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "Defaults", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "UnLesionNeurons", Doc: "UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "LesionNeurons", Doc: "LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer\nreturns number of neurons lesioned.  Emits error if prop > 1 as indication that percent\nmight have been passed", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"prop"}, Returns: []string{"int"}}}, Embeds: []types.Field{{Name: "LayerBase"}}, Fields: []types.Field{{Name: "Params", Doc: "Params are layer parameters (pointer to item in Network.LayerParams)."}, {Name: "Network", Doc: "our parent network, in case we need to use it to find\nother layers etc; set when added by network."}, {Name: "Type", Doc: "Type is the type of layer, which drives specialized computation as needed."}, {Name: "NNeurons", Doc: "NNeurons is the number of neurons in the layer."}, {Name: "NeurStIndex", Doc: "NeurStIndex is the starting index of neurons for this layer within\nthe global Network list."}, {Name: "NPools", Doc: "NPools is the number of inhibitory pools based on layer shape,\nwith the first one representing the entire set of neurons in the layer,\nand 4D shaped layers have sub-pools after that."}, {Name: "MaxData", Doc: "MaxData is the maximum amount of input data that can be processed in\nparallel in one pass of the network (copied from [NetworkIndexes]).\nNeuron, Pool, Values storage is allocated to hold this amount."}, {Name: "RecvPaths", Doc: "RecvPaths is the list of receiving pathways into this layer from other layers."}, {Name: "SendPaths", Doc: "SendPaths is the list of sending pathways from this layer to other layers."}, {Name: "BuildConfig", Doc: "BuildConfig has configuration data set when the network is configured,\nthat is used during the network Build() process via PostBuild method,\nafter all the structure of the network has been fully constructed.\nIn particular, the Params is nil until Build, so setting anything\nspecific in there (e.g., an index to another layer) must be done\nas a second pass.  Note that Params are all applied after Build\nand can set user-modifiable params, so this is for more special\nalgorithm structural parameters set during ConfigNet() methods."}, {Name: "DefaultParams", Doc: "DefaultParams are closures that apply default parameters\nprior to user-set parameters. These are useful for specific layer\nfunctionality in specialized brain areas (e.g., Rubicon, BG etc)\nnot associated with a layer type, which otherwise is used to hard-code\ninitial default parameters."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerIndexes", IDName: "layer-indexes", Doc: "LayerIndexes contains index access into network global arrays for GPU.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "NPools", Doc: "NPools is the total number of pools for this layer, including layer-wide."}, {Name: "NeurSt", Doc: "start of neurons for this layer in global array (same as Layer.NeurStIndex)"}, {Name: "NNeurons", Doc: "number of neurons in layer"}, {Name: "RecvSt", Doc: "start index into RecvPaths global array"}, {Name: "RecvN", Doc: "number of recv pathways"}, {Name: "SendSt", Doc: "start index into RecvPaths global array"}, {Name: "SendN", Doc: "number of recv pathways"}, {Name: "ExtsSt", Doc: "starting neuron index in global Exts list of external input for this layer.\nOnly for Input / Target / Compare layer types"}, {Name: "ShpPlY", Doc: "layer shape Pools Y dimension -- 1 for 2D"}, {Name: "ShpPlX", Doc: "layer shape Pools X dimension -- 1 for 2D"}, {Name: "ShpUnY", Doc: "layer shape Units Y dimension"}, {Name: "ShpUnX", Doc: "layer shape Units X dimension"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerInhibIndexes", IDName: "layer-inhib-indexes", Doc: "LayerInhibIndexes contains indexes of layers for between-layer inhibition.", Fields: []types.Field{{Name: "Index1", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib1Name if present -- -1 if not used"}, {Name: "Index2", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib2Name if present -- -1 if not used"}, {Name: "Index3", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib3Name if present -- -1 if not used"}, {Name: "Index4", Doc: "idx of Layer to geta layer-level inhibition from -- set during Build from BuildConfig LayInhib4Name if present -- -1 if not used"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerParams", IDName: "layer-params", Doc: "LayerParams contains all of the layer parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a read-only data storage buffer.", Fields: []types.Field{{Name: "Type", Doc: "Type is the functional type of layer, which determines the code path\nfor specialized layer types, and is synchronized with [Layer.Type]."}, {Name: "Index", Doc: "Index of this layer in [Layers] list."}, {Name: "MaxData", Doc: "MaxData is the maximum number of data parallel elements."}, {Name: "PoolSt", Doc: "PoolSt is the start of pools for this layer; first one is always the layer-wide pool."}, {Name: "Acts", Doc: "Activation parameters and methods for computing activations"}, {Name: "Inhib", Doc: "Inhibition parameters and methods for computing layer-level inhibition"}, {Name: "LayInhib", Doc: "LayInhib has indexes of layers that contribute between-layer inhibition\n to this layer. Set these indexes via BuildConfig LayInhibXName (X = 1, 2...)."}, {Name: "Learn", Doc: "Learn has learning parameters and methods that operate at the neuron level."}, {Name: "Bursts", Doc: "Bursts has [BurstParams] that determine how the 5IB Burst activation\nis computed from CaP integrated spiking values in Super layers."}, {Name: "CT", Doc: "CT has params for the CT corticothalamic layer and PTPred layer that\ngenerates predictions over the Pulvinar using context. Uses the CtxtGe\nexcitatory input plus stronger NMDA channels to maintain context trace."}, {Name: "Pulvinar", Doc: "Pulvinar has parameters for how the plus-phase (outcome) state of Pulvinar\nthalamic relay cell neurons is computed from the corresponding driver\nneuron Burst activation (or CaP if not Super)."}, {Name: "DSMatrix", Doc: "DSMatrixParams has parameters for dorsal Matrix layers, for SPN / MSN\ndirect and indirect pathways."}, {Name: "Striatum", Doc: "Striatum has params and indexes for striatum layers: DSMatrix, VSMatrix, DSPatch."}, {Name: "GP", Doc: "GP has params for GP (globus pallidus) of the BG layers."}, {Name: "IO", Doc: "IOParams has parameters for the IO inferior olive neurons,\nwhich compute a temporal offset error signal between CNiIO inhibitory\npredictions and excitatory sensory input, contingent on initial\nabove-threshold efferent copy motor trigger input (modulatory)."}, {Name: "CNeUp", Doc: "CNeUp has parameters for learning in the cerebellar nucleus\noutput neurons, which are tonically active and learn to maintain a target\nactivity level in the presence and absence of inputs."}, {Name: "LDT", Doc: "LDT has parameters for laterodorsal tegmentum ACh salience neuromodulatory\nsignal, driven by superior colliculus stimulus novelty, US input / absence,\nand OFC / ACC inhibition."}, {Name: "VTA", Doc: "VTA has parameters for ventral tegmental area dopamine (DA) based on\nLHb PVDA (primary value -- at US time, computed at start of each trial\nand stored in LHbPVDA global value) and Amygdala (CeM) CS / learned\nvalue (LV) activations, which update every cycle."}, {Name: "RWPred", Doc: "RWPred has parameters for reward prediction using a simple Rescorla-Wagner\nlearning rule (i.e., PV learning in the Rubicon framework)."}, {Name: "RWDa", Doc: "RWDa has parameters for reward prediction dopamine using a simple\nRescorla-Wagner learning rule (i.e., PV learning in the Rubicon framework)."}, {Name: "TDInteg", Doc: "TDInteg has parameters for temporal differences (TD) reward integration layer."}, {Name: "TDDa", Doc: "TDDa has parameters for dopamine (DA) signal as the temporal difference\n(TD) between the TDIntegLayer activations in the minus and plus phase."}, {Name: "Indexes", Doc: "Indexes has recv and send pathway array access info."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerTypes", IDName: "layer-types", Doc: "LayerTypes enumerates all the different types of layers,\nfor the different algorithm types supported.\nClass parameter styles automatically key off of these types."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerVars", IDName: "layer-vars", Doc: "LayerVars are layer-level state values."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnCaParams", IDName: "learn-ca-params", Doc: "LearnCaParams parameterizes the neuron-level calcium signals driving learning:\nLearnCa = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or\nuse the more complex and dynamaic VGCC channel directly.\nLearnCa is then integrated in a cascading manner at multiple time scales:\nCaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase).", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}, {Tool: "gosl", Directive: "import", Args: []string{"github.com/emer/axon/v2/kinase"}}}, Fields: []types.Field{{Name: "Norm", Doc: "Norm is the denominator used for normalizing [LearnCa], so the\nmax is roughly 1 - 1.5 or so, which works best in terms of previous\nstandard learning rules, and overall learning performance."}, {Name: "SpikeVGCC", Doc: "SpikeVGCC uses spikes to generate VGCC instead of actual VGCC current.\nSee SpikeVGCCa for calcium contribution from each spike."}, {Name: "SpikeVgccCa", Doc: "SpikeVgccCa is the multiplier on spike for computing Ca contribution\nto [LearnCa], in SpikeVGCC mode."}, {Name: "VgccTau", Doc: "VgccTau is the time constant of decay for VgccCa calcium.\nIt is highly transient around spikes, so decay and diffusion\nfactors are more important than for long-lasting NMDA factor.\nVgccCa is integrated separately in [VgccCaInt] prior to adding\ninto NMDA Ca in [LearnCa]."}, {Name: "ETraceTau", Doc: "ETraceTau is the time constant for integrating an eligibility trace factor,\nwhich computes an exponential integrator of local neuron-wise error gradients."}, {Name: "ETraceScale", Doc: "ETraceScale multiplies the contribution of the ETrace to learning, determining\nthe strength of its effect."}, {Name: "pad"}, {Name: "pad1"}, {Name: "Dt", Doc: "Dt are time constants for integrating [LearnCa] across\nM, P and D cascading levels."}, {Name: "VgccDt", Doc: "VgccDt rate = 1 / tau"}, {Name: "ETraceDt", Doc: "ETraceDt rate = 1 / tau"}, {Name: "NormInv", Doc: "NormInv = 1 / Norm"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnTimingParams", IDName: "learn-timing-params", Doc: "LearnTimingParams parameterizes the timing of Ca-driven Kinase\nalgorithm learning, based on detecting the first major peak of\ndifferential fast - slow activity associated with the start of\nthe minus phases: [TimePeak]. Learning can occur some number of ms later.", Fields: []types.Field{{Name: "SynCaCycles", Doc: "SynCaCycles is the number of cycles over which to integrate the synaptic\npre * post calcium trace, which provides the credit assignment factor.\nMust be a multiple of CaBinCycles (10). Used for all learning (timed or not)."}, {Name: "LearnThr", Doc: "LearnThr is the threshold on CaD that must be reached in order to be\neligible for learning. Applies to non-timing based learning too."}, {Name: "Refractory", Doc: "Refractory makes new learning depend on dropping below the learning\nthreshold. Applies to non-timing based learning too."}, {Name: "On", Doc: "On indicates whether to use the timing parameters to drive\nlearning timing, or instead just learn at the end of the trial\nautomatically."}, {Name: "StartThr", Doc: "StartThr is the threshold on the [TimeDiff] value to count as a peak."}, {Name: "Cycles", Doc: "Cycles is the number of cycles (ms) after the [TimePeak] before\nlearning occurs, or the peak detection is reset to start anew."}, {Name: "TimeDiffTau", Doc: "Time constant for integrating [TimeDiff] as the absolute value of\nCaDiff integrated over time to smooth out significant local bumps."}, {Name: "TimeDiffDt", Doc: "Dt is 1/Tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TrgAvgActParams", IDName: "trg-avg-act-params", Doc: "TrgAvgActParams govern the target and actual long-term average activity in neurons.\nTarget value is adapted by neuron-wise error and difference in actual vs. target.\ndrives synaptic scaling at a slow timescale (Network.SlowInterval).", Fields: []types.Field{{Name: "GiBaseInit", Doc: "GiBaseInit sets an initial [GiBase] value, as a proportion of TrgRange.Max - [TrgAvg].\nThis gives neurons differences in intrinsic inhibition / leak as a starting bias.\nThis is independent of using the target values to scale synaptic weights. Only used if > 0."}, {Name: "RescaleOn", Doc: "RescaleOn is whether to use target average activity mechanism to rescale\nsynaptic weights, so that activity tracks the target values."}, {Name: "ErrLRate", Doc: "ErrLRate is the learning rate for adjustments to [TrgAvg] value based on the\nneuron-level error signal. Population TrgAvg values are renormalized to\na fixed overall average, in TrgRange. Generally, deviating from the default value\nof this parameter doesn't make much difference."}, {Name: "SynScaleRate", Doc: "SynScaleRate is a rate parameter for how much to scale synaptic weights\nin proportion to the [AvgDif] between target and actual proportion activity.\nThis determines the effective strength of the constraint, and larger models\nmay need more than the weaker default value."}, {Name: "SubMean", Doc: "SubMean is the amount of the mean [TrgAvg] change to subtract when updating.\n1 = full zero sum changes. 1 works best in general, but in some cases it\nmay be better to start with 0 and then increase using network SetSubMean\nmethod at a later point."}, {Name: "Permute", Doc: "Permute the order of TrgAvg values within layer. Otherwise they are just\nassigned in order from highest to lowest for easy visualization.\nGenerally must be true if any topographic weights are being used."}, {Name: "Pool", Doc: "Pool means use pool-level target values if pool-level inhibition and\n4D pooled layers are present. If pool sizes are relatively small,\nthen may not be useful to distribute targets just within pool."}, {Name: "pad"}, {Name: "TrgRange", Doc: "TrgRange is the range of target normalized average activations.\nIndividual neuron [TrgAvg] values are assigned values within this range,\nand clamped within this range. This is a critical parameter and the default\nusually works best."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLRateParams", IDName: "rl-rate-params", Doc: "RLRateParams are receiving neuron learning rate modulation parameters.\nHas two factors: the derivative of the sigmoid based on CaD\nactivity levels, and the max-normalized phase-wise differences in activity\n(Diff): |CaP - CaD| / max(CaP, CaD).", Fields: []types.Field{{Name: "On", Doc: "On toggles use of learning rate modulation."}, {Name: "SigmoidLinear", Doc: "SigmoidLinear uses a linear sigmoid function: if act > .5: 1-act; else act\notherwise use the actual sigmoid derivative which is squared: a(1-a).\nThis can improve learning in some cases but is generally not beneficial."}, {Name: "SigmoidMin", Doc: "SigmoidMin is the minimum learning rate multiplier for sigmoidal\nact (1-act) factor, which prevents lrate from going too low for extreme values.\nSet to 1 to disable Sigmoid derivative factor, which is default for Target layers."}, {Name: "Diff", Doc: "Diff modulates learning rate as a function of max-normalized plus - minus\ndifferences, which reduces learning for more active neurons and emphasizes\nit for less active ones. This is typically essential.\nDiff = |CaP - CaD| / max(CaP, CaD)."}, {Name: "SpikeThr", Doc: "SpikeThr is the threshold on Max(CaP, CaD) below which Min lrate applies.\nMust be > 0 to prevent div by zero."}, {Name: "DiffThr", Doc: "DiffThr is the threshold on recv neuron error delta, i.e., |CaP - CaD|\nbelow which lrate is at Min value."}, {Name: "Min", Doc: "Min is the minimum learning rate value when |CaP - CaD| Diff is below DiffThr."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnNeuronParams", IDName: "learn-neuron-params", Doc: "LearnNeuronParams manages learning-related parameters at the neuron-level.\nThis is mainly the running average activations that drive learning", Fields: []types.Field{{Name: "CaLearn", Doc: "CaLearn parameterizes the neuron-level calcium signals driving learning:\nLearnCa = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking\nor use the more complex and dynamic VGCC channel directly.  LearnCa is then\nintegrated in a cascading manner at multiple time scales:\nLearnCaM (as in calmodulin), LearnCaP (ltP, CaMKII, plus phase),\nLearnCaD (ltD, DAPK1, minus phase)."}, {Name: "Timing", Doc: "LearnTimingParams parameterizes the timing of Ca-driven Kinase\nalgorithm learning, based on detecting the first major peak of\ndifferential fast - slow activity associated with the minus phase."}, {Name: "CaSpike", Doc: "CaSpike parameterizes the neuron-level spike-driven calcium signals:\nCaM (calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase).\nThese values are used in various cases as a proxy for the activation (spiking)\nbased learning signal."}, {Name: "LearnNMDA", Doc: "NMDA channel parameters used for learning, vs. the ones driving activation.\nThis allows exploration of learning parameters independent of their effects\non active maintenance contributions of NMDA, and may be supported by different\nreceptor subtypes."}, {Name: "TrgAvgAct", Doc: "TrgAvgAct has the synaptic scaling parameters for regulating overall average\nactivity compared to neuron's own target level."}, {Name: "RLRate", Doc: "RLRate has the recv neuron learning rate modulation params: an additional\nerror-based modulation of learning for receiver side:\nRLRate = |CaP - CaD| / Max(CaP, CaD)"}, {Name: "NeuroMod", Doc: "NeuroMod parameterizes neuromodulation effects on learning rate and activity,\nas a function of layer-level DA and ACh values, which are updated from global\nContext values, and computed from reinforcement learning algorithms."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtInitParams", IDName: "s-wt-init-params", Doc: "SWtInitParams for initial SWt (slow, structural weight) values.", Fields: []types.Field{{Name: "SPct", Doc: "SPct is how much of the initial random weights to capture in the\nslow, structural SWt values, with the rest going into the online learning\nLWt values. 1 gives the strongest initial biasing effect, for larger\nmodels that need more structural support. 0.5 should work for most models\nwhere stronger constraints are not needed."}, {Name: "Mean", Doc: "Mean is the target mean weight value across receiving neuron's pathway.\nThe mean SWt values are constrained to remain at this value.\nSome pathways may benefit from lower mean of .4."}, {Name: "Var", Doc: "Var is the initial variance in weight values, prior to constraints."}, {Name: "Sym", Doc: "Sym symmetrizes the initial weight values with those in reciprocal pathway.\nTypically true for bidirectional excitatory connections."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtAdaptParams", IDName: "s-wt-adapt-params", Doc: "SWtAdaptParams manages adaptation of the [SWt] (slow, structural weight) values.", Fields: []types.Field{{Name: "On", Doc: "On enables adaptation of [SWt] values at a slower time scale. If false, SWt\nvalues are not updated, in which case it is generally good to set Init.SPct=0 too."}, {Name: "LRate", Doc: "LRate is the learning rate multiplier on the accumulated [DWt] values\n(which already have fast LRate applied), to drive updating of [SWt]\nduring slow outer loop updating. Lower values impose stronger constraints,\nfor larger networks that need more structural support, e.g., 0.001 is better\nafter 1,000 epochs in large models. 0.1 is fine for smaller models."}, {Name: "SubMean", Doc: "SubMean is the amount of the mean to subtract from [SWt] delta when updating,\nto impose a zero-sum constraint on overall structural weight strengths.\nGenerally best to set to 1. There is a separate SubMean factor for [LWt]."}, {Name: "HiMeanDecay", Doc: "HiMeanDecay specifies a decay factor applied across all [LWt] weights\nin proportion to the deviation of the average effective weight value [Wt]\nabove the HiMeanThr threshold. This is applied at the slow learning interval\nand should be very slow, for counteracting a gradual accumulation in overall\nweights that can occur even with SubMean factors (which only operate on weights\nthat are actually changing on the current trial)."}, {Name: "HiMeanThr", Doc: "HiMeanThr specifies a decay factor applied across all [LWt] weights\nin proportion to the deviation of the average effective weight value [Wt]\naway from SWt.Init.Mean. This is applied at the slow learning interval\nand should be very slow, for counteracting a gradual accumulation in overall\nweights that can occur even with SubMean factors, which only operate on weights\nthat are actually changing on the current trial."}, {Name: "SigGain", Doc: "SigGain is the gain of the sigmoidal constrast enhancement function\nused to transform learned, linear [LWt] values into [Wt] values.\nThis is critical to offset the damping effect of exponential soft bounding,\nbut some special cases with different learning rules may benefit by making\nthis linear (1) instead."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtParams", IDName: "s-wt-params", Doc: "SWtParams manages structural, slowly adapting weight values [SWt],\nin terms of initialization and updating over course of learning.\nSWts impose initial and slowly adapting constraints on neuron connectivity\nto encourage differentiation of neuron representations and overall good behavior\nin terms of not hogging the representational space.\nThe [TrgAvg] activity constraint is not enforced through SWt: it needs to be\nmore dynamic and is supported by the regular learned weights [LWt].", Fields: []types.Field{{Name: "Init", Doc: "Init controls the initialization of [SWt] values."}, {Name: "Adapt", Doc: "Adapt controls adaptation of [SWt] values in response to [LWt] learning."}, {Name: "Limit", Doc: "Limit limits the range of [SWt] values, so that they do not fully\ndetermine the effective overall weight value."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateParams", IDName: "l-rate-params", Doc: "LRateParams manages learning rate parameters for scaling [DWt] delta\nweight values that then update [LWt] online learned weights.\nIt has two optional modulation factors on top of a Base learning rate.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Base", Doc: "Base learning rate for this pathway, which can be modulated\nby the other factors below. Generally larger networks use slower rates."}, {Name: "Sched", Doc: "Sched is a scheduled learning rate multiplier, simulating reduction\nin plasticity over aging. Use the [Network.LRateSched] method to apply\na given value to all pathways in the network."}, {Name: "Mod", Doc: "Mod is a dynamic learning rate modulation factor, typically driven by\nneuromodulation (e.g., dopamine)."}, {Name: "Eff", Doc: "Eff is the net effective actual learning rate multiplier used in\ncomputing [DWt]: Eff = Mod * Sched * Base"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DWtParams", IDName: "d-wt-params", Doc: "DWtParams has misc parameters for computing weight changes ([DWt]) for the default\nkinase trace-based error-driven cortical learning rule, and for other specialized\nlearning rules.", Fields: []types.Field{{Name: "SynCa20", Doc: "SynCa20 uses an effective 20msec time window for synaptic calcium computation\nfrom the [CaBins] values for send and recv neurons in computing the SynCa\nsynaptic calcium value. The default is 10msec, i.e., 1 bin, which works well\nfor most cases. This uses 2 bins if set."}, {Name: "CaPScale", Doc: "CaPScale is a separate multiplier for the CaP component of synaptic calcium, to\nallow separate weighting of potentiation (CaP) vs. depression (CaD) factors.\nAn increased CaP level results in an overall potentiation bias, which acts\nlike a hebbian learning factor, whereas a lower value produces more negatively\nbiased synaptic weight changes, which may help with an overall hogging dynamic.\nThe default of 1 works best in most cases."}, {Name: "SubMean", Doc: "SubMean is the amount of the mean [dWt] to subtract for updating the online\nlearning [LWt] values, producing a zero-sum effect. 1.0 = full zero-sum dWt.\nOnly applies to non-zero DWts. There is a separate such factor for [SWt].\nTypically set to 0 for standard trace learning pathways, although some require it\nfor stability over the long haul. Can use [Network.SetSubMean] to set to 1 after\nsignificant early learning has occurred with 0.\nSome special path types (e.g., Hebb) benefit from SubMean = 1 always."}, {Name: "SynTraceTau", Doc: "SynTraceTau is the time constant for integrating the synaptic trace [Tr]\nas a function of the synaptic activity credit assignment factor at the end\nof the theta cycle learning timescale. Larger values (greater than 1)\nproduce longer time windows of integration, and should only be used when\nthere is temporal structure to be learned across these longer timescales.\nThis synaptic trace is beneficial in addition to the receiver-based\neligibility trace ETraceLearn."}, {Name: "LearnThr", Doc: "LearnThr is the threshold for learning, applied to SynCa CaP and CaD for Kinase\ncortical learning rule.\nIn Matrix and VSPatch it applies to normalized GeIntNorm value: setting this relatively\nhigh encourages sparser representations."}, {Name: "SynTraceDt", Doc: "Dt rate = 1 / tau"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HebbParams", IDName: "hebb-params", Doc: "HebbParams for optional hebbian learning that replaces the\ndefault learning rule, based on S = sending activity,\nR = receiving activity", Fields: []types.Field{{Name: "On", Doc: "On turns on the use of the Hebbian learning rule instead of the default."}, {Name: "Up", Doc: "Up is the strength multiplier for hebbian increases, based on R * S * (1-LWt)."}, {Name: "Down", Doc: "Down is the strength multiplier for hebbian decreases, based on R * (1 - S) * LWt."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnSynParams", IDName: "learn-syn-params", Doc: "LearnSynParams manages learning-related parameters at the synapse-level.", Fields: []types.Field{{Name: "Learn", Doc: "Learn enables learning for this pathway."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "LRate", Doc: "LRateParams manages learning rate parameters for scaling [DWt] delta\nweight values that then update [LWt] online learned weights.\nIt has two optional modulation factors on top of a Base learning rate."}, {Name: "DWt", Doc: "DWtParams has misc parameters for computing weight changes ([DWt]) for the default\ntrace-based cortical learning rule and for other specialized learning rules."}, {Name: "Hebb", Doc: "hebbian learning option, which overrides the default learning rules"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateMod", IDName: "l-rate-mod", Doc: "LRateMod implements global learning rate modulation, based on a performance-based\nfactor, for example error. Increasing levels of the factor = higher learning rate.\nThis can be added to a Sim and called prior to DWt() to dynamically change lrate\nbased on overall network performance. It is not used by default in the standard params.", Directives: []types.Directive{{Tool: "gosl", Directive: "end"}}, Fields: []types.Field{{Name: "On", Doc: "toggle use of this modulation factor"}, {Name: "Base", Doc: "baseline learning rate -- what you get for correct cases"}, {Name: "pad"}, {Name: "pad1"}, {Name: "Range", Doc: "defines the range over which modulation occurs for the modulator factor -- Min and below get the Base level of learning rate modulation, Max and above get a modulation of 1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ViewTimes", IDName: "view-times", Doc: "ViewTimes are the options for when the NetView can be updated."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NetViewUpdate", IDName: "net-view-update", Doc: "NetViewUpdate manages time scales for updating the NetView.\nUse one of these for each mode you want to control separately.", Fields: []types.Field{{Name: "On", Doc: "On toggles update of display on"}, {Name: "Time", Doc: "Time scale to update the network view (Cycle to Trial timescales)."}, {Name: "CounterFunc", Doc: "CounterFunc returns the counter string showing current counters etc."}, {Name: "View", Doc: "View is the network view."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NetworkIndexes", IDName: "network-indexes", Doc: "NetworkIndexes are indexes and sizes for processing network.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "MaxData", Doc: "MaxData is the maximum number of data inputs that can be processed\nin parallel in one pass of the network.\nNeuron storage is allocated to hold this amount during\nBuild process, and this value reflects that."}, {Name: "MaxDelay", Doc: "MaxDelay is the maximum synaptic delay across all pathways at the time of\n[Network.Build]. This determines the size of the spike sending delay buffers."}, {Name: "NCaBins", Doc: "NCaBins is the total number of [CaBins] in the neuron state variables.\nSet to [Context.ThetaCycles] / [Context.CaBinCycles] in Build."}, {Name: "NLayers", Doc: "NLayers is the number of layers in the network."}, {Name: "NNeurons", Doc: "NNeurons is the total number of neurons."}, {Name: "NPools", Doc: "NPools is the total number of pools."}, {Name: "NPaths", Doc: "NPaths is the total number of paths."}, {Name: "NSyns", Doc: "NSyns is the total number of synapses."}, {Name: "RubiconNPosUSs", Doc: "RubiconNPosUSs is the total number of Rubicon Drives / positive USs."}, {Name: "RubiconNCosts", Doc: "RubiconNCosts is the total number of Rubicon Costs."}, {Name: "RubiconNNegUSs", Doc: "RubiconNNegUSs is the total number of .Rubicon Negative USs."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Network", IDName: "network", Doc: "Network implements the Axon spiking model.\nMost of the fields are copied to the global vars, needed for GPU,\nvia the SetAsCurrent method, and must be slices or tensors so that\nthere is one canonical underlying instance of all such data.\nThere are also Layer and Path lists that are used to scaffold the\nbuilding and display of the network, but contain no data.", Directives: []types.Directive{{Tool: "gosl", Directive: "end"}}, Methods: []types.Method{{Name: "InitWeights", Doc: "InitWeights initializes synaptic weights and all other associated long-term state variables\nincluding running-average state values (e.g., layer running average activations etc)", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- not automatically called", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "ShowAllGlobals", Doc: "ShowAllGlobals shows a listing of all Global variables and values.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "Build", Doc: "Build constructs the layer and pathway state based on the layer shapes\nand patterns of interconnectivity. Everything in the network must have been\nconfigured by this point, including key values in Context such as ThetaCycles\nand CaBinCycles which drive allocation of number of [CaBins] neuron\nvariables and corresponding [GvCaBinWts] global scalar variables.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Returns: []string{"error"}}}, Embeds: []types.Field{{Name: "NetworkBase"}}, Fields: []types.Field{{Name: "Rubicon", Doc: "Rubicon system for goal-driven motivated behavior,\nincluding Rubicon phasic dopamine signaling.\nManages internal drives, US outcomes. Core LHb (lateral habenula)\nand VTA (ventral tegmental area) dopamine are computed\nin equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nRenders USLayer, PVLayer, DrivesLayer representations\nbased on state updated here."}, {Name: "Layers", Doc: "Layers is the array of layers, used for CPU initialization, not GPU computation."}, {Name: "Paths", Doc: "Paths has pointers to all pathways in the network, sender-based, for CPU initialization,\nnot GPU computation."}, {Name: "LayerClassMap", Doc: "LayerClassMap is a map from class name to layer names."}, {Name: "NThreads", Doc: "NThreads is number of threads to use for parallel processing."}, {Name: "RecFunTimes", Doc: "record function timer information."}, {Name: "FunTimes", Doc: "timers for each major function (step of processing)."}, {Name: "LayerParams", Doc: "LayerParams are all the layer parameters. [NLayers]"}, {Name: "PathParams", Doc: "PathParams are all the path parameters, in sending order. [NPaths]"}, {Name: "NetworkIxs", Doc: "NetworkIxs have indexes and sizes for entire network (one only)."}, {Name: "PoolIxs", Doc: "PoolIxs have index values for each Pool.\n[Layer * Pools][PoolIndexVars]"}, {Name: "NeuronIxs", Doc: "NeuronIxs have index values for each neuron: index into layer, pools.\n[Neurons][Indexes]"}, {Name: "SynapseIxs", Doc: "SynapseIxs have index values for each synapse:\nproviding index into recv, send neurons, path.\n[Indexes][NSyns]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}, {Name: "PathSendCon", Doc: "PathSendCon are starting offset and N cons for each sending neuron,\nfor indexing into the Syns synapses, which are organized sender-based.\n[NSendCon][StartNN]; NSendCon = [Layer][SendPaths][SendNeurons]"}, {Name: "RecvPathIxs", Doc: "RecvPathIxs indexes into Paths (organized by SendPath) organized\nby recv pathways. needed for iterating through recv paths efficiently on GPU.\n[NRecvPaths] = [Layer][RecvPaths]"}, {Name: "PathRecvCon", Doc: "PathRecvCon are the receiving path starting index and number of connections.\n[NRecvCon][StartNN]; NRecvCon = [Layer][RecvPaths][RecvNeurons]"}, {Name: "RecvSynIxs", Doc: "RecvSynIxs are the indexes into Synapses for each recv neuron, organized\ninto blocks according to PathRecvCon, for receiver-based access.\n[NSyns] = [Layer][RecvPaths][RecvNeurons][Syns]"}, {Name: "Ctx", Doc: "Ctx is the context state (one). Other copies of Context can be maintained\nand [SetContext] to update this one, but this instance is the canonical one."}, {Name: "Neurons", Doc: "Neurons are all the neuron state variables.\n[Neurons][Data][Vars]"}, {Name: "NeuronAvgs", Doc: "NeuronAvgs are variables with averages over the\nData parallel dimension for each neuron.\n[Neurons][Vars]"}, {Name: "Pools", Doc: "Pools are the [PoolVars] float32 state values for layer and sub-pool inhibition,\nIncluding the float32 AvgMax values by Phase and variable: use [AvgMaxVarIndex].\n[Layer * Pools][Data][PoolVars+AvgMax]"}, {Name: "PoolsInt", Doc: "PoolsInt are the [PoolIntVars] int32 state values for layer and sub-pool\ninhibition, AvgMax atomic integration, and other vars: use [AvgMaxIntVarIndex]\n[Layer * Pools][Data][PoolIntVars+AvgMax]"}, {Name: "LayerStates", Doc: "LayerStates holds layer-level state values, with variables defined in\n[LayerVars], for each layer and Data parallel index.\n[Layer][Data][LayerVarsN]"}, {Name: "GlobalScalars", Doc: "GlobalScalars are the global scalar state variables.\n[GlobalScalarVarsN+2*NCaWeights][Data]"}, {Name: "GlobalVectors", Doc: "GlobalVectors are the global vector state variables.\n[GlobalVectorsN][MaxGlobalVecN][Data]"}, {Name: "Exts", Doc: "Exts are external input values for all Input / Target / Compare layers\nin the network. The ApplyExt methods write to this per layer,\nand it is then actually applied in one consistent method.\n[NExts][Data]; NExts = [In / Out Layers][Neurons]"}, {Name: "PathGBuf", Doc: "PathGBuf is the conductance buffer for accumulating spikes.\nSubslices are allocated to each pathway.\nUses int-encoded values for faster GPU atomic integration.\n[NPathNeur][Data][MaxDel+1]; NPathNeur = [Layer][RecvPaths][RecvNeurons]"}, {Name: "PathGSyns", Doc: "PathGSyns are synaptic conductance integrated over time per pathway\nper recv neurons. spikes come in via PathBuf.\nsubslices are allocated to each pathway.\n[NPathNeur][Data]"}, {Name: "Synapses", Doc: "\tSynapses are the synapse level variables (weights etc).\n\nThese do not depend on the data parallel index, unlike [SynapseTraces].\n[NSyns][Vars]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}, {Name: "SynapseTraces", Doc: "SynapseTraces are synaptic variables that depend on the data\nparallel index, for accumulating learning traces and weight changes per data.\nThis is the largest data size, so multiple instances are used\nto handle larger networks.\n[NSyns][Data][Vars]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DAModTypes", IDName: "da-mod-types", Doc: "DAModTypes are types of dopamine modulation of neural activity."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ValenceTypes", IDName: "valence-types", Doc: "ValenceTypes are types of valence coding: positive or negative."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuroModParams", IDName: "neuro-mod-params", Doc: "NeuroModParams specifies the effects of neuromodulators on neural\nactivity and learning rate.  These can apply to any neuron type,\nand are applied in the core cycle update equations.", Fields: []types.Field{{Name: "DAMod", Doc: "dopamine receptor-based effects of dopamine modulation\non excitatory and inhibitory conductances: D1 is excitatory,\nD2 is inhibitory as a function of increasing dopamine."}, {Name: "Valence", Doc: "valence coding of this layer, which may affect specific layer\ntypes but does not directly affect neuromodulators currently."}, {Name: "DAModGain", Doc: "dopamine modulation of excitatory and inhibitory conductances\n(i.e., \"performance dopamine\" effect: this does NOT affect\nlearning dopamine modulation in terms of RLrate): g *= 1 + (DAModGain * DA)."}, {Name: "DALRateSign", Doc: "modulate the sign of the learning rate factor according to\nthe DA sign, taking into account the DAMod sign reversal for D2Mod,\nalso using BurstGain and DipGain to modulate DA value.\nOtherwise, only the magnitude of the learning rate is modulated\nas a function of raw DA magnitude according to DALRateMod\n(without additional gain factors)."}, {Name: "DALRateMod", Doc: "if not using DALRateSign, this is the proportion of maximum learning\nrate that Abs(DA) magnitude can modulate.\ne.g., if 0.2, then DA = 0 = 80% of std learning rate, 1 = 100%."}, {Name: "AChLRateMod", Doc: "proportion of maximum learning rate that ACh can modulate.\ne.g., if 0.2, then ACh = 0 = 80% of std learning rate, 1 = 100%."}, {Name: "AChDisInhib", Doc: "amount of extra Gi inhibition added in proportion to 1 - ACh level.\nmakes ACh disinhibitory"}, {Name: "BurstGain", Doc: "multiplicative gain factor applied to positive dopamine signals.\nThis operates on the raw dopamine signal prior to any effect\nof D2 receptors in reversing its sign!"}, {Name: "DipGain", Doc: "multiplicative gain factor applied to negative dopamine signals.\nThis operates on the raw dopamine signal prior to any effect\nof D2 receptors in reversing its sign!\nshould be small for acq, but roughly equal to burst for ext."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronFlags", IDName: "neuron-flags", Doc: "NeuronFlags are bit-flags encoding relevant binary state for neurons"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronVars", IDName: "neuron-vars", Doc: "NeuronVars are the neuron variables representing current active state,\nspecific to each input data state.\nSee NeuronAvgVars for vars shared across data."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVars", IDName: "neuron-avg-vars", Doc: "NeuronAvgVars are mostly neuron variables involved in longer-term average activity\nwhich is aggregated over time and not specific to each input data state,\nalong with any other state that is not input data specific."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronIndexVars", IDName: "neuron-index-vars", Doc: "NeuronIndexVars are neuron-level indexes used to access layers and pools\nfrom the individual neuron level."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerSheets", IDName: "layer-sheets", Doc: "LayerSheets contains Layer parameter Sheets."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerSheet", IDName: "layer-sheet", Doc: "LayerSheet is one Layer parameter Sheet."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerSel", IDName: "layer-sel", Doc: "LayerSel is one Layer parameter Selector."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerSearches", IDName: "layer-searches", Doc: "LayerSearches is a list of parameter Search elements."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathSheets", IDName: "path-sheets", Doc: "PathSheets contains Path parameter Sheets."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathSheet", IDName: "path-sheet", Doc: "PathSheet is one Path parameter Sheet."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathSel", IDName: "path-sel", Doc: "PathSel is one Path parameter Selector."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathSearches", IDName: "path-searches", Doc: "PathSearches is a list of parameter Search elements."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Params", IDName: "params", Doc: "Params contains the [LayerParams] and [PathParams] parameter setting functions\nprovided by the [emergent] [params] package.", Fields: []types.Field{{Name: "Layer", Doc: "Layer has the parameters to apply to the [LayerParams] for layers."}, {Name: "Path", Doc: "Path has the parameters to apply to the [PathParams] for paths."}, {Name: "ExtraSheets", Doc: "ExtraSheets has optional additional sheets of parameters to apply\nafter the default Base sheet. Use \"Script\" for default Script sheet.\nMultiple names separated by spaces can be used (don't put spaces in Sheet names!)"}, {Name: "Tag", Doc: "Tag is an optional additional tag to add to log file names to identify\na specific run of the model (typically set by a config file or args)."}, {Name: "Script", Doc: "Script is a parameter setting script, which adds to the Layer and Path sheets\ntypically using the \"Script\" set name."}, {Name: "Interp", Doc: "Interp is the yaegi interpreter for running the script."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Path", IDName: "path", Doc: "Path implements axon spiking communication and learning.", Embeds: []types.Field{{Name: "PathBase"}}, Fields: []types.Field{{Name: "Params", Doc: "path parameters."}, {Name: "Send", Doc: "sending layer for this pathway."}, {Name: "Recv", Doc: "receiving layer for this pathway."}, {Name: "Type", Doc: "type of pathway."}, {Name: "DefaultParams", Doc: "DefaultParams are functions to apply parameters prior to user-set\nparameters. These are useful for specific functionality in specialized\nbrain areas (e.g., Rubicon, BG etc) not associated with a path type,\nwhich otherwise is used to hard-code initial default parameters."}, {Name: "RecvConNAvgMax", Doc: "average and maximum number of recv connections in the receiving layer"}, {Name: "SendConNAvgMax", Doc: "average and maximum number of sending connections in the sending layer"}, {Name: "SynStIndex", Doc: "start index into global Synapse array:"}, {Name: "NSyns", Doc: "number of synapses in this pathway"}, {Name: "RecvCon", Doc: "starting offset and N cons for each recv neuron, for indexing into the RecvSynIndex array of indexes into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathRecvCons slice for GPU usage."}, {Name: "RecvSynIndex", Doc: "index into Syns synaptic state for each sending unit and connection within that, for the sending pathway which does not own the synapses, and instead indexes into recv-ordered list"}, {Name: "RecvConIndex", Doc: "for each recv synapse, this is index of *sending* neuron  It is generally preferable to use the Synapse SendIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "SendCon", Doc: "starting offset and N cons for each sending neuron, for indexing into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathSendCons slice for GPU usage."}, {Name: "SendConIndex", Doc: "index of other neuron that receives the sender's synaptic input, ordered by the sending layer's order of units as the outer loop, and SendCon.N receiving units within that.  It is generally preferable to use the Synapse RecvIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.StartN", IDName: "start-n", Doc: "StartN holds a starting offset index and a number of items\narranged from Start to Start+N (exclusive).\nThis is not 16 byte padded and only for use on CPU side.", Fields: []types.Field{{Name: "Start", Doc: "starting offset"}, {Name: "N", Doc: "number of items --"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathIndexes", IDName: "path-indexes", Doc: "PathIndexes contains path-level index information into global memory arrays", Fields: []types.Field{{Name: "RecvLayer", Doc: "RecvLayer is the index of the receiving layer in global list of layers."}, {Name: "RecvNeurSt", Doc: "RecvNeurSt is the starting index of neurons in recv layer,\nso we don't need layer to get to neurons."}, {Name: "RecvNeurN", Doc: "RecvNeurN is the number of neurons in recv layer."}, {Name: "SendLayer", Doc: "SendLayer is the index of the sending layer in global list of layers."}, {Name: "SendNeurSt", Doc: "SendNeurSt is the starting index of neurons in sending layer,\nso we don't need layer to get to neurons."}, {Name: "SendNeurN", Doc: "SendNeurN is the number of neurons in send layer"}, {Name: "SynapseSt", Doc: "SynapseSt is the start index into global Synapse array.\n[Layer][SendPaths][Synapses]."}, {Name: "SendConSt", Doc: "SendConSt is the start index into global PathSendCon array.\n[Layer][SendPaths][SendNeurons]"}, {Name: "RecvConSt", Doc: "RecvConSt is the start index into global PathRecvCon array.\n[Layer][RecvPaths][RecvNeurons]"}, {Name: "RecvSynSt", Doc: "RecvSynSt is the start index into global sender-based Synapse index array.\n[Layer][SendPaths][Synapses]"}, {Name: "NPathNeurSt", Doc: "NPathNeurSt is the start NPathNeur index into PathGBuf, PathGSyns global arrays.\n[Layer][RecvPaths][RecvNeurons]"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GScaleValues", IDName: "g-scale-values", Doc: "GScaleValues holds the conductance scaling values.\nThese are computed once at start and remain constant thereafter,\nand therefore belong on Params and not on PathValues.", Fields: []types.Field{{Name: "Scale", Doc: "scaling factor for integrating synaptic input conductances (G's), originally computed as a function of sending layer activity and number of connections, and typically adapted from there -- see Path.PathScale adapt params"}, {Name: "Rel", Doc: "normalized relative proportion of total receiving conductance for this pathway: PathScale.Rel / sum(PathScale.Rel across relevant paths)"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathParams", IDName: "path-params", Doc: "PathParams contains all of the path parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a read-only storage buffer.", Fields: []types.Field{{Name: "Type", Doc: "Type is the functional type of path, which determines the code path\nfor specialized types, and is synchronized with [Path.Type]."}, {Name: "Index", Doc: "Index is the index of the pathway in global path list: [Layer][SendPaths]"}, {Name: "pad"}, {Name: "pad1"}, {Name: "Indexes", Doc: "recv and send neuron-level pathway index array access info"}, {Name: "Com", Doc: "synaptic communication parameters: delay, probability of failure"}, {Name: "PathScale", Doc: "pathway scaling parameters for computing GScale:\nmodulates overall strength of pathway, using both\nabsolute and relative factors, with adaptation option to maintain target max conductances"}, {Name: "SWts", Doc: "slowly adapting, structural weight value parameters,\nwhich control initial weight values and slower outer-loop adjustments"}, {Name: "Learn", Doc: "synaptic-level learning parameters for learning in the fast LWt values."}, {Name: "GScale", Doc: "conductance scaling values"}, {Name: "RLPred", Doc: "Params for RWPath and TDPredPath for doing dopamine-modulated learning\nfor reward prediction: Da * Send activity.\nUse in RWPredLayer or TDPredLayer typically to generate reward predictions.\nIf the Da sign is positive, the first recv unit learns fully; for negative,\nsecond one learns fully.\nLower lrate applies for opposite cases.  Weights are positive-only."}, {Name: "VSMatrix", Doc: "VSMatrix has parameters for trace-based learning in the VSMatrixPath.\nA trace of synaptic co-activity is formed, and then modulated by\ndopamine whenever it occurs.\nThis bridges the temporal gap between gating activity and subsequent activity,\nand is based biologically on synaptic tags.\nDSPatch provides modulation of trace activity based on local critic signal."}, {Name: "DSMatrix", Doc: "DSMatrix has parameters for trace-based learning in the DSMatrixPath.\nA trace of synaptic co-activity is formed, and then modulated by\ndopamine whenever it occurs.\nThis bridges the temporal gap between gating activity and subsequent activity,\nand is based biologically on synaptic tags.\nDSPatch provides modulation of trace activity based on local critic signal."}, {Name: "BLA", Doc: "Basolateral Amygdala pathway parameters."}, {Name: "Hip", Doc: "Hip bench parameters."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathTypes", IDName: "path-types", Doc: "PathTypes enumerates all the different types of axon pathways,\nfor the different algorithm types supported.\nClass parameter styles automatically key off of these types."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DSMatrixParams", IDName: "ds-matrix-params", Doc: "DSMatrixParams has parameters for DSMatrixLayer.\nDA, ACh learning rate modulation is pre-computed on the recv neuron\nRLRate variable via NeuroMod.\nMust set Learn.NeuroMod.DAMod = D1Mod or D2Mod via SetBuildConfig(\"DAMod\").", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "PatchD1Range", Doc: "PatchD1Range is the range of PatchD1 values to normalize into effective value."}, {Name: "PatchD2Range", Doc: "PatchD2Range is the range of PatchD2 values to normalize into effective value."}, {Name: "PatchDAModGain", Doc: "PatchDAModGain is a separate NeuroMod.DAModGain factor applying\nto DA performance gain effects from the Patch-based DA values.\nThe standard NeuroMod parameters apply only to the final outcome-based\ndopamine values."}, {Name: "PatchBurstGain", Doc: "PatchBurstGain is a separate NeuroMod.BurstGain-like factor applying\nto DA performance gain effects from the Patch-based DA values.\nThe standard NeuroMod parameters apply only to the final outcome-based\ndopamine values, which do not drive performance DA effects in dorsal striatum.\nNeuroMod.DAModGain does control overall performance gain from patch."}, {Name: "PatchD1Index", Doc: "Index of PatchD1 layer to get striosome modulation state from.\nSet during Build from BuildConfig PatchD1Name."}, {Name: "PatchD2Index", Doc: "Index of PatchD2 layer to get striosome modulation state from.\nSet during Build from BuildConfig PatchD2Name."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.StriatumParams", IDName: "striatum-params", Doc: "StriatumParams has params and indexes for BG Striatum layers including\nDSMatrixLayer, VSMatrixLayer, and DSPatchLayer.", Fields: []types.Field{{Name: "GateThr", Doc: "GateThr is the threshold on layer Avg CaPMax for Matrix Go and BG Thal\nlayers to count as having gated."}, {Name: "OtherIndex", Doc: "Index of other layer (D2 if we are D1 and vice-versa).\nSet during Build from BuildConfig OtherName."}, {Name: "PFIndex", Doc: "Index of PF parafasciculus layer to get gating output state from.\nSet during Build from BuildConfig PFName."}, {Name: "ThalLay1Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay2Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay3Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay4Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay5Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay6Index", Doc: "Index of thalamus layer that we gate. needed to get gating information.\nSet during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPLayerTypes", IDName: "gp-layer-types", Doc: "GPLayerTypes is a GPLayer axon-specific layer type enum."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPParams", IDName: "gp-params", Doc: "GPLayer represents a globus pallidus layer, including:\nGPePr, GPeAk (arkypallidal), and GPi (see GPType for type).\nTypically just a single unit per Pool representing a given stripe.", Fields: []types.Field{{Name: "GPType", Doc: "type of GP Layer -- must set during config using SetBuildConfig of GPType."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DSMatrixPathParams", IDName: "ds-matrix-path-params", Doc: "DSMatrixPathParams for trace-based learning in the MatrixPath.\nA trace of synaptic co-activity is formed, and then modulated by dopamine\nwhenever it occurs.  This bridges the temporal gap between gating activity\nand subsequent activity, and is based biologically on synaptic tags.\nTrace is applied to DWt and reset at the time of reward.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "PatchDA", Doc: "PatchDA is proportion of Credit trace factor for learning\nto modulate by PatchDA versus just standard s*r activity factor."}, {Name: "Credit", Doc: "Credit is proportion of trace activity driven by the credit assignment factor\nbased on the PF modulatory inputs, synaptic activity (send * recv),\nand Patch DA, which indicates extent to which gating at this time is net\nassociated with subsequent reward or not."}, {Name: "Delta", Doc: "Delta is weight for trace activity that is a function of the minus-plus delta\nactivity signal on the receiving SPN neuron, independent of PF modulation.\nThis should always be 1 except for testing disabling: adjust NonDelta\nrelative to it, and the overall learning rate."}, {Name: "D2Scale", Doc: "D2Scale is a scaling factor for the DAD2 learning factor relative to\nthe DAD1 contribution (which is 1 - DAD1)."}, {Name: "OffTrace", Doc: "OffTrace is a multiplier on trace contribution when action output\ncommunicated by PF is not above threshold."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.VSMatrixPathParams", IDName: "vs-matrix-path-params", Doc: "VSMatrixPathParams for trace-based learning in the VSMatrixPath,\nfor ventral striatum paths.\nA trace of synaptic co-activity is formed, and then modulated by dopamine\nwhenever it occurs.  This bridges the temporal gap between gating activity\nand subsequent activity, and is based biologically on synaptic tags.\nTrace is applied to DWt and reset at the time of reward.", Fields: []types.Field{{Name: "RewActLearn", Doc: "RewActLearn makes learning based on activity at time of reward,\nin inverse proportion to the GoalMaint activity: i.e., if there was no\ngoal maintenance, learn at reward to encourage goal engagement next time,\nbut otherwise, do not further reinforce at time of reward, because the\nactual goal gating learning trace is a better learning signal.\nOtherwise, only uses accumulated trace but doesn't include rew-time activity,\ne.g., for testing cases that do not have GoalMaint."}, {Name: "Delta", Doc: "Delta is weight for trace activity that is a function of the minus-plus delta\nactivity signal on the receiving SPN neuron, independent of PF modulation.\nThis should always be 1 except for testing disabling: adjust NonDelta\nrelative to it, and the overall learning rate."}, {Name: "Credit", Doc: "Credit is proportion of trace activity driven by the credit assignment factor\nbased on the PF modulatory inputs, synaptic activity (send * recv),\nand Patch DA, which indicates extent to which gating at this time is net\nassociated with subsequent reward or not."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PoolIndexVars", IDName: "pool-index-vars"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PoolIntVars", IDName: "pool-int-vars", Doc: "PoolIntVars are int32 pool variables, for computing fsfffb inhibition etc.\nNote that we use int32 instead of uint32 so that overflow errors can be detected.\nSee [PoolVars] for float32 variables."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMax", IDName: "avg-max", Doc: "AvgMax are Avg and Max"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxPhases", IDName: "avg-max-phases", Doc: "AvgMaxPhases are the different Phases over which AvgMax values are tracked."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxVars", IDName: "avg-max-vars", Doc: "AvgMaxVars are the different Neuron variables for which [AvgMaxPhases]\nis computed."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RandFunIndex", IDName: "rand-fun-index", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWPredParams", IDName: "rw-pred-params", Doc: "RWPredParams parameterizes reward prediction for a simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "PredRange", Doc: "default 0.1..0.99 range of predictions that can be represented -- having a truncated range preserves some sensitivity in dopamine at the extremes of good or poor performance"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWDaParams", IDName: "rw-da-params", Doc: "RWDaParams computes a dopamine (DA) signal using simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "RWPredLayIndex", Doc: "idx of RWPredLayer to get reward prediction from -- set during Build from BuildConfig RWPredLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDIntegParams", IDName: "td-integ-params", Doc: "TDIntegParams are params for reward integrator layer", Fields: []types.Field{{Name: "Discount", Doc: "discount factor -- how much to discount the future prediction from TDPred"}, {Name: "PredGain", Doc: "gain factor on TD rew pred activations"}, {Name: "TDPredLayIndex", Doc: "idx of TDPredLayer to get reward prediction from -- set during Build from BuildConfig TDPredLayName"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDDaParams", IDName: "td-da-params", Doc: "TDDaParams are params for dopamine (DA) signal as the temporal difference (TD)\nbetween the TDIntegLayer activations in the minus and plus phase.", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "TDIntegLayIndex", Doc: "idx of TDIntegLayer to get reward prediction from -- set during Build from BuildConfig TDIntegLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLPredPathParams", IDName: "rl-pred-path-params", Doc: "RLPredPathParams does dopamine-modulated learning for reward prediction: Da * Send.Act\nUsed by RWPath and TDPredPath within corresponding RWPredLayer or TDPredLayer\nto generate reward predictions based on its incoming weights, using linear activation\nfunction. Has no weight bounds or limits on sign etc.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "OppSignLRate", Doc: "how much to learn on opposite DA sign coding neuron (0..1)"}, {Name: "DaTol", Doc: "tolerance on DA -- if below this abs value, then DA goes to zero and there is no learning -- prevents prediction from exactly learning to cancel out reward value, retaining a residual valence of signal"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LDTParams", IDName: "ldt-params", Doc: "LDTParams compute reward salience as ACh global neuromodulatory signal\nas a function of the MAX activation of its inputs from salience detecting\nlayers (e.g., the superior colliculus: SC), and whenever there is an external\nUS outcome input (signalled by the global GvHasRew flag).\nACh from salience inputs is discounted by GoalMaint activity,\nreducing distraction when pursuing a goal, but US ACh activity is not so reduced.\nACh modulates excitability of goal-gating layers.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "SrcThr", Doc: "SrcThr is the threshold per input source, on absolute value (magnitude),\nto count as a significant reward event, which then drives maximal ACh.\nSet to 0 to disable this nonlinear behavior."}, {Name: "Rew", Doc: "Rew uses the global Context.NeuroMod.HasRew flag to drive ACh:\nif there is some kind of external reward being given, then\nACh goes to 1, else 0 for this component."}, {Name: "MaintInhib", Doc: "MaintInhib is the extent to which active goal maintenance (via Global GoalMaint)\ninhibits ACh signals: when goal engaged, distractability is lower."}, {Name: "SrcLay1Index", Doc: "index of Layer to get max activity from; set during Build from BuildConfig\nSrcLay1Name if present -- -1 if not used."}, {Name: "SrcLay2Index", Doc: "index of Layer to get max activity from; set during Build from BuildConfig\nSrcLay2Name if present -- -1 if not used."}, {Name: "SrcLay3Index", Doc: "index of Layer to get max activity from; set during Build from BuildConfig\nSrcLay3Name if present -- -1 if not used."}, {Name: "SrcLay4Index", Doc: "index of Layer to get max activity from; set during Build from BuildConfig\nSrcLay4Name if present -- -1 if not used."}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.VTAParams", IDName: "vta-params", Doc: "VTAParams are for computing overall VTA DA based on LHb PVDA\n(primary value -- at US time, computed at start of each trial\nand stored in LHbPVDA global value)\nand Amygdala (CeM) CS / learned value (LV) activations, which update\nevery cycle.", Fields: []types.Field{{Name: "CeMGain", Doc: "gain on CeM activity difference (CeMPos - CeMNeg) for generating LV CS-driven dopamine values"}, {Name: "LHbGain", Doc: "gain on computed LHb DA (Burst - Dip) -- for controlling DA levels"}, {Name: "AChThr", Doc: "threshold on ACh level required to generate LV CS-driven dopamine burst"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLAPathParams", IDName: "bla-path-params", Doc: "BLAPathParams has parameters for basolateral amygdala learning.\nLearning is driven by the Tr trace as function of ACh * Send Act\nrecorded prior to US, and at US, recv unit delta: CaP - CaDPrev\ntimes normalized GeIntNorm for recv unit credit assignment.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "NegDeltaLRate", Doc: "use 0.01 for acquisition (don't unlearn) and 1 for extinction.\nnegative delta learning rate multiplier"}, {Name: "AChThr", Doc: "threshold on this layer's ACh level for trace learning updates"}, {Name: "USTrace", Doc: "proportion of US time stimulus activity to use for the trace component of"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DriveParams", IDName: "drive-params", Doc: "DriveParams manages the drive parameters for computing and updating drive state.\nMost of the params are for optional case where drives are automatically\nupdated based on US consumption (which satisfies drives) and time passing\n(which increases drives).", Fields: []types.Field{{Name: "DriveMin", Doc: "minimum effective drive value, which is an automatic baseline ensuring\nthat a positive US results in at least some minimal level of reward.\nUnlike Base values, this is not reflected in the activity of the drive\nvalues, and applies at the time of reward calculation as a minimum baseline."}, {Name: "Base", Doc: "baseline levels for each drive, which is what they naturally trend toward\nin the absence of any input.  Set inactive drives to 0 baseline,\nactive ones typically elevated baseline (0-1 range)."}, {Name: "Tau", Doc: "time constants in ThetaCycle (trial) units for natural update toward\nBase values. 0 values means no natural update (can be updated externally)."}, {Name: "Satisfaction", Doc: "decrement in drive value when US is consumed, thus partially satisfying\nthe drive. Positive values are subtracted from current Drive value."}, {Name: "Dt", Doc: "1/Tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.UrgencyParams", IDName: "urgency-params", Doc: "UrgencyParams has urgency (increasing pressure to do something)\nand parameters for updating it.\nRaw urgency integrates effort when _not_ goal engaged\nwhile effort (negative US 0) integrates when a goal _is_ engaged.", Fields: []types.Field{{Name: "U50", Doc: "value of raw urgency where the urgency activation level is 50%"}, {Name: "Power", Doc: "exponent on the urge factor -- valid numbers are 1,2,4,6"}, {Name: "Thr", Doc: "threshold for urge -- cuts off small baseline values"}, {Name: "DAtonic", Doc: "gain factor for driving tonic DA levels as a function of urgency"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.USParams", IDName: "us-params", Doc: "USParams control how positive and negative USs and Costs are\nweighted and integrated to compute an overall PV primary value.", Fields: []types.Field{{Name: "PVposGain", Doc: "gain factor applied to sum of weighted, drive-scaled positive USs\nto compute PVpos primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust the overall scaling of PVpos reward within 0-1\nnormalized range (see also PVnegGain).\nEach USpos is assumed to be in 0-1 range, with a default of 1."}, {Name: "PVnegGain", Doc: "gain factor applied to sum of weighted negative USs and Costs\nto compute PVneg primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust overall scaling of PVneg within 0-1\nnormalized range (see also PVposGain)."}, {Name: "USnegGains", Doc: "Negative US gain factor for encoding each individual negative US,\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the USneg pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see PVnegWts), and generally must be larger than the weights to leverage\nthe dynamic range within each US pool."}, {Name: "CostGains", Doc: "Cost gain factor for encoding the individual Time, Effort etc costs\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the Cost pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see CostWts), and generally must be larger than the weights to use\nthe full dynamic range within each US pool."}, {Name: "PVposWts", Doc: "weight factor applied to each separate positive US on the way to computing\nthe overall PVpos summary value, to control the weighting of each US\nrelative to the others. Each pos US is also multiplied by its dynamic\nDrive factor as well.\nUse PVposGain to control the overall scaling of the PVpos value."}, {Name: "PVnegWts", Doc: "weight factor applied to each separate negative US on the way to computing\nthe overall PVneg summary value, to control the weighting of each US\nrelative to the others, and to the Costs.  These default to 1."}, {Name: "PVcostWts", Doc: "weight factor applied to each separate Cost (Time, Effort, etc) on the\nway to computing the overall PVneg summary value, to control the weighting\nof each Cost relative to the others, and relative to the negative USs.\nThe first pool is Time, second is Effort, and these are typically weighted\nlower (.02) than salient simulation-specific USs (1)."}, {Name: "USposEst", Doc: "computed estimated US values, based on OFCposPT and VSMatrix gating, in PVposEst"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LHbParams", IDName: "l-hb-params", Doc: "LHbParams has values for computing LHb & RMTg which drives dips / pauses in DA firing.\nLHb handles all US-related (PV = primary value) processing.\nPositive net LHb activity drives dips / pauses in VTA DA activity,\ne.g., when predicted pos > actual or actual neg > predicted.\nNegative net LHb activity drives bursts in VTA DA activity,\ne.g., when actual pos > predicted (redundant with LV / Amygdala)\nor \"relief\" burst when actual neg < predicted.", Fields: []types.Field{{Name: "VSPatchNonRewThr", Doc: "threshold on VSPatch prediction during a non-reward trial"}, {Name: "VSPatchGain", Doc: "gain on the VSPatchD1 - D2 difference to drive the net VSPatch DA\nprediction signal, which goes in VSPatchPos and RewPred global variables"}, {Name: "VSPatchVarTau", Doc: "decay time constant for computing the temporal variance in VSPatch\nvalues over time"}, {Name: "NegThr", Doc: "threshold factor that multiplies integrated pvNeg value\nto establish a threshold for whether the integrated pvPos value\nis good enough to drive overall net positive reward.\nIf pvPos wins, it is then multiplicatively discounted by pvNeg;\notherwise, pvNeg is discounted by pvPos."}, {Name: "BurstGain", Doc: "gain multiplier on PVpos for purposes of generating bursts\n(not for discounting negative dips)."}, {Name: "DipGain", Doc: "gain multiplier on PVneg for purposes of generating dips\n(not for discounting positive bursts)."}, {Name: "VSPatchVarDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GiveUpParams", IDName: "give-up-params", Doc: "GiveUpParams are parameters for computing when to give up,\nbased on Utility, Timing and Progress factors.", Fields: []types.Field{{Name: "ProbThr", Doc: "threshold on GiveUp probability, below which no give up is triggered"}, {Name: "MinGiveUpSum", Doc: "minimum GiveUpSum value, which is the denominator in the sigmoidal function.\nThis minimum prevents division by zero and any other degenerate values."}, {Name: "Utility", Doc: "the factor multiplying utility values: cost and expected positive outcome"}, {Name: "Timing", Doc: "the factor multiplying timing values from VSPatch"}, {Name: "Progress", Doc: "the factor multiplying progress values based on time-integrated progress\ntoward the goal"}, {Name: "MinUtility", Doc: "minimum utility cost and reward estimate values -- when they are below\nthese levels (at the start) then utility is effectively neutral,\nso the other factors take precedence."}, {Name: "VSPatchSumMax", Doc: "maximum VSPatchPosSum for normalizing the value for give-up weighing"}, {Name: "VSPatchVarMax", Doc: "maximum VSPatchPosVar for normalizing the value for give-up weighing"}, {Name: "ProgressRateTau", Doc: "time constant for integrating the ProgressRate\nvalues over time"}, {Name: "ProgressRateDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Rubicon", IDName: "rubicon", Doc: "Rubicon implements core elements of the Rubicon goal-directed motivational\nmodel, representing the core brainstem-level (hypothalamus) bodily drives\nand resulting dopamine from US (unconditioned stimulus) inputs,\nsubsuming the earlier Rubicon model of primary value (PV)\nand learned value (LV), describing the functions of the Amygala,\nVentral Striatum, VTA and associated midbrain nuclei (LDT, LHb, RMTg).\nCore LHb (lateral habenula) and VTA (ventral tegmental area) dopamine\nare computed in equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nThe Drives, Effort, US and resulting LHb PV dopamine computation all happens at the\nat the start of each trial (NewState, Step).  The LV / CS dopamine is computed\ncycle-by-cycle by the VTA layer using parameters set by the VTA layer.\nRenders USLayer, PVLayer, DrivesLayer representations based on state updated here.", Fields: []types.Field{{Name: "NPosUSs", Doc: "number of possible positive US states and corresponding drives.\nThe first is always reserved for novelty / curiosity.\nMust be set programmatically via SetNUSs method,\nwhich allocates corresponding parameters."}, {Name: "NNegUSs", Doc: "number of possible phasic negative US states (e.g., shock, impact etc).\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "NCosts", Doc: "number of possible costs, typically including accumulated time and effort costs.\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "Drive", Doc: "parameters and state for built-in drives that form the core motivations\nof the agent, controlled by lateral hypothalamus and associated\nbody state monitoring such as glucose levels and thirst."}, {Name: "Urgency", Doc: "urgency (increasing pressure to do something) and parameters for\n\n\tupdating it. Raw urgency is incremented by same units as effort,\n\nbut is only reset with a positive US."}, {Name: "USs", Doc: "controls how positive and negative USs are weighted and integrated to\ncompute an overall PV primary value."}, {Name: "LHb", Doc: "lateral habenula (LHb) parameters and state, which drives\ndipping / pausing in dopamine when the predicted positive\noutcome > actual, or actual negative outcome > predicted.\nCan also drive bursting for the converse, and via matrix phasic firing."}, {Name: "GiveUp", Doc: "parameters for giving up based on PV pos - neg difference"}, {Name: "ValDecode", Doc: "population code decoding parameters for estimates from layers"}, {Name: "decodeActs"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.FieldValue", IDName: "field-value", Doc: "FieldValue holds the value of a field in a struct.", Fields: []types.Field{{Name: "Path"}, {Name: "Field"}, {Name: "Value"}, {Name: "Parent"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseVars", IDName: "synapse-vars", Doc: "SynapseVars are the synapse variables representing synaptic weights, etc.\nThese do not depend on the data parallel index (di).\nSee [SynapseTraceVars] for variables that do depend on di."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseTraceVars", IDName: "synapse-trace-vars", Doc: "SynapseTraceVars are synaptic variables that depend on the data\nparallel index, for accumulating learning traces and weight changes per data."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseIndexVars", IDName: "synapse-index-vars", Doc: "SynapseIndexVars are synapse-level indexes used to access neurons and paths\nfrom the individual synapse level of processing."})
