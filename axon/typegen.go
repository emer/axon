// Code generated by "core generate -add-types"; DO NOT EDIT.

package axon

import (
	"cogentcore.org/core/types"
)

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathGTypes", IDName: "path-g-types", Doc: "PathGTypes represents the conductance (G) effects of a given pathway,\nincluding excitatory, inhibitory, and modulatory."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynComParams", IDName: "syn-com-params", Doc: "SynComParams are synaptic communication parameters:\nused in the Path parameters.  Includes delay and\nprobability of failure, and Inhib for inhibitory connections,\nand modulatory pathways that have multiplicative-like effects.", Fields: []types.Field{{Name: "GType", Doc: "type of conductance (G) communicated by this pathway"}, {Name: "Delay", Doc: "additional synaptic delay in msec for inputs arriving at this pathway.\nMust be <= MaxDelay which is set during network building based on MaxDelay\nof any existing Path in the network. Delay = 0 means a spike reaches\nreceivers in the next Cycle, which is the minimum time (1 msec).\nBiologically, subtract 1 from biological synaptic delay values to set\ncorresponding Delay value."}, {Name: "MaxDelay", Doc: "maximum value of Delay, based on MaxDelay values when the BuildGBuf\nfunction was called during [Network.Build]. Cannot set it longer than this,\nexcept by calling BuildGBuf on network after changing MaxDelay to a larger\nvalue in any pathway in the network."}, {Name: "DelLen", Doc: "delay length = actual length of the GBuf buffer per neuron = Delay+1; just for speed"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathScaleParams", IDName: "path-scale-params", Doc: "PathScaleParams are pathway scaling parameters: modulates overall strength of pathway,\nusing both absolute and relative factors.", Fields: []types.Field{{Name: "Rel", Doc: "relative scaling that shifts balance between different pathways -- this is subject to normalization across all other pathways into receiving neuron, and determines the GScale.Target for adapting scaling"}, {Name: "Abs", Doc: "absolute multiplier adjustment factor for the path scaling -- can be used to adjust for idiosyncrasies not accommodated by the standard scaling based on initial target activation level and relative scaling factors -- any adaptation operates by directly adjusting scaling factor from the initially computed value"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeParams", IDName: "spike-params", Doc: "SpikeParams contains spiking activation function params.\nImplements a basic thresholded Vm model, and optionally\nthe AdEx adaptive exponential function (adapt is KNaAdapt)", Fields: []types.Field{{Name: "Thr", Doc: "threshold value Theta (Q) for firing output activation (.5 is more accurate value based on AdEx biological parameters and normalization"}, {Name: "VmR", Doc: "post-spiking membrane potential to reset to, produces refractory effect if lower than VmInit -- 0.3 is appropriate biologically based value for AdEx (Brette & Gurstner, 2005) parameters.  See also RTau"}, {Name: "Tr", Doc: "post-spiking explicit refractory period, in cycles -- prevents Vm updating for this number of cycles post firing -- Vm is reduced in exponential steps over this period according to RTau, being fixed at Tr to VmR exactly"}, {Name: "RTau", Doc: "time constant for decaying Vm down to VmR -- at end of Tr it is set to VmR exactly -- this provides a more realistic shape of the post-spiking Vm which is only relevant for more realistic channels that key off of Vm -- does not otherwise affect standard computation"}, {Name: "Exp", Doc: "if true, turn on exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpSlope", Doc: "slope in Vm (2 mV = .02 in normalized units) for extra exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpThr", Doc: "membrane potential threshold for actually triggering a spike when using the exponential mechanism"}, {Name: "MaxHz", Doc: "for translating spiking interval (rate) into rate-code activation equivalent, what is the maximum firing rate associated with a maximum activation value of 1"}, {Name: "ISITau", Doc: "constant for integrating the spiking interval in estimating spiking rate"}, {Name: "ISIDt", Doc: "rate = 1 / tau"}, {Name: "RDt", Doc: "rate = 1 / tau"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DendParams", IDName: "dend-params", Doc: "DendParams are the parameters for updating dendrite-specific dynamics", Fields: []types.Field{{Name: "GbarExp", Doc: "dendrite-specific strength multiplier of the exponential spiking drive on Vm -- e.g., .5 makes it half as strong as at the soma (which uses Gbar.L as a strength multiplier per the AdEx standard model)"}, {Name: "GbarR", Doc: "dendrite-specific conductance of Kdr delayed rectifier currents, used to reset membrane potential for dendrite -- applied for Tr msec"}, {Name: "SSGi", Doc: "SST+ somatostatin positive slow spiking inhibition level specifically affecting dendritic Vm (VmDend) -- this is important for countering a positive feedback loop from NMDA getting stronger over the course of learning -- also typically requires SubMean = 1 for TrgAvgAct and learning to fully counter this feedback loop."}, {Name: "HasMod", Doc: "set automatically based on whether this layer has any recv pathways that have a GType conductance type of Modulatory -- if so, then multiply GeSyn etc by GModSyn"}, {Name: "ModGain", Doc: "multiplicative gain factor on the total modulatory input -- this can also be controlled by the PathScale.Abs factor on ModulatoryG inputs, but it is convenient to be able to control on the layer as well."}, {Name: "ModACh", Doc: "if true, modulatory signal also includes ACh multiplicative factor"}, {Name: "ModBase", Doc: "baseline modulatory level for modulatory effects -- net modulation is ModBase + ModGain * GModSyn"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActInitParams", IDName: "act-init-params", Doc: "ActInitParams are initial values for key network state variables.\nInitialized in InitActs called by InitWeights, and provides target values for DecayState.", Fields: []types.Field{{Name: "Vm", Doc: "initial membrane potential -- see Erev.L for the resting potential (typically .3)"}, {Name: "Act", Doc: "initial activation value -- typically 0"}, {Name: "GeBase", Doc: "baseline level of excitatory conductance (net input) -- Ge is initialized to this value, and it is added in as a constant background level of excitatory input -- captures all the other inputs not represented in the model, and intrinsic excitability, etc"}, {Name: "GiBase", Doc: "baseline level of inhibitory conductance (net input) -- Gi is initialized to this value, and it is added in as a constant background level of inhibitory input -- captures all the other inputs not represented in the model"}, {Name: "GeVar", Doc: "variance (sigma) of gaussian distribution around baseline Ge values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "GiVar", Doc: "variance (sigma) of gaussian distribution around baseline Gi values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DecayParams", IDName: "decay-params", Doc: "DecayParams control the decay of activation state in the DecayState function\ncalled in NewState when a new state is to be processed.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Act", Doc: "proportion to decay most activation state variables toward initial values at start of every ThetaCycle (except those controlled separately below) -- if 1 it is effectively equivalent to full clear, resetting other derived values.  ISI is reset every AlphaCycle to get a fresh sample of activations (doesn't affect direct computation -- only readout)."}, {Name: "Glong", Doc: "proportion to decay long-lasting conductances, NMDA and GABA, and also the dendritic membrane potential -- when using random stimulus order, it is important to decay this significantly to allow a fresh start -- but set Act to 0 to enable ongoing activity to keep neurons in their sensitive regime."}, {Name: "AHP", Doc: "decay of afterhyperpolarization currents, including mAHP, sAHP, and KNa, Kir -- has a separate decay because often useful to have this not decay at all even if decay is on."}, {Name: "LearnCa", Doc: "decay of Ca variables driven by spiking activity used in learning: CaSpk* and Ca* variables. These are typically not decayed but may need to be in some situations."}, {Name: "OnRew", Doc: "decay layer at end of ThetaCycle when there is a global reward -- true by default for PTPred, PTMaint and PFC Super layers"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DtParams", IDName: "dt-params", Doc: "DtParams are time and rate constants for temporal derivatives in Axon (Vm, G)", Fields: []types.Field{{Name: "Integ", Doc: "overall rate constant for numerical integration, for all equations at the unit level -- all time constants are specified in millisecond units, with one cycle = 1 msec -- if you instead want to make one cycle = 2 msec, you can do this globally by setting this integ value to 2 (etc).  However, stability issues will likely arise if you go too high.  For improved numerical stability, you may even need to reduce this value to 0.5 or possibly even lower (typically however this is not necessary).  MUST also coordinate this with network.time_inc variable to ensure that global network.time reflects simulated time accurately"}, {Name: "VmTau", Doc: "membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmDendTau", Doc: "dendritic membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmSteps", Doc: "number of integration steps to take in computing new Vm value -- this is the one computation that can be most numerically unstable so taking multiple steps with proportionally smaller dt is beneficial"}, {Name: "GeTau", Doc: "time constant for decay of excitatory AMPA receptor conductance."}, {Name: "GiTau", Doc: "time constant for decay of inhibitory GABAa receptor conductance."}, {Name: "IntTau", Doc: "time constant for integrating values over timescale of an individual input state (e.g., roughly 200 msec -- theta cycle), used in computing ActInt, GeInt from Ge, and GiInt from GiSyn -- this is used for scoring performance, not for learning, in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life),"}, {Name: "LongAvgTau", Doc: "time constant for integrating slower long-time-scale averages, such as nrn.ActAvg, Pool.ActsMAvg, ActsPAvg -- computed in NewState when a new input state is present (i.e., not msec but in units of a theta cycle) (tau is roughly how long it takes for value to change significantly) -- set lower for smaller models"}, {Name: "MaxCycStart", Doc: "cycle to start updating the SpkMaxCa, SpkMax values within a theta cycle -- early cycles often reflect prior state"}, {Name: "VmDt", Doc: "nominal rate = Integ / tau"}, {Name: "VmDendDt", Doc: "nominal rate = Integ / tau"}, {Name: "DtStep", Doc: "1 / VmSteps"}, {Name: "GeDt", Doc: "rate = Integ / tau"}, {Name: "GiDt", Doc: "rate = Integ / tau"}, {Name: "IntDt", Doc: "rate = Integ / tau"}, {Name: "LongAvgDt", Doc: "rate = 1 / tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeNoiseParams", IDName: "spike-noise-params", Doc: "SpikeNoiseParams parameterizes background spiking activity impinging on the neuron,\nsimulated using a poisson spiking process.", Fields: []types.Field{{Name: "On", Doc: "add noise simulating background spiking levels"}, {Name: "GeHz", Doc: "mean frequency of excitatory spikes -- typically 50Hz but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Ge", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "GiHz", Doc: "mean frequency of inhibitory spikes -- typically 100Hz fast spiking but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Gi", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "MaintGe", Doc: "add Ge noise to GeMaintRaw instead of standard Ge -- used for PTMaintLayer for example"}, {Name: "GeExpInt", Doc: "Exp(-Interval) which is the threshold for GeNoiseP as it is updated"}, {Name: "GiExpInt", Doc: "Exp(-Interval) which is the threshold for GiNoiseP as it is updated"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ClampParams", IDName: "clamp-params", Doc: "ClampParams specify how external inputs drive excitatory conductances\n(like a current clamp) -- either adds or overwrites existing conductances.\nNoise is added in either case.", Fields: []types.Field{{Name: "IsInput", Doc: "is this a clamped input layer?  set automatically based on layer type at initialization"}, {Name: "IsTarget", Doc: "is this a target layer?  set automatically based on layer type at initialization"}, {Name: "Ge", Doc: "amount of Ge driven for clamping -- generally use 0.8 for Target layers, 1.5 for Input layers"}, {Name: "Add", Doc: "add external conductance on top of any existing -- generally this is not a good idea for target layers (creates a main effect that learning can never match), but may be ok for input layers"}, {Name: "ErrThr", Doc: "threshold on neuron Act activity to count as active for computing error relative to target in PctErr method"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SMaintParams", IDName: "s-maint-params", Doc: "SMaintParams for self-maintenance simulating a population of\nNMDA-interconnected spiking neurons", Fields: []types.Field{{Name: "On", Doc: "is self maintenance active?"}, {Name: "NNeurons", Doc: "number of neurons within the self-maintenance pool,\neach of which is assumed to have the same probability of spiking"}, {Name: "Gbar", Doc: "conductance multiplier for self maintenance synapses"}, {Name: "Inhib", Doc: "inhib controls how much of the extra maintenance conductance goes to the GeExt, which drives extra proportional inhibition"}, {Name: "ISI", Doc: "ISI (inter spike interval) range -- min is used as min ISIAvg for poisson spike rate expected from the population, and above max, no additional maintenance conductance is added"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PopCodeParams", IDName: "pop-code-params", Doc: "PopCodeParams provides an encoding of scalar value using population code,\nwhere a single continuous (scalar) value is encoded as a gaussian bump\nacross a population of neurons (1 dimensional).\nIt can also modulate rate code and number of neurons active according to the value.\nThis is for layers that represent values as in the Rubicon system (from Context.Rubicon).\nBoth normalized activation values (1 max) and Ge conductance values can be generated.", Fields: []types.Field{{Name: "On", Doc: "use popcode encoding of variable(s) that this layer represents"}, {Name: "Ge", Doc: "Ge multiplier for driving excitatory conductance based on PopCode -- multiplies normalized activation values"}, {Name: "Min", Doc: "minimum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "Max", Doc: "maximum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "MinAct", Doc: "activation multiplier for values at Min end of range, where values at Max end have an activation of 1 -- if this is &lt; 1, then there is a rate code proportional to the value in addition to the popcode pattern -- see also MinSigma, MaxSigma"}, {Name: "MinSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "MaxSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "Clip", Doc: "ensure that encoded and decoded value remains within specified range"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActParams", IDName: "act-params", Doc: "axon.ActParams contains all the activation computation params and functions\nfor basic Axon, at the neuron level .\nThis is included in axon.Layer to drive the computation.", Fields: []types.Field{{Name: "Spikes", Doc: "Spiking function parameters"}, {Name: "Dend", Doc: "dendrite-specific parameters"}, {Name: "Init", Doc: "initial values for key network state variables -- initialized in InitActs called by InitWeights, and provides target values for DecayState"}, {Name: "Decay", Doc: "amount to decay between AlphaCycles, simulating passage of time and effects of saccades etc, especially important for environments with random temporal structure (e.g., most standard neural net training corpora)"}, {Name: "Dt", Doc: "time and rate constants for temporal derivatives / updating of activation state"}, {Name: "Gbar", Doc: "maximal conductances levels for channels"}, {Name: "Erev", Doc: "reversal potentials for each channel"}, {Name: "Clamp", Doc: "how external inputs drive neural activations"}, {Name: "Noise", Doc: "how, where, when, and how much noise to add"}, {Name: "VmRange", Doc: "range for Vm membrane potential -- -- important to keep just at extreme range of reversal potentials to prevent numerical instability"}, {Name: "Mahp", Doc: "M-type medium time-scale afterhyperpolarization mAHP current -- this is the primary form of adaptation on the time scale of multiple sequences of spikes"}, {Name: "Sahp", Doc: "slow time-scale afterhyperpolarization sAHP current -- integrates CaSpkD at theta cycle intervals and produces a hard cutoff on sustained activity for any neuron"}, {Name: "KNa", Doc: "sodium-gated potassium channel adaptation parameters -- activates a leak-like current as a function of neural activity (firing = Na influx) at two different time-scales (Slick = medium, Slack = slow)"}, {Name: "Kir", Doc: "potassium (K) inwardly rectifying (ir) current, which is similar to GABAB\n(which is a GABA modulated Kir channel).  This channel is off by default\nbut plays a critical role in making medium spiny neurons (MSNs) relatively\nquiet in the striatum."}, {Name: "NMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "MaintNMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "GabaB", Doc: "GABA-B / GIRK channel parameters"}, {Name: "VGCC", Doc: "voltage gated calcium channels -- provide a key additional source of Ca for learning and positive-feedback loop upstate for active neurons"}, {Name: "AK", Doc: "A-type potassium (K) channel that is particularly important for limiting the runaway excitation from VGCC channels"}, {Name: "SKCa", Doc: "small-conductance calcium-activated potassium channel produces the pausing function as a consequence of rapid bursting."}, {Name: "SMaint", Doc: "for self-maintenance simulating a population of\nNMDA-interconnected spiking neurons"}, {Name: "PopCode", Doc: "provides encoding population codes, used to represent a single continuous (scalar) value, across a population of units / neurons (1 dimensional)"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLANovelPath", IDName: "bla-novel-path", Doc: "BLANovelPath connects all other pools to the first, Novelty, pool in a BLA layer.\nThis allows the known US representations to specifically inhibit the novelty pool."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Context", IDName: "context", Doc: "Context contains all of the global context state info\nthat is shared across every step of the computation.\nIt is passed around to all relevant computational functions,\nand is updated on the CPU and synced to the GPU after every cycle.\nIt contains timing, Testing vs. Training mode, random number context, etc.\nThere is one canonical instance on the network as Ctx, always get it from\nthe network.Context() method.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "NData", Doc: "number of data parallel items to process currently."}, {Name: "Mode", Doc: "current running mode, using sim-defined enum, e.g., Train, Test, etc."}, {Name: "Testing", Doc: "Testing is true if the model is being run in a testing mode,\nso no weight changes or other associated computations should be done.\nThis flag should only affect learning-related behavior."}, {Name: "Phase", Doc: "Phase counter: typicaly 0-1 for minus-plus."}, {Name: "PlusPhase", Doc: "PlusPhase is true if this is the plus phase, when the outcome / bursting\nis occurring, driving positive learning; else minus phase."}, {Name: "PhaseCycle", Doc: "Cycle within current phase, minus or plus."}, {Name: "Cycle", Doc: "Cycle within Trial: number of iterations of activation updating (settling)\non the current state. This is reset at NewState."}, {Name: "ThetaCycles", Doc: "ThetaCycles is the length of the theta cycle (i.e., Trial), in terms of 1 msec Cycles.\nSome network update steps depend on doing something at the end of the\ntheta cycle (e.g., CTCtxtPath)."}, {Name: "CyclesTotal", Doc: "CyclesTotal is the accumulated cycle count, which increments continuously\nfrom whenever it was last reset. Typically this is the number of milliseconds\nin simulation time."}, {Name: "Time", Doc: "Time is the accumulated amount of time the network has been running,\nin simulation-time (not real world time), in seconds."}, {Name: "TrialsTotal", Doc: "TrialsTotal is the total trial count, which increments continuously in NewState\n_only in Train mode_ from whenever it was last reset. Can be used for synchronizing\nweight updates across nodes."}, {Name: "TimePerCycle", Doc: "TimePerCycle is the amount of Time to increment per cycle."}, {Name: "SlowInterval", Doc: "SlowInterval is how frequently in Trials to perform slow adaptive processes\nsuch as synaptic scaling, inhibition adaptation, associated in the brain with sleep,\nvia the SlowAdapt method.  This should be long enough for meaningful changes\nto accumulate. 100 is default but could easily be longer in larger models.\nBecause SlowCounter is incremented by NData, high NData cases (e.g. 16) likely need to\nincrease this value, e.g., 400 seems to produce overall consistent results in various models."}, {Name: "SlowCounter", Doc: "SlowCounter increments for each training trial, to trigger SlowAdapt at SlowInterval.\nThis is incremented by NData to maintain consistency across different values of this parameter."}, {Name: "pad"}, {Name: "pad1"}, {Name: "RandCounter", Doc: "RandCounter is the random counter, incremented by maximum number of\npossible random numbers generated per cycle, regardless of how\nmany are actually used. This is shared across all layers so must\nencompass all possible param settings."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BurstParams", IDName: "burst-params", Doc: "BurstParams determine how the 5IB Burst activation is computed from\nCaSpkP integrated spiking values in Super layers -- thresholded.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"deep_layers"}}}, Fields: []types.Field{{Name: "ThrRel", Doc: "Relative component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  This is the distance between the average and maximum activation values within layer (e.g., 0 = average, 1 = max).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "ThrAbs", Doc: "Absolute component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CTParams", IDName: "ct-params", Doc: "CTParams control the CT corticothalamic neuron special behavior", Fields: []types.Field{{Name: "GeGain", Doc: "gain factor for context excitatory input, which is constant as compared to the spiking input from other pathways, so it must be downscaled accordingly.  This can make a difference and may need to be scaled up or down."}, {Name: "DecayTau", Doc: "decay time constant for context Ge input -- if > 0, decays over time so intrinsic circuit dynamics have to take over.  For single-step copy-based cases, set to 0, while longer-time-scale dynamics should use 50 (80 for 280 cycles)"}, {Name: "DecayDt", Doc: "1 / tau"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PulvParams", IDName: "pulv-params", Doc: "PulvParams provides parameters for how the plus-phase (outcome)\nstate of Pulvinar thalamic relay cell neurons is computed from\nthe corresponding driver neuron Burst activation (or CaSpkP if not Super)", Fields: []types.Field{{Name: "DriveScale", Doc: "multiplier on driver input strength, multiplies CaSpkP from driver layer to produce Ge excitatory input to Pulv unit."}, {Name: "FullDriveAct", Doc: "Level of Max driver layer CaSpkP at which the drivers fully drive the burst phase activation.  If there is weaker driver input, then (Max/FullDriveAct) proportion of the non-driver inputs remain and this critically prevents the network from learning to turn activation off, which is difficult and severely degrades learning."}, {Name: "DriveLayIndex", Doc: "index of layer that generates the driving activity into this one -- set via SetBuildConfig(DriveLayName) setting"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GlobalScalarVars", IDName: "global-scalar-vars", Doc: "GlobalScalarVars are network-wide scalar variables, such as neuromodulators,\nreward, etc including the state for the Rubicon phasic dopamine model.\nThese are stored in the Network.GlobalScalars tensor and corresponding global variable."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GlobalVectorVars", IDName: "global-vector-vars", Doc: "GlobalVectorVars are network-wide vector variables, such as drives,\ncosts, US outcomes, with [MaxGlobalVecN] values per variable.\nThese are stored in the Network.GlobalVectors tensor and\ncorresponding global variable."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPUVars", IDName: "gpu-vars", Doc: "GPUVars is an enum for GPU variables, for specifying what to sync."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipConfig", IDName: "hip-config", Doc: "HipConfig have the hippocampus size and connectivity parameters", Fields: []types.Field{{Name: "EC2Size", Doc: "size of EC2"}, {Name: "EC3NPool", Doc: "number of EC3 pools (outer dimension)"}, {Name: "EC3NNrn", Doc: "number of neurons in one EC3 pool"}, {Name: "CA1NNrn", Doc: "number of neurons in one CA1 pool"}, {Name: "CA3Size", Doc: "size of CA3"}, {Name: "DGRatio", Doc: "size of DG / CA3"}, {Name: "EC3ToEC2PCon", Doc: "percent connectivity from EC3 to EC2"}, {Name: "EC2ToDGPCon", Doc: "percent connectivity from EC2 to DG"}, {Name: "EC2ToCA3PCon", Doc: "percent connectivity from EC2 to CA3"}, {Name: "CA3ToCA1PCon", Doc: "percent connectivity from CA3 to CA1"}, {Name: "DGToCA3PCon", Doc: "percent connectivity into CA3 from DG"}, {Name: "EC2LatRadius", Doc: "lateral radius of connectivity in EC2"}, {Name: "EC2LatSigma", Doc: "lateral gaussian sigma in EC2 for how quickly weights fall off with distance"}, {Name: "MossyDelta", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in training, applied at the start of a trial to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "MossyDeltaTest", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in testing, applied during 2nd-3rd quarters to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "ThetaLow", Doc: "low theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "ThetaHigh", Doc: "high theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "EC5Clamp", Doc: "flag for clamping the EC5 from EC5ClampSrc"}, {Name: "EC5ClampSrc", Doc: "source layer for EC5 clamping activations in the plus phase -- biologically it is EC3 but can use an Input layer if available"}, {Name: "EC5ClampTest", Doc: "clamp the EC5 from EC5ClampSrc during testing as well as training -- this will overwrite any target values that might be used in stats (e.g., in the basic hip example), so it must be turned off there"}, {Name: "EC5ClampThr", Doc: "threshold for binarizing EC5 clamp values -- any value above this is clamped to 1, else 0 -- helps produce a cleaner learning signal.  Set to 0 to not perform any binarization."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipPathParams", IDName: "hip-path-params", Doc: "HipPathParams define behavior of hippocampus paths, which have special learning rules", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"hip_paths"}}}, Fields: []types.Field{{Name: "Hebb", Doc: "Hebbian learning proportion"}, {Name: "Err", Doc: "EDL proportion"}, {Name: "SAvgCor", Doc: "proportion of correction to apply to sending average activation for hebbian learning component (0=none, 1=all, .5=half, etc)"}, {Name: "SAvgThr", Doc: "threshold of sending average activation below which learning does not occur (prevents learning when there is no input)"}, {Name: "SNominal", Doc: "sending layer Nominal (need to manually set it to be the same as the sending layer)"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActAvgParams", IDName: "act-avg-params", Doc: "ActAvgParams represents the nominal average activity levels in the layer\nand parameters for adapting the computed Gi inhibition levels to maintain\naverage activity within a target range.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}, {Tool: "gosl", Directive: "import", Args: []string{"github.com/emer/axon/v2/fsfffb"}}}, Fields: []types.Field{{Name: "Nominal", Doc: "nominal estimated average activity level in the layer, which is used in computing the scaling factor on sending pathways from this layer.  In general it should roughly match the layer ActAvg.ActMAvg value, which can be logged using the axon.LogAddDiagnosticItems function.  If layers receiving from this layer are not getting enough Ge excitation, then this Nominal level can be lowered to increase pathway strength (fewer active neurons means each one contributes more, so scaling factor goes as the inverse of activity level), or vice-versa if Ge is too high.  It is also the basis for the target activity level used for the AdaptGi option -- see the Offset which is added to this value."}, {Name: "AdaptGi", Doc: "enable adapting of layer inhibition Gi multiplier factor (stored in layer GiMult value) to maintain a target layer level of ActAvg.Nominal.  This generally works well and improves the long-term stability of the models.  It is not enabled by default because it depends on having established a reasonable Nominal + Offset target activity level."}, {Name: "Offset", Doc: "offset to add to Nominal for the target average activity that drives adaptation of Gi for this layer.  Typically the Nominal level is good, but sometimes Nominal must be adjusted up or down to achieve desired Ge scaling, so this Offset can compensate accordingly."}, {Name: "HiTol", Doc: "tolerance for higher than Target target average activation as a proportion of that target value (0 = exactly the target, 0.2 = 20% higher than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "LoTol", Doc: "tolerance for lower than Target target average activation as a proportion of that target value (0 = exactly the target, 0.5 = 50% lower than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "AdaptRate", Doc: "rate of Gi adaptation as function of AdaptRate * (Target - ActMAvg) / Target -- occurs at spaced intervals determined by Network.SlowInterval value -- slower values such as 0.01 may be needed for large networks and sparse layers."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.InhibParams", IDName: "inhib-params", Doc: "InhibParams contains all the inhibition computation params and functions for basic Axon.\nThis is included in LayerParams to support computation.\nAlso includes the expected average activation in the layer, which is used for\nG conductance rescaling and potentially for adapting inhibition over time.", Fields: []types.Field{{Name: "ActAvg", Doc: "ActAvg has layer-level and pool-level average activation initial values\nand updating / adaptation thereof.\nInitial values help determine initial scaling factors."}, {Name: "Layer", Doc: "Layer determines inhibition across the entire layer.\nInput layers generally use Gi = 0.8 or 0.9, 1.3 or higher for sparse layers.\nIf the layer has sub-pools (4D shape) then this is effectively between-pool inhibition."}, {Name: "Pool", Doc: "Pool determines inhibition within sub-pools of units, for layers with 4D shape.\nThis is almost always necessary if the layer has sub-pools."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Layer", IDName: "layer", Doc: "Layer implements the basic Axon spiking activation function,\nand manages learning in the pathways.", Methods: []types.Method{{Name: "InitWeights", Doc: "InitWeights initializes the weight values in the network, i.e., resetting learning\nAlso calls InitActs", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx", "nt"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- only called automatically during InitWeights", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "Defaults", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "UnLesionNeurons", Doc: "UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "LesionNeurons", Doc: "LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer\nreturns number of neurons lesioned.  Emits error if prop > 1 as indication that percent\nmight have been passed", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"prop"}, Returns: []string{"int"}}}, Embeds: []types.Field{{Name: "LayerBase"}}, Fields: []types.Field{{Name: "Params", Doc: "Params are layer parameters (pointer to item in Network.LayerParams)."}, {Name: "Network", Doc: "our parent network, in case we need to use it to find\nother layers etc; set when added by network."}, {Name: "Type", Doc: "Type is the type of layer, which drives specialized computation as needed."}, {Name: "NNeurons", Doc: "NNeurons is the number of neurons in the layer."}, {Name: "NeurStIndex", Doc: "NeurStIndex is the starting index of neurons for this layer within\nthe global Network list."}, {Name: "NPools", Doc: "NPools is the number of inhibitory pools based on layer shape,\nwith the first one representing the entire set of neurons in the layer,\nand 4D shaped layers have sub-pools after that."}, {Name: "MaxData", Doc: "MaxData is the maximum amount of input data that can be processed in\nparallel in one pass of the network (copied from [NetworkIndexes]).\nNeuron, Pool, Values storage is allocated to hold this amount."}, {Name: "RecvPaths", Doc: "RecvPaths is the list of receiving pathways into this layer from other layers."}, {Name: "SendPaths", Doc: "SendPaths is the list of sending pathways from this layer to other layers."}, {Name: "BuildConfig", Doc: "BuildConfig has configuration data set when the network is configured,\nthat is used during the network Build() process via PostBuild method,\nafter all the structure of the network has been fully constructed.\nIn particular, the Params is nil until Build, so setting anything\nspecific in there (e.g., an index to another layer) must be done\nas a second pass.  Note that Params are all applied after Build\nand can set user-modifiable params, so this is for more special\nalgorithm structural parameters set during ConfigNet() methods."}, {Name: "DefaultParams", Doc: "DefaultParams are default parameters that are applied prior to user-set\nparameters. These are useful for specific layer functionality in specialized\nbrain areas (e.g., Rubicon, BG etc) not associated with a layer type,\nwhich otherwise is used to hard-code initial default parameters.\nTypically just set to a literal map."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerIndexes", IDName: "layer-indexes", Doc: "LayerIndexes contains index access into network global arrays for GPU.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "NPools", Doc: "NPools is the total number of pools for this layer, including layer-wide."}, {Name: "NeurSt", Doc: "start of neurons for this layer in global array (same as Layer.NeurStIndex)"}, {Name: "NNeurons", Doc: "number of neurons in layer"}, {Name: "RecvSt", Doc: "start index into RecvPaths global array"}, {Name: "RecvN", Doc: "number of recv pathways"}, {Name: "SendSt", Doc: "start index into RecvPaths global array"}, {Name: "SendN", Doc: "number of recv pathways"}, {Name: "ExtsSt", Doc: "starting neuron index in global Exts list of external input for this layer.\nOnly for Input / Target / Compare layer types"}, {Name: "ShpPlY", Doc: "layer shape Pools Y dimension -- 1 for 2D"}, {Name: "ShpPlX", Doc: "layer shape Pools X dimension -- 1 for 2D"}, {Name: "ShpUnY", Doc: "layer shape Units Y dimension"}, {Name: "ShpUnX", Doc: "layer shape Units X dimension"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerInhibIndexes", IDName: "layer-inhib-indexes", Doc: "LayerInhibIndexes contains indexes of layers for between-layer inhibition.", Fields: []types.Field{{Name: "Index1", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib1Name if present -- -1 if not used"}, {Name: "Index2", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib2Name if present -- -1 if not used"}, {Name: "Index3", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib3Name if present -- -1 if not used"}, {Name: "Index4", Doc: "idx of Layer to geta layer-level inhibition from -- set during Build from BuildConfig LayInhib4Name if present -- -1 if not used"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerParams", IDName: "layer-params", Doc: "LayerParams contains all of the layer parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []types.Field{{Name: "Type", Doc: "functional type of layer -- determines functional code path for specialized layer types, and is synchronized with the Layer.Type value"}, {Name: "Index", Doc: "Index of this layer in [Layers] list."}, {Name: "MaxData", Doc: "MaxData is the maximum number of data parallel elements."}, {Name: "PoolSt", Doc: "PoolSt is the start of pools for this layer; first one is always the layer-wide pool."}, {Name: "Acts", Doc: "Activation parameters and methods for computing activations"}, {Name: "Inhib", Doc: "Inhibition parameters and methods for computing layer-level inhibition"}, {Name: "LayInhib", Doc: "indexes of layers that contribute between-layer inhibition to this layer -- set these indexes via BuildConfig LayInhibXName (X = 1, 2...)"}, {Name: "Learn", Doc: "Learning parameters and methods that operate at the neuron level"}, {Name: "Bursts", Doc: "BurstParams determine how the 5IB Burst activation is computed from CaSpkP integrated spiking values in Super layers -- thresholded."}, {Name: "CT", Doc: "] params for the CT corticothalamic layer and PTPred layer that generates predictions over the Pulvinar using context -- uses the CtxtGe excitatory input plus stronger NMDA channels to maintain context trace"}, {Name: "Pulv", Doc: "provides parameters for how the plus-phase (outcome) state of Pulvinar thalamic relay cell neurons is computed from the corresponding driver neuron Burst activation (or CaSpkP if not Super)"}, {Name: "Matrix", Doc: "parameters for BG Striatum Matrix MSN layers, which are the main Go / NoGo gating units in BG."}, {Name: "GP", Doc: "type of GP Layer."}, {Name: "LDT", Doc: "parameterizes laterodorsal tegmentum ACh salience neuromodulatory signal, driven by superior colliculus stimulus novelty, US input / absence, and OFC / ACC inhibition"}, {Name: "VTA", Doc: "parameterizes computing overall VTA DA based on LHb PVDA (primary value -- at US time, computed at start of each trial and stored in LHbPVDA global value) and Amygdala (CeM) CS / learned value (LV) activations, which update every cycle."}, {Name: "RWPred", Doc: "parameterizes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework)."}, {Name: "RWDa", Doc: "parameterizes reward prediction dopamine for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework)."}, {Name: "TDInteg", Doc: "parameterizes TD reward integration layer"}, {Name: "TDDa", Doc: "parameterizes dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase."}, {Name: "Indexes", Doc: "recv and send pathway array access info"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerTypes", IDName: "layer-types", Doc: "LayerTypes enumerates all the different types of layers,\nfor the different algorithm types supported.\nClass parameter styles automatically key off of these types."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerVars", IDName: "layer-vars", Doc: "LayerVars are layer-level state values."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CaLrnParams", IDName: "ca-lrn-params", Doc: "CaLrnParams parameterizes the neuron-level calcium signals driving learning:\nCaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or\nuse the more complex and dynamic VGCC channel directly.\nCaLrn is then integrated in a cascading manner at multiple time scales:\nCaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase).", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}, {Tool: "gosl", Directive: "import", Args: []string{"github.com/emer/axon/v2/kinase"}}}, Fields: []types.Field{{Name: "Norm", Doc: "denomenator used for normalizing CaLrn, so the max is roughly 1 - 1.5 or so, which works best in terms of previous standard learning rules, and overall learning performance"}, {Name: "SpkVGCC", Doc: "use spikes to generate VGCC instead of actual VGCC current -- see SpkVGCCa for calcium contribution from each spike"}, {Name: "SpkVgccCa", Doc: "multiplier on spike for computing Ca contribution to CaLrn in SpkVGCC mode"}, {Name: "VgccTau", Doc: "time constant of decay for VgccCa calcium -- it is highly transient around spikes, so decay and diffusion factors are more important than for long-lasting NMDA factor.  VgccCa is integrated separately int VgccCaInt prior to adding into NMDA Ca in CaLrn"}, {Name: "Dt", Doc: "time constants for integrating CaLrn across M, P and D cascading levels"}, {Name: "UpdateThr", Doc: "Threshold on CaSpkP CaSpkD value for updating synapse-level Ca values (SynCa) -- this is purely a performance optimization that excludes random infrequent spikes -- 0.05 works well on larger networks but not smaller, which require the .01 default."}, {Name: "VgccDt", Doc: "rate = 1 / tau"}, {Name: "NormInv", Doc: "= 1 / Norm"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TrgAvgActParams", IDName: "trg-avg-act-params", Doc: "TrgAvgActParams govern the target and actual long-term average activity in neurons.\nTarget value is adapted by neuron-wise error and difference in actual vs. target.\ndrives synaptic scaling at a slow timescale (Network.SlowInterval).", Fields: []types.Field{{Name: "GiBaseInit", Doc: "if this is > 0, then each neuron's GiBase is initialized as this proportion of TrgRange.Max - TrgAvg -- gives neurons differences in intrinsic inhibition / leak as a starting bias.  This is independent of using the target values to scale synaptic weights."}, {Name: "RescaleOn", Doc: "whether to use target average activity mechanism to rescale synaptic weights, so that activity tracks the target values"}, {Name: "ErrLRate", Doc: "learning rate for adjustments to Trg value based on unit-level error signal.  Population TrgAvg values are renormalized to fixed overall average in TrgRange. Generally, deviating from the default doesn't make much difference."}, {Name: "SynScaleRate", Doc: "rate parameter for how much to scale synaptic weights in proportion to the AvgDif between target and actual proportion activity -- this determines the effective strength of the constraint, and larger models may need more than the weaker default value."}, {Name: "SubMean", Doc: "amount of mean trg change to subtract -- 1 = full zero sum.  1 works best in general -- but in some cases it may be better to start with 0 and then increase using network SetSubMean method at a later point."}, {Name: "Permute", Doc: "permute the order of TrgAvg values within layer -- otherwise they are just assigned in order from highest to lowest for easy visualization -- generally must be true if any topographic weights are being used"}, {Name: "Pool", Doc: "use pool-level target values if pool-level inhibition and 4D pooled layers are present -- if pool sizes are relatively small, then may not be useful to distribute targets just within pool"}, {Name: "pad"}, {Name: "TrgRange", Doc: "range of target normalized average activations -- individual neurons are assigned values within this range to TrgAvg, and clamped within this range."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLRateParams", IDName: "rl-rate-params", Doc: "RLRateParams are recv neuron learning rate modulation parameters.\nHas two factors: the derivative of the sigmoid based on CaSpkD\nactivity levels, and based on the phase-wise differences in activity (Diff).", Fields: []types.Field{{Name: "On", Doc: "use learning rate modulation"}, {Name: "SigmoidLinear", Doc: "use a linear sigmoid function: if act > .5: 1-act; else act\notherwise use the actual sigmoid derivative which is squared: a(1-a)"}, {Name: "SigmoidMin", Doc: "minimum learning rate multiplier for sigmoidal act (1-act) factor,\nwhich prevents lrate from going too low for extreme values.\nSet to 1 to disable Sigmoid derivative factor, which is default for Target layers."}, {Name: "Diff", Doc: "modulate learning rate as a function of plus - minus differences"}, {Name: "SpkThr", Doc: "threshold on Max(CaSpkP, CaSpkD) below which Min lrate applies.\nmust be > 0 to prevent div by zero."}, {Name: "DiffThr", Doc: "threshold on recv neuron error delta, i.e., |CaSpkP - CaSpkD| below which lrate is at Min value"}, {Name: "Min", Doc: "for Diff component, minimum learning rate value when below ActDiffThr"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnNeurParams", IDName: "learn-neur-params", Doc: "axon.LearnNeurParams manages learning-related parameters at the neuron-level.\nThis is mainly the running average activations that drive learning", Fields: []types.Field{{Name: "CaLearn", Doc: "parameterizes the neuron-level calcium signals driving learning: CaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or use the more complex and dynamic VGCC channel directly.  CaLrn is then integrated in a cascading manner at multiple time scales: CaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase)."}, {Name: "CaSpk", Doc: "parameterizes the neuron-level spike-driven calcium signals, starting with CaSyn that is integrated at the neuron level, and drives synapse-level, pre * post Ca integration, which provides the Tr trace that multiplies error signals, and drives learning directly for Target layers. CaSpk* values are integrated separately at the Neuron level and used for UpdateThr and RLRate as a proxy for the activation (spiking) based learning signal."}, {Name: "LrnNMDA", Doc: "NMDA channel parameters used for learning, vs. the ones driving activation -- allows exploration of learning parameters independent of their effects on active maintenance contributions of NMDA, and may be supported by different receptor subtypes"}, {Name: "TrgAvgAct", Doc: "synaptic scaling parameters for regulating overall average activity compared to neuron's own target level"}, {Name: "RLRate", Doc: "recv neuron learning rate modulation params -- an additional error-based modulation of learning for receiver side: RLRate = |CaSpkP - CaSpkD| / Max(CaSpkP, CaSpkD)"}, {Name: "NeuroMod", Doc: "neuromodulation effects on learning rate and activity, as a function of layer-level DA and ACh values, which are updated from global Context values, and computed from reinforcement learning algorithms"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtInitParams", IDName: "s-wt-init-params", Doc: "SWtInitParams for initial SWt values", Fields: []types.Field{{Name: "SPct", Doc: "how much of the initial random weights are captured in the SWt values -- rest goes into the LWt values.  1 gives the strongest initial biasing effect, for larger models that need more structural support. 0.5 should work for most models where stronger constraints are not needed."}, {Name: "Mean", Doc: "target mean weight values across receiving neuron's pathway -- the mean SWt values are constrained to remain at this value.  some pathways may benefit from lower mean of .4"}, {Name: "Var", Doc: "initial variance in weight values, prior to constraints."}, {Name: "Sym", Doc: "symmetrize the initial weight values with those in reciprocal pathway -- typically true for bidirectional excitatory connections"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtAdaptParams", IDName: "s-wt-adapt-params", Doc: "SWtAdaptParams manages adaptation of SWt values", Fields: []types.Field{{Name: "On", Doc: "if true, adaptation is active -- if false, SWt values are not updated, in which case it is generally good to have Init.SPct=0 too."}, {Name: "LRate", Doc: "learning rate multiplier on the accumulated DWt values (which already have fast LRate applied) to incorporate into SWt during slow outer loop updating -- lower values impose stronger constraints, for larger networks that need more structural support, e.g., 0.001 is better after 1,000 epochs in large models.  0.1 is fine for smaller models."}, {Name: "SubMean", Doc: "amount of mean to subtract from SWt delta when updating -- generally best to set to 1"}, {Name: "SigGain", Doc: "gain of sigmoidal constrast enhancement function used to transform learned, linear LWt values into Wt values"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtParams", IDName: "s-wt-params", Doc: "SWtParams manages structural, slowly adapting weight values (SWt),\nin terms of initialization and updating over course of learning.\nSWts impose initial and slowly adapting constraints on neuron connectivity\nto encourage differentiation of neuron representations and overall good behavior\nin terms of not hogging the representational space.\nThe TrgAvg activity constraint is not enforced through SWt -- it needs to be\nmore dynamic and supported by the regular learned weights.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Init", Doc: "initialization of SWt values"}, {Name: "Adapt", Doc: "adaptation of SWt values in response to LWt learning"}, {Name: "Limit", Doc: "range limits for SWt values"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateParams", IDName: "l-rate-params", Doc: "LRateParams manages learning rate parameters", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "Base", Doc: "base learning rate for this pathway -- can be modulated by other factors below -- for larger networks, use slower rates such as 0.04, smaller networks can use faster 0.2."}, {Name: "Sched", Doc: "scheduled learning rate multiplier, simulating reduction in plasticity over aging"}, {Name: "Mod", Doc: "dynamic learning rate modulation due to neuromodulatory or other such factors"}, {Name: "Eff", Doc: "effective actual learning rate multiplier used in computing DWt: Eff = eMod * Sched * Base"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TraceParams", IDName: "trace-params", Doc: "TraceParams manages parameters associated with temporal trace learning", Fields: []types.Field{{Name: "Tau", Doc: "time constant for integrating trace over theta cycle timescales.\ngoverns the decay rate of syanptic trace"}, {Name: "SubMean", Doc: "amount of the mean dWt to subtract, producing a zero-sum effect -- 1.0 = full zero-sum dWt -- only on non-zero DWts.  typically set to 0 for standard trace learning pathways, although some require it for stability over the long haul.  can use SetSubMean to set to 1 after significant early learning has occurred with 0.  Some special path types (e.g., Hebb) benefit from SubMean = 1 always"}, {Name: "LearnThr", Doc: "threshold for learning, depending on different algorithms -- in Matrix and VSPatch it applies to normalized GeIntNorm value -- setting this relatively high encourages sparser representations"}, {Name: "Dt", Doc: "rate = 1 / tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateMod", IDName: "l-rate-mod", Doc: "LRateMod implements global learning rate modulation, based on a performance-based\nfactor, for example error.  Increasing levels of the factor = higher learning rate.\nThis can be added to a Sim and called prior to DWt() to dynamically change lrate\nbased on overall network performance.", Fields: []types.Field{{Name: "On", Doc: "toggle use of this modulation factor"}, {Name: "Base", Doc: "baseline learning rate -- what you get for correct cases"}, {Name: "pad"}, {Name: "pad1"}, {Name: "Range", Doc: "defines the range over which modulation occurs for the modulator factor -- Min and below get the Base level of learning rate modulation, Max and above get a modulation of 1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HebbParams", IDName: "hebb-params", Doc: "HebbParams for optional hebbian learning that replaces the\ndefault learning rule, based on S = sending activity,\nR = receiving activity", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "On", Doc: "if On, then the standard learning rule is replaced with a Hebbian learning rule"}, {Name: "Up", Doc: "strength multiplier for hebbian increases, based on R * S * (1-LWt)"}, {Name: "Down", Doc: "strength multiplier for hebbian decreases, based on R * (1 - S) * LWt"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnSynParams", IDName: "learn-syn-params", Doc: "LearnSynParams manages learning-related parameters at the synapse-level.", Fields: []types.Field{{Name: "Learn", Doc: "enable learning for this pathway"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "LRate", Doc: "learning rate parameters, supporting two levels of modulation on top of base learning rate."}, {Name: "Trace", Doc: "trace-based learning parameters"}, {Name: "KinaseCa", Doc: "kinase calcium Ca integration parameters: using linear regression parameters"}, {Name: "Hebb", Doc: "hebbian learning option, which overrides the default learning rules"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ViewTimes", IDName: "view-times", Doc: "ViewTimes are the options for when the NetView can be updated."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NetViewUpdate", IDName: "net-view-update", Doc: "NetViewUpdate manages time scales for updating the NetView.\nUse one of these for each mode you want to control separately.", Fields: []types.Field{{Name: "On", Doc: "toggles update of display on"}, {Name: "Time", Doc: "Time scale to update the network view."}, {Name: "View", Doc: "View is the network view."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NetworkIndexes", IDName: "network-indexes", Doc: "NetworkIndexes are indexes and sizes for processing network.", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "MaxData", Doc: "MaxData is the maximum number of data inputs that can be processed\nin parallel in one pass of the network.\nNeuron storage is allocated to hold this amount during\nBuild process, and this value reflects that."}, {Name: "MaxDelay", Doc: "MaxDelay is the maximum synaptic delay across all pathways at the time of\n[Network.Build]. This determines the size of the spike sending delay buffers."}, {Name: "NLayers", Doc: "NLayers is the number of layers in the network."}, {Name: "NNeurons", Doc: "NNeurons is the total number of neurons."}, {Name: "NPools", Doc: "NPools is the total number of pools."}, {Name: "NPaths", Doc: "NPaths is the total number of paths."}, {Name: "NSyns", Doc: "NSyns is the total number of synapses."}, {Name: "RubiconNPosUSs", Doc: "RubiconNPosUSs is the total number of Rubicon Drives / positive USs."}, {Name: "RubiconNCosts", Doc: "RubiconNCosts is the total number of Rubicon Costs."}, {Name: "RubiconNNegUSs", Doc: "RubiconNNegUSs is the total number of .Rubicon Negative USs."}, {Name: "GPUMaxBuffFloats", Doc: "GPUMaxBuffFloats is the maximum size in float32 (4 bytes) of a GPU buffer\nneeded for GPU access."}, {Name: "GPUSynCaBanks", Doc: "GPUSyncCaBanks is the total number of SynCa banks of GPUMaxBufferBytes arrays in GPU."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Network", IDName: "network", Doc: "Network implements the Axon spiking model.\nMost of the fields are copied to the global vars, needed for GPU,\nvia the SetAsCurrent method, and must be slices or tensors so that\nthere is one canonical underlying instance of all such data.\nThere are also Layer and Path lists that are used to scaffold the\nbuilding and display of the network, but contain no data.", Directives: []types.Directive{{Tool: "gosl", Directive: "end"}}, Methods: []types.Method{{Name: "InitWeights", Doc: "InitWeights initializes synaptic weights and all other associated long-term state variables\nincluding running-average state values (e.g., layer running average activations etc)", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- not automatically called", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "ShowAllGlobals", Doc: "ShowAllGlobals shows a listing of all Global variables and values.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "Build", Doc: "Build constructs the layer and pathway state based on the layer shapes\nand patterns of interconnectivity.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Returns: []string{"error"}}}, Embeds: []types.Field{{Name: "NetworkBase"}}, Fields: []types.Field{{Name: "Rubicon", Doc: "Rubicon system for goal-driven motivated behavior,\nincluding Rubicon phasic dopamine signaling.\nManages internal drives, US outcomes. Core LHb (lateral habenula)\nand VTA (ventral tegmental area) dopamine are computed\nin equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nRenders USLayer, PVLayer, DrivesLayer representations\nbased on state updated here."}, {Name: "Layers", Doc: "array of layers."}, {Name: "Paths", Doc: "pointers to all pathways in the network, sender-based."}, {Name: "NThreads", Doc: "number of threads to use for parallel processing."}, {Name: "RecFunTimes", Doc: "record function timer information."}, {Name: "FunTimes", Doc: "timers for each major function (step of processing)."}, {Name: "LayParams", Doc: "LayParams are all the layer parameters. [NLayers]"}, {Name: "PathParams", Doc: "PathParams are all the path parameters, in sending order. [NPaths]"}, {Name: "NetworkIxs", Doc: "NetworkIxs have indexes and sizes for entire network (one only)."}, {Name: "NeuronIxs", Doc: "NeuronIxs have index values for each neuron: index into layer, pools.\n[Indexes][Neurons]"}, {Name: "SynapseIxs", Doc: "SynapseIxs have index values for each synapse:\nproviding index into recv, send neurons, path.\n[Indexes][NSyns]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}, {Name: "PathSendCon", Doc: "PathSendCon are starting offset and N cons for each sending neuron,\nfor indexing into the Syns synapses, which are organized sender-based.\n[NSendCon][StartNN]; NSendCon = [Layer][SendPaths][SendNeurons]"}, {Name: "RecvPathIxs", Doc: "RecvPathIxs indexes into Paths (organized by SendPath) organized\nby recv pathways. needed for iterating through recv paths efficiently on GPU.\n[NRecvPaths] = [Layer][RecvPaths]"}, {Name: "PathRecvCon", Doc: "PathRecvCon are the receiving path starting index and number of connections.\n[NRecvCon][StartNN]; NRecvCon = [Layer][RecvPaths][RecvNeurons]"}, {Name: "RecvSynIxs", Doc: "RecvSynIxs are the indexes into Synapses for each recv neuron, organized\ninto blocks according to PathRecvCon, for receiver-based access.\n[NSyns] = [Layer][RecvPaths][RecvNeurons][Syns]"}, {Name: "Ctx", Doc: "Ctx is the context state (one). Other copies of Context can be maintained\nand [SetContext] to update this one, but this instance is the canonical one."}, {Name: "Neurons", Doc: "Neurons are all the neuron state variables.\n[Vars][Neurons][Data]"}, {Name: "NeuronAvgs", Doc: "NeuronAvgs are variables with averages over the\nData parallel dimension for each neuron.\n[Vars][Neurons]"}, {Name: "Pools", Doc: "Pools are the [PoolVars] float32 state values for layer and sub-pool inhibition,\nIncluding the float32 AvgMax values by Phase and variable: use [AvgMaxVarIndex].\n[PoolVars+AvgMax][Layer * Pools][Data]"}, {Name: "PoolsInt", Doc: "PoolsInt are the [PoolIntVars] int32 state values for layer and sub-pool\ninhibition, AvgMax atomic integration, and other vars: use [AvgMaxIntVarIndex]\n[PoolIntVars+AvgMax][Layer * Pools][Data]"}, {Name: "LayerStates", Doc: "LayerStates holds layer-level state values, with variables defined in\n[LayerVars], for each layer and Data parallel index.\n[LayerVarsN][Layer][Data]"}, {Name: "GlobalScalars", Doc: "GlobalScalars are the global scalar state variables.\n[GlobalScalarsN][Data]"}, {Name: "GlobalVectors", Doc: "GlobalVectors are the global vector state variables.\n[GlobalVectorsN][MaxGlobalVecN][Data]"}, {Name: "Exts", Doc: "Exts are external input values for all Input / Target / Compare layers\nin the network. The ApplyExt methods write to this per layer,\nand it is then actually applied in one consistent method.\n[NExts][Data]; NExts = [In / Out Layers][Neurons]"}, {Name: "PathGBuf", Doc: "PathGBuf is the conductance buffer for accumulating spikes.\nSubslices are allocated to each pathway.\nUses int-encoded values for faster GPU atomic integration.\n[MaxDel+1][NPathNeur][Data]; NPathNeur = [Layer][RecvPaths][RecvNeurons]"}, {Name: "PathGSyns", Doc: "PathGSyns are synaptic conductance integrated over time per pathway\nper recv neurons. spikes come in via PathBuf.\nsubslices are allocated to each pathway.\n[NPathNeur][Data]"}, {Name: "Synapses", Doc: "\tSynapses are the synapse level variables (weights etc).\n\nThese do not depend on the data parallel index, unlike [SynapseTraces].\n[Vars][NSyns]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}, {Name: "SynapseTraces", Doc: "SynapseTraces are synaptic variables that depend on the data\nparallel index, for accumulating learning traces and weight changes per data.\nThis is the largest data size, so multiple instances are used\nto handle larger networks.\n[Vars][NSyns][Data]; NSyns = [Layer][SendPaths][SendNeurons][Syns]"}, {Name: "SynapseTraces1", Doc: "SynapseTraces1 is an overflow buffer fro SynapseTraces."}, {Name: "SynapseTraces2", Doc: "SynapseTraces2 is an overflow buffer fro SynapseTraces."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DAModTypes", IDName: "da-mod-types", Doc: "DAModTypes are types of dopamine modulation of neural activity."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ValenceTypes", IDName: "valence-types", Doc: "ValenceTypes are types of valence coding: positive or negative."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuroModParams", IDName: "neuro-mod-params", Doc: "NeuroModParams specifies the effects of neuromodulators on neural\nactivity and learning rate.  These can apply to any neuron type,\nand are applied in the core cycle update equations.", Fields: []types.Field{{Name: "DAMod", Doc: "dopamine receptor-based effects of dopamine modulation on excitatory and inhibitory conductances: D1 is excitatory, D2 is inhibitory as a function of increasing dopamine"}, {Name: "Valence", Doc: "valence coding of this layer -- may affect specific layer types but does not directly affect neuromodulators currently"}, {Name: "DAModGain", Doc: "dopamine modulation of excitatory and inhibitory conductances (i.e., \"performance dopamine\" effect -- this does NOT affect learning dopamine modulation in terms of RLrate): g *= 1 + (DAModGain * DA)"}, {Name: "DALRateSign", Doc: "modulate the sign of the learning rate factor according to the DA sign, taking into account the DAMod sign reversal for D2Mod, also using BurstGain and DipGain to modulate DA value -- otherwise, only the magnitude of the learning rate is modulated as a function of raw DA magnitude according to DALRateMod (without additional gain factors)"}, {Name: "DALRateMod", Doc: "if not using DALRateSign, this is the proportion of maximum learning rate that Abs(DA) magnitude can modulate -- e.g., if 0.2, then DA = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChLRateMod", Doc: "proportion of maximum learning rate that ACh can modulate -- e.g., if 0.2, then ACh = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChDisInhib", Doc: "amount of extra Gi inhibition added in proportion to 1 - ACh level -- makes ACh disinhibitory"}, {Name: "BurstGain", Doc: "multiplicative gain factor applied to positive dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign!"}, {Name: "DipGain", Doc: "multiplicative gain factor applied to negative dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign! should be small for acq, but roughly equal to burst for ext"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronFlags", IDName: "neuron-flags", Doc: "NeuronFlags are bit-flags encoding relevant binary state for neurons"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronVars", IDName: "neuron-vars", Doc: "NeuronVars are the neuron variables representing current active state,\nspecific to each input data state.\nSee NeuronAvgVars for vars shared across data."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVars", IDName: "neuron-avg-vars", Doc: "NeuronAvgVars are mostly neuron variables involved in longer-term average activity\nwhich is aggregated over time and not specific to each input data state,\nalong with any other state that is not input data specific."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronIndexVars", IDName: "neuron-index-vars", Doc: "NeuronIndexVars are neuron-level indexes used to access layers and pools\nfrom the individual neuron level."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Path", IDName: "path", Doc: "Path implements axon spiking communication and learning.", Embeds: []types.Field{{Name: "PathBase"}}, Fields: []types.Field{{Name: "Params", Doc: "path parameters."}, {Name: "Send", Doc: "sending layer for this pathway."}, {Name: "Recv", Doc: "receiving layer for this pathway."}, {Name: "Type", Doc: "type of pathway."}, {Name: "DefaultParams", Doc: "default parameters that are applied prior to user-set parameters.\nthese are useful for specific functionality in specialized brain areas\n(e.g., Rubicon, BG etc) not associated with a path type, which otherwise\nis used to hard-code initial default parameters.\nTypically just set to a literal map."}, {Name: "RecvConNAvgMax", Doc: "average and maximum number of recv connections in the receiving layer"}, {Name: "SendConNAvgMax", Doc: "average and maximum number of sending connections in the sending layer"}, {Name: "SynStIndex", Doc: "start index into global Synapse array:"}, {Name: "NSyns", Doc: "number of synapses in this pathway"}, {Name: "RecvCon", Doc: "starting offset and N cons for each recv neuron, for indexing into the RecvSynIndex array of indexes into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathRecvCons slice for GPU usage."}, {Name: "RecvSynIndex", Doc: "index into Syns synaptic state for each sending unit and connection within that, for the sending pathway which does not own the synapses, and instead indexes into recv-ordered list"}, {Name: "RecvConIndex", Doc: "for each recv synapse, this is index of *sending* neuron  It is generally preferable to use the Synapse SendIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "SendCon", Doc: "starting offset and N cons for each sending neuron, for indexing into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathSendCons slice for GPU usage."}, {Name: "SendConIndex", Doc: "index of other neuron that receives the sender's synaptic input, ordered by the sending layer's order of units as the outer loop, and SendCon.N receiving units within that.  It is generally preferable to use the Synapse RecvIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.StartN", IDName: "start-n", Doc: "StartN holds a starting offset index and a number of items\narranged from Start to Start+N (exclusive).\nThis is not 16 byte padded and only for use on CPU side.", Fields: []types.Field{{Name: "Start", Doc: "starting offset"}, {Name: "N", Doc: "number of items --"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathIndexes", IDName: "path-indexes", Doc: "PathIndexes contains path-level index information into global memory arrays", Fields: []types.Field{{Name: "PathIndex", Doc: "PathIndex is the index of the pathway in global path list: [Layer][SendPaths]"}, {Name: "RecvLayer", Doc: "RecvLayer is the index of the receiving layer in global list of layers."}, {Name: "RecvNeurSt", Doc: "RecvNeurSt is the starting index of neurons in recv layer,\nso we don't need layer to get to neurons."}, {Name: "RecvNeurN", Doc: "RecvNeurN is the number of neurons in recv layer."}, {Name: "SendLayer", Doc: "SendLayer is the index of the sending layer in global list of layers."}, {Name: "SendNeurSt", Doc: "SendNeurSt is the starting index of neurons in sending layer,\nso we don't need layer to get to neurons."}, {Name: "SendNeurN", Doc: "SendNeurN is the number of neurons in send layer"}, {Name: "SynapseSt", Doc: "SynapseSt is the start index into global Synapse array.\n[Layer][SendPaths][Synapses]."}, {Name: "SendConSt", Doc: "SendConSt is the start index into global PathSendCon array.\n[Layer][SendPaths][SendNeurons]"}, {Name: "RecvConSt", Doc: "RecvConSt is the start index into global PathRecvCon array.\n[Layer][RecvPaths][RecvNeurons]"}, {Name: "RecvSynSt", Doc: "RecvSynSt is the start index into global sender-based Synapse index array.\n[Layer][SendPaths][Synapses]"}, {Name: "NPathNeurSt", Doc: "NPathNeurSt is the start NPathNeur index into PathGBuf, PathGSyns global arrays.\n[Layer][RecvPaths][RecvNeurons]"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GScaleValues", IDName: "g-scale-values", Doc: "GScaleValues holds the conductance scaling values.\nThese are computed once at start and remain constant thereafter,\nand therefore belong on Params and not on PathValues.", Fields: []types.Field{{Name: "Scale", Doc: "scaling factor for integrating synaptic input conductances (G's), originally computed as a function of sending layer activity and number of connections, and typically adapted from there -- see Path.PathScale adapt params"}, {Name: "Rel", Doc: "normalized relative proportion of total receiving conductance for this pathway: PathScale.Rel / sum(PathScale.Rel across relevant paths)"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathParams", IDName: "path-params", Doc: "PathParams contains all of the path parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []types.Field{{Name: "PathType", Doc: "functional type of path, which determines functional code path\nfor specialized layer types, and is synchronized with the Path.Type value"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "Indexes", Doc: "recv and send neuron-level pathway index array access info"}, {Name: "Com", Doc: "synaptic communication parameters: delay, probability of failure"}, {Name: "PathScale", Doc: "pathway scaling parameters for computing GScale:\nmodulates overall strength of pathway, using both\nabsolute and relative factors, with adaptation option to maintain target max conductances"}, {Name: "SWts", Doc: "slowly adapting, structural weight value parameters,\nwhich control initial weight values and slower outer-loop adjustments"}, {Name: "Learn", Doc: "synaptic-level learning parameters for learning in the fast LWt values."}, {Name: "GScale", Doc: "conductance scaling values"}, {Name: "RLPred", Doc: "Params for RWPath and TDPredPath for doing dopamine-modulated learning\nfor reward prediction: Da * Send activity.\nUse in RWPredLayer or TDPredLayer typically to generate reward predictions.\nIf the Da sign is positive, the first recv unit learns fully; for negative,\nsecond one learns fully.\nLower lrate applies for opposite cases.  Weights are positive-only."}, {Name: "Matrix", Doc: "for trace-based learning in the MatrixPath. A trace of synaptic co-activity\nis formed, and then modulated by dopamine whenever it occurs.\nThis bridges the temporal gap between gating activity and subsequent activity,\nand is based biologically on synaptic tags.\nTrace is reset at time of reward based on ACh level from CINs."}, {Name: "BLA", Doc: "Basolateral Amygdala pathway parameters."}, {Name: "Hip", Doc: "Hip bench parameters."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathTypes", IDName: "path-types", Doc: "PathTypes enumerates all the different types of axon pathways,\nfor the different algorithm types supported.\nClass parameter styles automatically key off of these types."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.MatrixParams", IDName: "matrix-params", Doc: "MatrixParams has parameters for BG Striatum Matrix MSN layers\nThese are the main Go / NoGo gating units in BG.\nDA, ACh learning rate modulation is pre-computed on the recv neuron\nRLRate variable via NeuroMod.  Also uses Pool.Gated for InvertNoGate,\nupdated in PlusPhase prior to DWt call.\nMust set Learn.NeuroMod.DAMod = D1Mod or D2Mod via SetBuildConfig(\"DAMod\").", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "GateThr", Doc: "threshold on layer Avg SpkMax for Matrix Go and VThal layers to count as having gated"}, {Name: "IsVS", Doc: "is this a ventral striatum (VS) matrix layer?  if true, the gating status of this layer is recorded in the Global state, and used for updating effort and other factors."}, {Name: "OtherMatrixIndex", Doc: "index of other matrix (Go if we are NoGo and vice-versa).    Set during Build from BuildConfig OtherMatrixName"}, {Name: "ThalLay1Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay2Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay2Name if present -- -1 if not used"}, {Name: "ThalLay3Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay3Name if present -- -1 if not used"}, {Name: "ThalLay4Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay4Name if present -- -1 if not used"}, {Name: "ThalLay5Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay5Name if present -- -1 if not used"}, {Name: "ThalLay6Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay6Name if present -- -1 if not used"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPLayerTypes", IDName: "gp-layer-types", Doc: "GPLayerTypes is a GPLayer axon-specific layer type enum."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPParams", IDName: "gp-params", Doc: "GPLayer represents a globus pallidus layer, including:\nGPePr, GPeAk (arkypallidal), and GPi (see GPType for type).\nTypically just a single unit per Pool representing a given stripe.", Fields: []types.Field{{Name: "GPType", Doc: "type of GP Layer -- must set during config using SetBuildConfig of GPType."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.MatrixPathParams", IDName: "matrix-path-params", Doc: "MatrixPathParams for trace-based learning in the MatrixPath.\nA trace of synaptic co-activity is formed, and then modulated by dopamine\nwhenever it occurs.  This bridges the temporal gap between gating activity\nand subsequent activity, and is based biologically on synaptic tags.\nTrace is applied to DWt and reset at the time of reward.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pcore_paths"}}}, Fields: []types.Field{{Name: "Credit", Doc: "proportion of trace activity driven by the basic credit assignment factor\nbased on the PF modulatory inputs and activity of the receiving neuron,\nrelative to the delta factor which is generally going to be smaller\nbecause it is an activity difference."}, {Name: "BasePF", Doc: "baseline amount of PF activity that modulates credit assignment learning,\nfor neurons with zero PF modulatory activity.\nThese were not part of the actual motor action, but can still get some\nsmaller amount of credit learning."}, {Name: "Delta", Doc: "weight for trace activity that is a function of the minus-plus delta\nactivity signal on the receiving MSN neuron, independent of PF modulation.\nThis should always be 1 except for testing disabling: adjust NonDelta\nrelative to it, and the overall learning rate."}, {Name: "VSRewLearn", Doc: "for ventral striatum, learn based on activity at time of reward,\nin inverse proportion to the GoalMaint activity: i.e., if there was no\ngoal maintenance, learn at reward to encourage goal engagement next time,\nbut otherwise, do not further reinforce at time of reward, because the\nactual goal gating learning trace is a better learning signal.\nOtherwise, only uses accumulated trace but doesn't include rew-time activity,\ne.g., for testing cases that do not have GoalMaint."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PoolIntVars", IDName: "pool-int-vars", Doc: "PoolIntVars are int32 pool variables, for computing fsfffb inhibition etc.\nNote that we use int32 instead of uint32 so that overflow errors can be detected.\nSee [PoolVars] for float32 variables."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMax", IDName: "avg-max", Doc: "AvgMax are Avg and Max"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxPhases", IDName: "avg-max-phases", Doc: "AvgMaxPhases are the different Phases over which AvgMax values are tracked."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxVars", IDName: "avg-max-vars", Doc: "AvgMaxVars are the different Neuron variables for which [AvgMaxPhases]\nis computed."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RandFunIndex", IDName: "rand-fun-index", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWPredParams", IDName: "rw-pred-params", Doc: "RWPredParams parameterizes reward prediction for a simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Directives: []types.Directive{{Tool: "gosl", Directive: "start"}}, Fields: []types.Field{{Name: "PredRange", Doc: "default 0.1..0.99 range of predictions that can be represented -- having a truncated range preserves some sensitivity in dopamine at the extremes of good or poor performance"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWDaParams", IDName: "rw-da-params", Doc: "RWDaParams computes a dopamine (DA) signal using simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "RWPredLayIndex", Doc: "idx of RWPredLayer to get reward prediction from -- set during Build from BuildConfig RWPredLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDIntegParams", IDName: "td-integ-params", Doc: "TDIntegParams are params for reward integrator layer", Fields: []types.Field{{Name: "Discount", Doc: "discount factor -- how much to discount the future prediction from TDPred"}, {Name: "PredGain", Doc: "gain factor on TD rew pred activations"}, {Name: "TDPredLayIndex", Doc: "idx of TDPredLayer to get reward prediction from -- set during Build from BuildConfig TDPredLayName"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDDaParams", IDName: "td-da-params", Doc: "TDDaParams are params for dopamine (DA) signal as the temporal difference (TD)\nbetween the TDIntegLayer activations in the minus and plus phase.", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "TDIntegLayIndex", Doc: "idx of TDIntegLayer to get reward prediction from -- set during Build from BuildConfig TDIntegLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLPredPathParams", IDName: "rl-pred-path-params", Doc: "RLPredPathParams does dopamine-modulated learning for reward prediction: Da * Send.Act\nUsed by RWPath and TDPredPath within corresponding RWPredLayer or TDPredLayer\nto generate reward predictions based on its incoming weights, using linear activation\nfunction. Has no weight bounds or limits on sign etc.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rl_paths"}}}, Fields: []types.Field{{Name: "OppSignLRate", Doc: "how much to learn on opposite DA sign coding neuron (0..1)"}, {Name: "DaTol", Doc: "tolerance on DA -- if below this abs value, then DA goes to zero and there is no learning -- prevents prediction from exactly learning to cancel out reward value, retaining a residual valence of signal"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LDTParams", IDName: "ldt-params", Doc: "LDTParams compute reward salience as ACh global neuromodulatory signal\nas a function of the MAX activation of its inputs from salience detecting\nlayers (e.g., the superior colliculus: SC), and whenever there is an external\nUS outcome input (signalled by the global GvHasRew flag).\nACh from salience inputs is discounted by GoalMaint activity,\nreducing distraction when pursuing a goal, but US ACh activity is not so reduced.\nACh modulates excitability of goal-gating layers.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rubicon_layers"}}}, Fields: []types.Field{{Name: "SrcThr", Doc: "threshold per input source, on absolute value (magnitude), to count as a significant reward event, which then drives maximal ACh -- set to 0 to disable this nonlinear behavior"}, {Name: "Rew", Doc: "use the global Context.NeuroMod.HasRew flag -- if there is some kind of external reward being given, then ACh goes to 1, else 0 for this component"}, {Name: "MaintInhib", Doc: "extent to which active goal maintenance (via Global GoalMaint)\ninhibits ACh signals: when goal engaged, distractability is lower."}, {Name: "SrcLay1Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay1Name if present -- -1 if not used"}, {Name: "SrcLay2Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay2Name if present -- -1 if not used"}, {Name: "SrcLay3Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay3Name if present -- -1 if not used"}, {Name: "SrcLay4Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay4Name if present -- -1 if not used"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.VTAParams", IDName: "vta-params", Doc: "VTAParams are for computing overall VTA DA based on LHb PVDA\n(primary value -- at US time, computed at start of each trial\nand stored in LHbPVDA global value)\nand Amygdala (CeM) CS / learned value (LV) activations, which update\nevery cycle.", Fields: []types.Field{{Name: "CeMGain", Doc: "gain on CeM activity difference (CeMPos - CeMNeg) for generating LV CS-driven dopamine values"}, {Name: "LHbGain", Doc: "gain on computed LHb DA (Burst - Dip) -- for controlling DA levels"}, {Name: "AChThr", Doc: "threshold on ACh level required to generate LV CS-driven dopamine burst"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLAPathParams", IDName: "bla-path-params", Doc: "BLAPathParams has parameters for basolateral amygdala learning.\nLearning is driven by the Tr trace as function of ACh * Send Act\nrecorded prior to US, and at US, recv unit delta: CaSpkP - SpkPrv\ntimes normalized GeIntNorm for recv unit credit assignment.\nThe Learn.Trace.Tau time constant determines trace updating over trials\nwhen ACh is above threshold -- this determines strength of second-order\nconditioning -- default of 1 means none, but can be increased as needed.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rubicon_paths"}}}, Fields: []types.Field{{Name: "NegDeltaLRate", Doc: "use 0.01 for acquisition (don't unlearn) and 1 for extinction -- negative delta learning rate multiplier"}, {Name: "AChThr", Doc: "threshold on this layer's ACh level for trace learning updates"}, {Name: "USTrace", Doc: "proportion of US time stimulus activity to use for the trace component of"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DriveParams", IDName: "drive-params", Doc: "DriveParams manages the drive parameters for computing and updating drive state.\nMost of the params are for optional case where drives are automatically\nupdated based on US consumption (which satisfies drives) and time passing\n(which increases drives).", Fields: []types.Field{{Name: "DriveMin", Doc: "minimum effective drive value, which is an automatic baseline ensuring\nthat a positive US results in at least some minimal level of reward.\nUnlike Base values, this is not reflected in the activity of the drive\nvalues, and applies at the time of reward calculation as a minimum baseline."}, {Name: "Base", Doc: "baseline levels for each drive, which is what they naturally trend toward\nin the absence of any input.  Set inactive drives to 0 baseline,\nactive ones typically elevated baseline (0-1 range)."}, {Name: "Tau", Doc: "time constants in ThetaCycle (trial) units for natural update toward\nBase values. 0 values means no natural update (can be updated externally)."}, {Name: "Satisfaction", Doc: "decrement in drive value when US is consumed, thus partially satisfying\nthe drive. Positive values are subtracted from current Drive value."}, {Name: "Dt", Doc: "1/Tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.UrgencyParams", IDName: "urgency-params", Doc: "UrgencyParams has urgency (increasing pressure to do something)\nand parameters for updating it.\nRaw urgency integrates effort when _not_ goal engaged\nwhile effort (negative US 0) integrates when a goal _is_ engaged.", Fields: []types.Field{{Name: "U50", Doc: "value of raw urgency where the urgency activation level is 50%"}, {Name: "Power", Doc: "exponent on the urge factor -- valid numbers are 1,2,4,6"}, {Name: "Thr", Doc: "threshold for urge -- cuts off small baseline values"}, {Name: "DAtonic", Doc: "gain factor for driving tonic DA levels as a function of urgency"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.USParams", IDName: "us-params", Doc: "USParams control how positive and negative USs and Costs are\nweighted and integrated to compute an overall PV primary value.", Fields: []types.Field{{Name: "PVposGain", Doc: "gain factor applied to sum of weighted, drive-scaled positive USs\nto compute PVpos primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust the overall scaling of PVpos reward within 0-1\nnormalized range (see also PVnegGain).\nEach USpos is assumed to be in 0-1 range, with a default of 1."}, {Name: "PVnegGain", Doc: "gain factor applied to sum of weighted negative USs and Costs\nto compute PVneg primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust overall scaling of PVneg within 0-1\nnormalized range (see also PVposGain)."}, {Name: "USnegGains", Doc: "Negative US gain factor for encoding each individual negative US,\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the USneg pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see PVnegWts), and generally must be larger than the weights to leverage\nthe dynamic range within each US pool."}, {Name: "CostGains", Doc: "Cost gain factor for encoding the individual Time, Effort etc costs\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the Cost pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see CostWts), and generally must be larger than the weights to use\nthe full dynamic range within each US pool."}, {Name: "PVposWts", Doc: "weight factor applied to each separate positive US on the way to computing\nthe overall PVpos summary value, to control the weighting of each US\nrelative to the others. Each pos US is also multiplied by its dynamic\nDrive factor as well.\nUse PVposGain to control the overall scaling of the PVpos value."}, {Name: "PVnegWts", Doc: "weight factor applied to each separate negative US on the way to computing\nthe overall PVneg summary value, to control the weighting of each US\nrelative to the others, and to the Costs.  These default to 1."}, {Name: "PVcostWts", Doc: "weight factor applied to each separate Cost (Time, Effort, etc) on the\nway to computing the overall PVneg summary value, to control the weighting\nof each Cost relative to the others, and relative to the negative USs.\nThe first pool is Time, second is Effort, and these are typically weighted\nlower (.02) than salient simulation-specific USs (1)."}, {Name: "USposEst", Doc: "computed estimated US values, based on OFCposPT and VSMatrix gating, in PVposEst"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LHbParams", IDName: "l-hb-params", Doc: "LHbParams has values for computing LHb & RMTg which drives dips / pauses in DA firing.\nLHb handles all US-related (PV = primary value) processing.\nPositive net LHb activity drives dips / pauses in VTA DA activity,\ne.g., when predicted pos > actual or actual neg > predicted.\nNegative net LHb activity drives bursts in VTA DA activity,\ne.g., when actual pos > predicted (redundant with LV / Amygdala)\nor \"relief\" burst when actual neg < predicted.", Fields: []types.Field{{Name: "VSPatchNonRewThr", Doc: "threshold on VSPatch prediction during a non-reward trial"}, {Name: "VSPatchGain", Doc: "gain on the VSPatchD1 - D2 difference to drive the net VSPatch DA\nprediction signal, which goes in VSPatchPos and RewPred global variables"}, {Name: "VSPatchVarTau", Doc: "decay time constant for computing the temporal variance in VSPatch\nvalues over time"}, {Name: "NegThr", Doc: "threshold factor that multiplies integrated pvNeg value\nto establish a threshold for whether the integrated pvPos value\nis good enough to drive overall net positive reward.\nIf pvPos wins, it is then multiplicatively discounted by pvNeg;\notherwise, pvNeg is discounted by pvPos."}, {Name: "BurstGain", Doc: "gain multiplier on PVpos for purposes of generating bursts\n(not for discounting negative dips)."}, {Name: "DipGain", Doc: "gain multiplier on PVneg for purposes of generating dips\n(not for discounting positive bursts)."}, {Name: "VSPatchVarDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GiveUpParams", IDName: "give-up-params", Doc: "GiveUpParams are parameters for computing when to give up,\nbased on Utility, Timing and Progress factors.", Fields: []types.Field{{Name: "ProbThr", Doc: "threshold on GiveUp probability, below which no give up is triggered"}, {Name: "MinGiveUpSum", Doc: "minimum GiveUpSum value, which is the denominator in the sigmoidal function.\nThis minimum prevents division by zero and any other degenerate values."}, {Name: "Utility", Doc: "the factor multiplying utility values: cost and expected positive outcome"}, {Name: "Timing", Doc: "the factor multiplying timing values from VSPatch"}, {Name: "Progress", Doc: "the factor multiplying progress values based on time-integrated progress\ntoward the goal"}, {Name: "MinUtility", Doc: "minimum utility cost and reward estimate values -- when they are below\nthese levels (at the start) then utility is effectively neutral,\nso the other factors take precedence."}, {Name: "VSPatchSumMax", Doc: "maximum VSPatchPosSum for normalizing the value for give-up weighing"}, {Name: "VSPatchVarMax", Doc: "maximum VSPatchPosVar for normalizing the value for give-up weighing"}, {Name: "ProgressRateTau", Doc: "time constant for integrating the ProgressRate\nvalues over time"}, {Name: "ProgressRateDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Rubicon", IDName: "rubicon", Doc: "Rubicon implements core elements of the Rubicon goal-directed motivational\nmodel, representing the core brainstem-level (hypothalamus) bodily drives\nand resulting dopamine from US (unconditioned stimulus) inputs,\nsubsuming the earlier Rubicon model of primary value (PV)\nand learned value (LV), describing the functions of the Amygala,\nVentral Striatum, VTA and associated midbrain nuclei (LDT, LHb, RMTg).\nCore LHb (lateral habenula) and VTA (ventral tegmental area) dopamine\nare computed in equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nThe Drives, Effort, US and resulting LHb PV dopamine computation all happens at the\nat the start of each trial (NewState, Step).  The LV / CS dopamine is computed\ncycle-by-cycle by the VTA layer using parameters set by the VTA layer.\nRenders USLayer, PVLayer, DrivesLayer representations based on state updated here.", Fields: []types.Field{{Name: "NPosUSs", Doc: "number of possible positive US states and corresponding drives.\nThe first is always reserved for novelty / curiosity.\nMust be set programmatically via SetNUSs method,\nwhich allocates corresponding parameters."}, {Name: "NNegUSs", Doc: "number of possible phasic negative US states (e.g., shock, impact etc).\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "NCosts", Doc: "number of possible costs, typically including accumulated time and effort costs.\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "Drive", Doc: "parameters and state for built-in drives that form the core motivations\nof the agent, controlled by lateral hypothalamus and associated\nbody state monitoring such as glucose levels and thirst."}, {Name: "Urgency", Doc: "urgency (increasing pressure to do something) and parameters for\n\n\tupdating it. Raw urgency is incremented by same units as effort,\n\nbut is only reset with a positive US."}, {Name: "USs", Doc: "controls how positive and negative USs are weighted and integrated to\ncompute an overall PV primary value."}, {Name: "LHb", Doc: "lateral habenula (LHb) parameters and state, which drives\ndipping / pausing in dopamine when the predicted positive\noutcome > actual, or actual negative outcome > predicted.\nCan also drive bursting for the converse, and via matrix phasic firing."}, {Name: "GiveUp", Doc: "parameters for giving up based on PV pos - neg difference"}, {Name: "ValDecode", Doc: "population code decoding parameters for estimates from layers"}, {Name: "decodeActs"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseVars", IDName: "synapse-vars", Doc: "SynapseVars are the synapse variables representing synaptic weights, etc.\nThese do not depend on the data parallel index (di).\nSee [SynapseTraceVars] for variables that do depend on di."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseTraceVars", IDName: "synapse-trace-vars", Doc: "SynapseTraceVars are synaptic variables that depend on the data\nparallel index, for accumulating learning traces and weight changes per data."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseIndexVars", IDName: "synapse-index-vars", Doc: "SynapseIndexVars are synapse-level indexes used to access neurons and paths\nfrom the individual synapse level of processing."})
