// Code generated by "core generate -add-types"; DO NOT EDIT.

package axon

import (
	"cogentcore.org/core/types"
)

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeParams", IDName: "spike-params", Doc: "SpikeParams contains spiking activation function params.\nImplements a basic thresholded Vm model, and optionally\nthe AdEx adaptive exponential function (adapt is KNaAdapt)", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"act"}}, {Tool: "gosl", Directive: "end", Args: []string{"act"}}, {Tool: "gosl", Directive: "start", Args: []string{"act"}}}, Fields: []types.Field{{Name: "Thr", Doc: "threshold value Theta (Q) for firing output activation (.5 is more accurate value based on AdEx biological parameters and normalization"}, {Name: "VmR", Doc: "post-spiking membrane potential to reset to, produces refractory effect if lower than VmInit -- 0.3 is appropriate biologically based value for AdEx (Brette & Gurstner, 2005) parameters.  See also RTau"}, {Name: "Tr", Doc: "post-spiking explicit refractory period, in cycles -- prevents Vm updating for this number of cycles post firing -- Vm is reduced in exponential steps over this period according to RTau, being fixed at Tr to VmR exactly"}, {Name: "RTau", Doc: "time constant for decaying Vm down to VmR -- at end of Tr it is set to VmR exactly -- this provides a more realistic shape of the post-spiking Vm which is only relevant for more realistic channels that key off of Vm -- does not otherwise affect standard computation"}, {Name: "Exp", Doc: "if true, turn on exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpSlope", Doc: "slope in Vm (2 mV = .02 in normalized units) for extra exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpThr", Doc: "membrane potential threshold for actually triggering a spike when using the exponential mechanism"}, {Name: "MaxHz", Doc: "for translating spiking interval (rate) into rate-code activation equivalent, what is the maximum firing rate associated with a maximum activation value of 1"}, {Name: "ISITau", Doc: "constant for integrating the spiking interval in estimating spiking rate"}, {Name: "ISIDt", Doc: "rate = 1 / tau"}, {Name: "RDt", Doc: "rate = 1 / tau"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DendParams", IDName: "dend-params", Doc: "DendParams are the parameters for updating dendrite-specific dynamics", Fields: []types.Field{{Name: "GbarExp", Doc: "dendrite-specific strength multiplier of the exponential spiking drive on Vm -- e.g., .5 makes it half as strong as at the soma (which uses Gbar.L as a strength multiplier per the AdEx standard model)"}, {Name: "GbarR", Doc: "dendrite-specific conductance of Kdr delayed rectifier currents, used to reset membrane potential for dendrite -- applied for Tr msec"}, {Name: "SSGi", Doc: "SST+ somatostatin positive slow spiking inhibition level specifically affecting dendritic Vm (VmDend) -- this is important for countering a positive feedback loop from NMDA getting stronger over the course of learning -- also typically requires SubMean = 1 for TrgAvgAct and learning to fully counter this feedback loop."}, {Name: "HasMod", Doc: "set automatically based on whether this layer has any recv pathways that have a GType conductance type of Modulatory -- if so, then multiply GeSyn etc by GModSyn"}, {Name: "ModGain", Doc: "multiplicative gain factor on the total modulatory input -- this can also be controlled by the PathScale.Abs factor on ModulatoryG inputs, but it is convenient to be able to control on the layer as well."}, {Name: "ModACh", Doc: "if true, modulatory signal also includes ACh multiplicative factor"}, {Name: "ModBase", Doc: "baseline modulatory level for modulatory effects -- net modulation is ModBase + ModGain * GModSyn"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActInitParams", IDName: "act-init-params", Doc: "ActInitParams are initial values for key network state variables.\nInitialized in InitActs called by InitWeights, and provides target values for DecayState.", Fields: []types.Field{{Name: "Vm", Doc: "initial membrane potential -- see Erev.L for the resting potential (typically .3)"}, {Name: "Act", Doc: "initial activation value -- typically 0"}, {Name: "GeBase", Doc: "baseline level of excitatory conductance (net input) -- Ge is initialized to this value, and it is added in as a constant background level of excitatory input -- captures all the other inputs not represented in the model, and intrinsic excitability, etc"}, {Name: "GiBase", Doc: "baseline level of inhibitory conductance (net input) -- Gi is initialized to this value, and it is added in as a constant background level of inhibitory input -- captures all the other inputs not represented in the model"}, {Name: "GeVar", Doc: "variance (sigma) of gaussian distribution around baseline Ge values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "GiVar", Doc: "variance (sigma) of gaussian distribution around baseline Gi values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DecayParams", IDName: "decay-params", Doc: "DecayParams control the decay of activation state in the DecayState function\ncalled in NewState when a new state is to be processed.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"act"}}}, Fields: []types.Field{{Name: "Act", Doc: "proportion to decay most activation state variables toward initial values at start of every ThetaCycle (except those controlled separately below) -- if 1 it is effectively equivalent to full clear, resetting other derived values.  ISI is reset every AlphaCycle to get a fresh sample of activations (doesn't affect direct computation -- only readout)."}, {Name: "Glong", Doc: "proportion to decay long-lasting conductances, NMDA and GABA, and also the dendritic membrane potential -- when using random stimulus order, it is important to decay this significantly to allow a fresh start -- but set Act to 0 to enable ongoing activity to keep neurons in their sensitive regime."}, {Name: "AHP", Doc: "decay of afterhyperpolarization currents, including mAHP, sAHP, and KNa, Kir -- has a separate decay because often useful to have this not decay at all even if decay is on."}, {Name: "LearnCa", Doc: "decay of Ca variables driven by spiking activity used in learning: CaSpk* and Ca* variables. These are typically not decayed but may need to be in some situations."}, {Name: "OnRew", Doc: "decay layer at end of ThetaCycle when there is a global reward -- true by default for PTPred, PTMaint and PFC Super layers"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DtParams", IDName: "dt-params", Doc: "DtParams are time and rate constants for temporal derivatives in Axon (Vm, G)", Fields: []types.Field{{Name: "Integ", Doc: "overall rate constant for numerical integration, for all equations at the unit level -- all time constants are specified in millisecond units, with one cycle = 1 msec -- if you instead want to make one cycle = 2 msec, you can do this globally by setting this integ value to 2 (etc).  However, stability issues will likely arise if you go too high.  For improved numerical stability, you may even need to reduce this value to 0.5 or possibly even lower (typically however this is not necessary).  MUST also coordinate this with network.time_inc variable to ensure that global network.time reflects simulated time accurately"}, {Name: "VmTau", Doc: "membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmDendTau", Doc: "dendritic membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmSteps", Doc: "number of integration steps to take in computing new Vm value -- this is the one computation that can be most numerically unstable so taking multiple steps with proportionally smaller dt is beneficial"}, {Name: "GeTau", Doc: "time constant for decay of excitatory AMPA receptor conductance."}, {Name: "GiTau", Doc: "time constant for decay of inhibitory GABAa receptor conductance."}, {Name: "IntTau", Doc: "time constant for integrating values over timescale of an individual input state (e.g., roughly 200 msec -- theta cycle), used in computing ActInt, GeInt from Ge, and GiInt from GiSyn -- this is used for scoring performance, not for learning, in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life),"}, {Name: "LongAvgTau", Doc: "time constant for integrating slower long-time-scale averages, such as nrn.ActAvg, Pool.ActsMAvg, ActsPAvg -- computed in NewState when a new input state is present (i.e., not msec but in units of a theta cycle) (tau is roughly how long it takes for value to change significantly) -- set lower for smaller models"}, {Name: "MaxCycStart", Doc: "cycle to start updating the SpkMaxCa, SpkMax values within a theta cycle -- early cycles often reflect prior state"}, {Name: "VmDt", Doc: "nominal rate = Integ / tau"}, {Name: "VmDendDt", Doc: "nominal rate = Integ / tau"}, {Name: "DtStep", Doc: "1 / VmSteps"}, {Name: "GeDt", Doc: "rate = Integ / tau"}, {Name: "GiDt", Doc: "rate = Integ / tau"}, {Name: "IntDt", Doc: "rate = Integ / tau"}, {Name: "LongAvgDt", Doc: "rate = 1 / tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SpikeNoiseParams", IDName: "spike-noise-params", Doc: "SpikeNoiseParams parameterizes background spiking activity impinging on the neuron,\nsimulated using a poisson spiking process.", Fields: []types.Field{{Name: "On", Doc: "add noise simulating background spiking levels"}, {Name: "GeHz", Doc: "mean frequency of excitatory spikes -- typically 50Hz but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Ge", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "GiHz", Doc: "mean frequency of inhibitory spikes -- typically 100Hz fast spiking but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Gi", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "MaintGe", Doc: "add Ge noise to GeMaintRaw instead of standard Ge -- used for PTMaintLayer for example"}, {Name: "GeExpInt", Doc: "Exp(-Interval) which is the threshold for GeNoiseP as it is updated"}, {Name: "GiExpInt", Doc: "Exp(-Interval) which is the threshold for GiNoiseP as it is updated"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ClampParams", IDName: "clamp-params", Doc: "ClampParams specify how external inputs drive excitatory conductances\n(like a current clamp) -- either adds or overwrites existing conductances.\nNoise is added in either case.", Fields: []types.Field{{Name: "IsInput", Doc: "is this a clamped input layer?  set automatically based on layer type at initialization"}, {Name: "IsTarget", Doc: "is this a target layer?  set automatically based on layer type at initialization"}, {Name: "Ge", Doc: "amount of Ge driven for clamping -- generally use 0.8 for Target layers, 1.5 for Input layers"}, {Name: "Add", Doc: "add external conductance on top of any existing -- generally this is not a good idea for target layers (creates a main effect that learning can never match), but may be ok for input layers"}, {Name: "ErrThr", Doc: "threshold on neuron Act activity to count as active for computing error relative to target in PctErr method"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SMaintParams", IDName: "s-maint-params", Doc: "SMaintParams for self-maintenance simulating a population of\nNMDA-interconnected spiking neurons", Fields: []types.Field{{Name: "On", Doc: "is self maintenance active?"}, {Name: "NNeurons", Doc: "number of neurons within the self-maintenance pool,\neach of which is assumed to have the same probability of spiking"}, {Name: "Gbar", Doc: "conductance multiplier for self maintenance synapses"}, {Name: "Inhib", Doc: "inhib controls how much of the extra maintenance conductance goes to the GeExt, which drives extra proportional inhibition"}, {Name: "ISI", Doc: "ISI (inter spike interval) range -- min is used as min ISIAvg for poisson spike rate expected from the population, and above max, no additional maintenance conductance is added"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PopCodeParams", IDName: "pop-code-params", Doc: "PopCodeParams provides an encoding of scalar value using population code,\nwhere a single continuous (scalar) value is encoded as a gaussian bump\nacross a population of neurons (1 dimensional).\nIt can also modulate rate code and number of neurons active according to the value.\nThis is for layers that represent values as in the Rubicon system (from Context.Rubicon).\nBoth normalized activation values (1 max) and Ge conductance values can be generated.", Fields: []types.Field{{Name: "On", Doc: "use popcode encoding of variable(s) that this layer represents"}, {Name: "Ge", Doc: "Ge multiplier for driving excitatory conductance based on PopCode -- multiplies normalized activation values"}, {Name: "Min", Doc: "minimum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "Max", Doc: "maximum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "MinAct", Doc: "activation multiplier for values at Min end of range, where values at Max end have an activation of 1 -- if this is &lt; 1, then there is a rate code proportional to the value in addition to the popcode pattern -- see also MinSigma, MaxSigma"}, {Name: "MinSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "MaxSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "Clip", Doc: "ensure that encoded and decoded value remains within specified range"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActParams", IDName: "act-params", Doc: "axon.ActParams contains all the activation computation params and functions\nfor basic Axon, at the neuron level .\nThis is included in axon.Layer to drive the computation.", Fields: []types.Field{{Name: "Spikes", Doc: "Spiking function parameters"}, {Name: "Dend", Doc: "dendrite-specific parameters"}, {Name: "Init", Doc: "initial values for key network state variables -- initialized in InitActs called by InitWeights, and provides target values for DecayState"}, {Name: "Decay", Doc: "amount to decay between AlphaCycles, simulating passage of time and effects of saccades etc, especially important for environments with random temporal structure (e.g., most standard neural net training corpora)"}, {Name: "Dt", Doc: "time and rate constants for temporal derivatives / updating of activation state"}, {Name: "Gbar", Doc: "maximal conductances levels for channels"}, {Name: "Erev", Doc: "reversal potentials for each channel"}, {Name: "Clamp", Doc: "how external inputs drive neural activations"}, {Name: "Noise", Doc: "how, where, when, and how much noise to add"}, {Name: "VmRange", Doc: "range for Vm membrane potential -- -- important to keep just at extreme range of reversal potentials to prevent numerical instability"}, {Name: "Mahp", Doc: "M-type medium time-scale afterhyperpolarization mAHP current -- this is the primary form of adaptation on the time scale of multiple sequences of spikes"}, {Name: "Sahp", Doc: "slow time-scale afterhyperpolarization sAHP current -- integrates CaSpkD at theta cycle intervals and produces a hard cutoff on sustained activity for any neuron"}, {Name: "KNa", Doc: "sodium-gated potassium channel adaptation parameters -- activates a leak-like current as a function of neural activity (firing = Na influx) at two different time-scales (Slick = medium, Slack = slow)"}, {Name: "Kir", Doc: "potassium (K) inwardly rectifying (ir) current, which is similar to GABAB\n(which is a GABA modulated Kir channel).  This channel is off by default\nbut plays a critical role in making medium spiny neurons (MSNs) relatively\nquiet in the striatum."}, {Name: "NMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "MaintNMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "GabaB", Doc: "GABA-B / GIRK channel parameters"}, {Name: "VGCC", Doc: "voltage gated calcium channels -- provide a key additional source of Ca for learning and positive-feedback loop upstate for active neurons"}, {Name: "AK", Doc: "A-type potassium (K) channel that is particularly important for limiting the runaway excitation from VGCC channels"}, {Name: "SKCa", Doc: "small-conductance calcium-activated potassium channel produces the pausing function as a consequence of rapid bursting."}, {Name: "SMaint", Doc: "for self-maintenance simulating a population of\nNMDA-interconnected spiking neurons"}, {Name: "PopCode", Doc: "provides encoding population codes, used to represent a single continuous (scalar) value, across a population of units / neurons (1 dimensional)"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathGTypes", IDName: "path-g-types", Doc: "PathGTypes represents the conductance (G) effects of a given pathway,\nincluding excitatory, inhibitory, and modulatory."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynComParams", IDName: "syn-com-params", Doc: "SynComParams are synaptic communication parameters:\nused in the Path parameters.  Includes delay and\nprobability of failure, and Inhib for inhibitory connections,\nand modulatory pathways that have multiplicative-like effects.", Fields: []types.Field{{Name: "GType", Doc: "type of conductance (G) communicated by this pathway"}, {Name: "Delay", Doc: "additional synaptic delay in msec for inputs arriving at this pathway.  Must be <= MaxDelay which is set during network building based on MaxDelay of any existing Path in the network.  Delay = 0 means a spike reaches receivers in the next Cycle, which is the minimum time (1 msec).  Biologically, subtract 1 from biological synaptic delay values to set corresponding Delay value."}, {Name: "MaxDelay", Doc: "maximum value of Delay -- based on MaxDelay values when the BuildGBuf function was called when the network was built -- cannot set it longer than this, except by calling BuildGBuf on network after changing MaxDelay to a larger value in any pathway in the network."}, {Name: "PFail", Doc: "probability of synaptic transmission failure -- if > 0, then weights are turned off at random as a function of PFail (times 1-SWt if PFailSwt)"}, {Name: "PFailSWt", Doc: "if true, then probability of failure is inversely proportional to SWt structural / slow weight value (i.e., multiply PFail * (1-SWt)))"}, {Name: "DelLen", Doc: "delay length = actual length of the GBuf buffer per neuron = Delay+1 -- just for speed"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathScaleParams", IDName: "path-scale-params", Doc: "PathScaleParams are pathway scaling parameters: modulates overall strength of pathway,\nusing both absolute and relative factors.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"act_path"}}}, Fields: []types.Field{{Name: "Rel", Doc: "relative scaling that shifts balance between different pathways -- this is subject to normalization across all other pathways into receiving neuron, and determines the GScale.Target for adapting scaling"}, {Name: "Abs", Doc: "absolute multiplier adjustment factor for the path scaling -- can be used to adjust for idiosyncrasies not accommodated by the standard scaling based on initial target activation level and relative scaling factors -- any adaptation operates by directly adjusting scaling factor from the initially computed value"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxI32", IDName: "avg-max-i32", Doc: "AvgMaxI32 holds average and max statistics for float32,\nand values used for computing them incrementally,\nusing a fixed precision int32 based float representation\nthat can be used with GPU-based atomic add and max functions.\nThis ONLY works for positive values with averages around 1, and\nthe N must be set IN ADVANCE to the correct number of items.\nOnce Calc() is called, the incremental values are reset\nvia Init() so it is always ready for updating without a separate\nInit() pass.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"avgmaxi"}}}, Fields: []types.Field{{Name: "Avg", Doc: "Average, from Calc when last computed as Sum / N"}, {Name: "Max", Doc: "Maximum value, copied from CurMax in Calc"}, {Name: "Sum", Doc: "sum for computing average -- incremented in UpdateVal, reset in Calc"}, {Name: "CurMax", Doc: "current maximum value, updated via UpdateVal, reset in Calc"}, {Name: "N", Doc: "number of items in the sum -- this must be set in advance to a known value and it is used in computing the float <-> int conversion factor to maximize precision."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AxonNetwork", IDName: "axon-network", Doc: "AxonNetwork defines the essential algorithmic API for Axon, at the network level.\nThese are the methods that the user calls in their Sim code:\n* NewState\n* Cycle\n* NewPhase\n* DWt\n* WtFromDwt\nBecause we don't want to have to force the user to use the interface cast in calling\nthese methods, we provide Impl versions here that are the implementations\nwhich the user-facing method calls through the interface cast.\nSpecialized algorithms should thus only change the Impl version, which is what\nis exposed here in this interface.\n\nThere is now a strong constraint that all Cycle level computation takes place\nin one pass at the Layer level, which greatly improves threading efficiency.\n\nAll of the structural API is in emer.Network, which this interface also inherits for\nconvenience.", Methods: []types.Method{{Name: "AsAxon", Doc: "AsAxon returns this network as a axon.Network -- so that the\nAxonNetwork interface does not need to include accessors\nto all the basic stuff", Returns: []string{"Network"}}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AxonLayer", IDName: "axon-layer", Doc: "AxonLayer defines the essential algorithmic API for Axon, at the layer level.\nThese are the methods that the axon.Network calls on its layers at each step\nof processing.  Other Layer types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Layer, which this interface also inherits for\nconvenience.", Methods: []types.Method{{Name: "AsAxon", Doc: "AsAxon returns this layer as a axon.Layer -- so that the AxonLayer\ninterface does not need to include accessors to all the basic stuff", Returns: []string{"Layer"}}, {Name: "PostBuild", Doc: "PostBuild performs special post-Build() configuration steps for specific algorithms,\nusing configuration data set in BuildConfig during the ConfigNet process."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AxonPath", IDName: "axon-path", Doc: "AxonPath defines the essential algorithmic API for Axon, at the pathway level.\nThese are the methods that the axon.Layer calls on its paths at each step\nof processing.  Other Path types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Path, which this interface also inherits for\nconvenience.", Methods: []types.Method{{Name: "AsAxon", Doc: "AsAxon returns this path as a axon.Path -- so that the AxonPath\ninterface does not need to include accessors to all the basic stuff.", Returns: []string{"Path"}}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AxonPaths", IDName: "axon-paths"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLANovelPath", IDName: "bla-novel-path", Doc: "BLANovelPath connects all other pools to the first, Novelty, pool in a BLA layer.\nThis allows the known US representations to specifically inhibit the novelty pool."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NetIndexes", IDName: "net-indexes", Doc: "NetIndexes are indexes and sizes for processing network", Directives: []types.Directive{{Tool: "gosl", Directive: "end", Args: []string{"context"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"context"}}, {Tool: "gosl", Directive: "endhlsl", Args: []string{"context"}}, {Tool: "gosl", Directive: "start", Args: []string{"context"}}}, Fields: []types.Field{{Name: "NData", Doc: "number of data parallel items to process currently"}, {Name: "NetIndex", Doc: "network index in global Networks list of networks -- needed for GPU shader kernel compatible network variable access functions (e.g., NrnV, SynV etc) in CPU mode"}, {Name: "MaxData", Doc: "maximum amount of data parallel"}, {Name: "NLayers", Doc: "number of layers in the network"}, {Name: "NNeurons", Doc: "total number of neurons"}, {Name: "NPools", Doc: "total number of pools excluding * MaxData factor"}, {Name: "NSyns", Doc: "total number of synapses"}, {Name: "GPUMaxBuffFloats", Doc: "maximum size in float32 (4 bytes) of a GPU buffer -- needed for GPU access"}, {Name: "GPUSynCaBanks", Doc: "total number of SynCa banks of GPUMaxBufferBytes arrays in GPU"}, {Name: "RubiconNPosUSs", Doc: "total number of .Rubicon Drives / positive USs"}, {Name: "RubiconNCosts", Doc: "total number of .Rubicon Costs"}, {Name: "RubiconNNegUSs", Doc: "total number of .Rubicon Negative USs"}, {Name: "GvCostOff", Doc: "offset into GlobalVars for Cost values"}, {Name: "GvCostStride", Doc: "stride into GlobalVars for Cost values"}, {Name: "GvUSnegOff", Doc: "offset into GlobalVars for USneg values"}, {Name: "GvUSnegStride", Doc: "stride into GlobalVars for USneg values"}, {Name: "GvUSposOff", Doc: "offset into GlobalVars for USpos, Drive, VSPatch values values"}, {Name: "GvUSposStride", Doc: "stride into GlobalVars for USpos, Drive, VSPatch values"}, {Name: "pad"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Context", IDName: "context", Doc: "Context contains all of the global context state info\nthat is shared across every step of the computation.\nIt is passed around to all relevant computational functions,\nand is updated on the CPU and synced to the GPU after every cycle.\nIt is the *only* mechanism for communication from CPU to GPU.\nIt contains timing, Testing vs. Training mode, random number context,\nglobal neuromodulation, etc.", Fields: []types.Field{{Name: "Mode", Doc: "current evaluation mode, e.g., Train, Test, etc"}, {Name: "Testing", Doc: "if true, the model is being run in a testing mode, so no weight changes or other associated computations are needed.  this flag should only affect learning-related behavior.  Is automatically updated based on Mode != Train"}, {Name: "Phase", Doc: "phase counter: typicaly 0-1 for minus-plus but can be more phases for other algorithms"}, {Name: "PlusPhase", Doc: "true if this is the plus phase, when the outcome / bursting is occurring, driving positive learning -- else minus phase"}, {Name: "PhaseCycle", Doc: "cycle within current phase -- minus or plus"}, {Name: "Cycle", Doc: "cycle counter: number of iterations of activation updating (settling) on the current state -- this counts time sequentially until reset with NewState"}, {Name: "ThetaCycles", Doc: "length of the theta cycle in terms of 1 msec Cycles -- some network update steps depend on doing something at the end of the theta cycle (e.g., CTCtxtPath)."}, {Name: "CyclesTotal", Doc: "total cycle count -- increments continuously from whenever it was last reset -- typically this is number of milliseconds in simulation time -- is int32 and not uint32 b/c used with Synapse CaUpT which needs to have a -1 case for expired update time"}, {Name: "Time", Doc: "accumulated amount of time the network has been running, in simulation-time (not real world time), in seconds"}, {Name: "TrialsTotal", Doc: "total trial count -- increments continuously in NewState call *only in Train mode* from whenever it was last reset -- can be used for synchronizing weight updates across nodes"}, {Name: "TimePerCycle", Doc: "amount of time to increment per cycle"}, {Name: "SlowInterval", Doc: "how frequently to perform slow adaptive processes such as synaptic scaling, inhibition adaptation, associated in the brain with sleep, in the SlowAdapt method.  This should be long enough for meaningful changes to accumulate -- 100 is default but could easily be longer in larger models.  Because SlowCtr is incremented by NData, high NData cases (e.g. 16) likely need to increase this value -- e.g., 400 seems to produce overall consistent results in various models."}, {Name: "SlowCtr", Doc: "counter for how long it has been since last SlowAdapt step.  Note that this is incremented by NData to maintain consistency across different values of this parameter."}, {Name: "SynCaCtr", Doc: "synaptic calcium counter, which drives the CaUpT synaptic value to optimize updating of this computationally expensive factor. It is incremented by 1 for each cycle, and reset at the SlowInterval, at which point the synaptic calcium values are all reset."}, {Name: "pad"}, {Name: "pad1"}, {Name: "NetIndexes", Doc: "indexes and sizes of current network"}, {Name: "NeuronVars", Doc: "stride offsets for accessing neuron variables"}, {Name: "NeuronAvgVars", Doc: "stride offsets for accessing neuron average variables"}, {Name: "NeuronIndexes", Doc: "stride offsets for accessing neuron indexes"}, {Name: "SynapseVars", Doc: "stride offsets for accessing synapse variables"}, {Name: "SynapseCaVars", Doc: "stride offsets for accessing synapse Ca variables"}, {Name: "SynapseIndexes", Doc: "stride offsets for accessing synapse indexes"}, {Name: "RandCtr", Doc: "random counter -- incremented by maximum number of possible random numbers generated per cycle, regardless of how many are actually used -- this is shared across all layers so must encompass all possible param settings."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BurstParams", IDName: "burst-params", Doc: "BurstParams determine how the 5IB Burst activation is computed from\nCaSpkP integrated spiking values in Super layers -- thresholded.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"deep_layers"}}}, Fields: []types.Field{{Name: "ThrRel", Doc: "Relative component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  This is the distance between the average and maximum activation values within layer (e.g., 0 = average, 1 = max).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "ThrAbs", Doc: "Absolute component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CTParams", IDName: "ct-params", Doc: "CTParams control the CT corticothalamic neuron special behavior", Fields: []types.Field{{Name: "GeGain", Doc: "gain factor for context excitatory input, which is constant as compared to the spiking input from other pathways, so it must be downscaled accordingly.  This can make a difference and may need to be scaled up or down."}, {Name: "DecayTau", Doc: "decay time constant for context Ge input -- if > 0, decays over time so intrinsic circuit dynamics have to take over.  For single-step copy-based cases, set to 0, while longer-time-scale dynamics should use 50 (80 for 280 cycles)"}, {Name: "DecayDt", Doc: "1 / tau"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PulvParams", IDName: "pulv-params", Doc: "PulvParams provides parameters for how the plus-phase (outcome)\nstate of Pulvinar thalamic relay cell neurons is computed from\nthe corresponding driver neuron Burst activation (or CaSpkP if not Super)", Fields: []types.Field{{Name: "DriveScale", Doc: "multiplier on driver input strength, multiplies CaSpkP from driver layer to produce Ge excitatory input to Pulv unit."}, {Name: "FullDriveAct", Doc: "Level of Max driver layer CaSpkP at which the drivers fully drive the burst phase activation.  If there is weaker driver input, then (Max/FullDriveAct) proportion of the non-driver inputs remain and this critically prevents the network from learning to turn activation off, which is difficult and severely degrades learning."}, {Name: "DriveLayIndex", Doc: "index of layer that generates the driving activity into this one -- set via SetBuildConfig(DriveLayName) setting"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GlobalVars", IDName: "global-vars", Doc: "GlobalVars are network-wide variables, such as neuromodulators, reward, drives, etc\nincluding the state for the Rubicon phasic dopamine model. These are stored\nin the Network.Globals float32 slice and corresponding global GPU slice."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PushOff", IDName: "push-off", Doc: "PushOff has push constants for setting offset into compute shader", Fields: []types.Field{{Name: "Off", Doc: "offset"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPU", IDName: "gpu", Doc: "GPU manages all of the GPU-based computation for a given Network.\nLives within the network.", Fields: []types.Field{{Name: "On", Doc: "if true, actually use the GPU"}, {Name: "RecFunTimes", Doc: "if true, slower separate shader pipeline runs are used, with a CPU-sync Wait at the end, to enable timing information about each individual shader to be collected using the network FunTimer system.  otherwise, only aggregate information is available about the entire Cycle call."}, {Name: "CycleByCycle", Doc: "if true, process each cycle one at a time.  Otherwise, 10 cycles at a time are processed in one batch."}, {Name: "Net", Doc: "the network we operate on -- we live under this net"}, {Name: "Ctx", Doc: "the context we use"}, {Name: "Sys", Doc: "the vgpu compute system"}, {Name: "Params", Doc: "VarSet = 0: the uniform LayerParams"}, {Name: "Indexes", Doc: "VarSet = 1: the storage indexes and PathParams"}, {Name: "Structs", Doc: "VarSet = 2: the Storage buffer for RW state structs and neuron floats"}, {Name: "Syns", Doc: "Varset = 3: the Storage buffer for synapses"}, {Name: "SynCas", Doc: "Varset = 4: the Storage buffer for SynCa banks"}, {Name: "Semaphores", Doc: "for sequencing commands"}, {Name: "NThreads", Doc: "number of warp threads -- typically 64 -- must update all hlsl files if changed!"}, {Name: "MaxBufferBytes", Doc: "maximum number of bytes per individual storage buffer element, from GPUProps.Limits.MaxStorageBufferRange"}, {Name: "SynapseCas0", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas1", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas2", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas3", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas4", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas5", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas6", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas7", Doc: "bank of floats for GPU access"}, {Name: "DidBind", Doc: "tracks var binding"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipConfig", IDName: "hip-config", Doc: "HipConfig have the hippocampus size and connectivity parameters", Fields: []types.Field{{Name: "EC2Size", Doc: "size of EC2"}, {Name: "EC3NPool", Doc: "number of EC3 pools (outer dimension)"}, {Name: "EC3NNrn", Doc: "number of neurons in one EC3 pool"}, {Name: "CA1NNrn", Doc: "number of neurons in one CA1 pool"}, {Name: "CA3Size", Doc: "size of CA3"}, {Name: "DGRatio", Doc: "size of DG / CA3"}, {Name: "EC3ToEC2PCon", Doc: "percent connectivity from EC3 to EC2"}, {Name: "EC2ToDGPCon", Doc: "percent connectivity from EC2 to DG"}, {Name: "EC2ToCA3PCon", Doc: "percent connectivity from EC2 to CA3"}, {Name: "CA3ToCA1PCon", Doc: "percent connectivity from CA3 to CA1"}, {Name: "DGToCA3PCon", Doc: "percent connectivity into CA3 from DG"}, {Name: "EC2LatRadius", Doc: "lateral radius of connectivity in EC2"}, {Name: "EC2LatSigma", Doc: "lateral gaussian sigma in EC2 for how quickly weights fall off with distance"}, {Name: "MossyDelta", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in training, applied at the start of a trial to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "MossyDeltaTest", Doc: "proportion of full mossy fiber strength (PathScale.Rel) for CA3 EDL in testing, applied during 2nd-3rd quarters to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "ThetaLow", Doc: "low theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "ThetaHigh", Doc: "high theta modulation value for temporal difference EDL -- sets PathScale.Rel on CA1 <-> EC paths consistent with Theta phase model"}, {Name: "EC5Clamp", Doc: "flag for clamping the EC5 from EC5ClampSrc"}, {Name: "EC5ClampSrc", Doc: "source layer for EC5 clamping activations in the plus phase -- biologically it is EC3 but can use an Input layer if available"}, {Name: "EC5ClampTest", Doc: "clamp the EC5 from EC5ClampSrc during testing as well as training -- this will overwrite any target values that might be used in stats (e.g., in the basic hip example), so it must be turned off there"}, {Name: "EC5ClampThr", Doc: "threshold for binarizing EC5 clamp values -- any value above this is clamped to 1, else 0 -- helps produce a cleaner learning signal.  Set to 0 to not perform any binarization."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HipPathParams", IDName: "hip-path-params", Doc: "HipPathParams define behavior of hippocampus paths, which have special learning rules", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"hip_paths"}}}, Fields: []types.Field{{Name: "Hebb", Doc: "Hebbian learning proportion"}, {Name: "Err", Doc: "EDL proportion"}, {Name: "SAvgCor", Doc: "proportion of correction to apply to sending average activation for hebbian learning component (0=none, 1=all, .5=half, etc)"}, {Name: "SAvgThr", Doc: "threshold of sending average activation below which learning does not occur (prevents learning when there is no input)"}, {Name: "SNominal", Doc: "sending layer Nominal (need to manually set it to be the same as the sending layer)"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActAvgParams", IDName: "act-avg-params", Doc: "ActAvgParams represents the nominal average activity levels in the layer\nand parameters for adapting the computed Gi inhibition levels to maintain\naverage activity within a target range.", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"inhib"}}, {Tool: "gosl", Directive: "end", Args: []string{"inhib"}}, {Tool: "gosl", Directive: "start", Args: []string{"inhib"}}}, Fields: []types.Field{{Name: "Nominal", Doc: "nominal estimated average activity level in the layer, which is used in computing the scaling factor on sending pathways from this layer.  In general it should roughly match the layer ActAvg.ActMAvg value, which can be logged using the axon.LogAddDiagnosticItems function.  If layers receiving from this layer are not getting enough Ge excitation, then this Nominal level can be lowered to increase pathway strength (fewer active neurons means each one contributes more, so scaling factor goes as the inverse of activity level), or vice-versa if Ge is too high.  It is also the basis for the target activity level used for the AdaptGi option -- see the Offset which is added to this value."}, {Name: "AdaptGi", Doc: "enable adapting of layer inhibition Gi multiplier factor (stored in layer GiMult value) to maintain a target layer level of ActAvg.Nominal.  This generally works well and improves the long-term stability of the models.  It is not enabled by default because it depends on having established a reasonable Nominal + Offset target activity level."}, {Name: "Offset", Doc: "offset to add to Nominal for the target average activity that drives adaptation of Gi for this layer.  Typically the Nominal level is good, but sometimes Nominal must be adjusted up or down to achieve desired Ge scaling, so this Offset can compensate accordingly."}, {Name: "HiTol", Doc: "tolerance for higher than Target target average activation as a proportion of that target value (0 = exactly the target, 0.2 = 20% higher than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "LoTol", Doc: "tolerance for lower than Target target average activation as a proportion of that target value (0 = exactly the target, 0.5 = 50% lower than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "AdaptRate", Doc: "rate of Gi adaptation as function of AdaptRate * (Target - ActMAvg) / Target -- occurs at spaced intervals determined by Network.SlowInterval value -- slower values such as 0.01 may be needed for large networks and sparse layers."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TopoInhibParams", IDName: "topo-inhib-params", Doc: "TopoInhibParams provides for topographic gaussian inhibition integrating over neighborhood.\nTODO: not currently being used", Fields: []types.Field{{Name: "On", Doc: "use topographic inhibition"}, {Name: "Width", Doc: "half-width of topographic inhibition within layer"}, {Name: "Sigma", Doc: "normalized gaussian sigma as proportion of Width, for gaussian weighting"}, {Name: "Wrap", Doc: "half-width of topographic inhibition within layer"}, {Name: "Gi", Doc: "overall inhibition multiplier for topographic inhibition (generally <= 1)"}, {Name: "FF", Doc: "overall inhibitory contribution from feedforward inhibition -- multiplies average Ge from pools or Ge from neurons"}, {Name: "FB", Doc: "overall inhibitory contribution from feedback inhibition -- multiplies average activation from pools or Act from neurons"}, {Name: "FF0", Doc: "feedforward zero point for Ge per neuron (summed Ge is compared to N * FF0) -- below this level, no FF inhibition is computed, above this it is FF * (Sum Ge - N * FF0)"}, {Name: "WidthWt", Doc: "weight value at width -- to assess the value of Sigma"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.InhibParams", IDName: "inhib-params", Doc: "axon.InhibParams contains all the inhibition computation params and functions for basic Axon\nThis is included in axon.Layer to support computation.\nThis also includes other misc layer-level params such as expected average activation in the layer\nwhich is used for Ge rescaling and potentially for adapting inhibition over time", Fields: []types.Field{{Name: "ActAvg", Doc: "layer-level and pool-level average activation initial values and updating / adaptation thereof -- initial values help determine initial scaling factors."}, {Name: "Layer", Doc: "inhibition across the entire layer -- inputs generally use Gi = 0.8 or 0.9, 1.3 or higher for sparse layers.  If the layer has sub-pools (4D shape) then this is effectively between-pool inhibition."}, {Name: "Pool", Doc: "inhibition within sub-pools of units, for layers with 4D shape -- almost always need this if the layer has pools."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Layer", IDName: "layer", Doc: "axon.Layer implements the basic Axon spiking activation function,\nand manages learning in the pathways.", Methods: []types.Method{{Name: "Defaults", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "InitWeights", Doc: "InitWeights initializes the weight values in the network, i.e., resetting learning\nAlso calls InitActs", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx", "nt"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- only called automatically during InitWeights", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "UnLesionNeurons", Doc: "UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "LesionNeurons", Doc: "LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer\nreturns number of neurons lesioned.  Emits error if prop > 1 as indication that percent\nmight have been passed", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"prop"}, Returns: []string{"int"}}}, Embeds: []types.Field{{Name: "LayerBase"}}, Fields: []types.Field{{Name: "Params", Doc: "all layer-level parameters -- these must remain constant once configured"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerBase", IDName: "layer-base", Doc: "LayerBase manages the structural elements of the layer, which are common\nto any Layer type.\nThe Base does not have algorithm-specific methods and parameters, so it can be easily\nreused for different algorithms, and cleanly separates the algorithm-specific code.\nAny dependency on the algorithm-level Layer can be captured in the AxonLayer interface,\naccessed via the AxonLay field.", Fields: []types.Field{{Name: "AxonLay", Doc: "we need a pointer to ourselves as an AxonLayer (which subsumes emer.Layer), which can always be used to extract the true underlying type of object when layer is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Network", Doc: "our parent network, in case we need to use it to find other layers etc -- set when added by network"}, {Name: "Off", Doc: "inactivate this layer -- allows for easy experimentation"}, {Name: "Type", Doc: "type of layer"}, {Name: "NNeurons", Doc: "number of neurons in the layer"}, {Name: "NeurStIndex", Doc: "starting index of neurons for this layer within the global Network list"}, {Name: "NPools", Doc: "number of pools based on layer shape -- at least 1 for layer pool + 4D subpools"}, {Name: "MaxData", Doc: "maximum amount of input data that can be processed in parallel in one pass of the network. Neuron, Pool, Values storage is allocated to hold this amount."}, {Name: "RecvPaths", Doc: "list of receiving pathways into this layer from other layers"}, {Name: "SendPaths", Doc: "list of sending pathways from this layer to other layers"}, {Name: "Values", Doc: "layer-level state values that are updated during computation -- one for each data parallel -- is a sub-slice of network full set"}, {Name: "Pools", Doc: "computes FS-FFFB inhibition and other pooled, aggregate state variables -- has at least 1 for entire layer (lpl = layer pool), and one for each sub-pool if shape supports that (4D) * 1 per data parallel (inner loop).  This is a sub-slice from overall Network Pools slice.  You must iterate over index and use pointer to modify values."}, {Name: "Exts", Doc: "external input values for this layer, allocated from network global Exts slice"}, {Name: "BuildConfig", Doc: "configuration data set when the network is configured, that is used during the network Build() process via PostBuild method, after all the structure of the network has been fully constructed.  In particular, the Params is nil until Build, so setting anything specific in there (e.g., an index to another layer) must be done as a second pass.  Note that Params are all applied after Build and can set user-modifiable params, so this is for more special algorithm structural parameters set during ConfigNet() methods.,"}, {Name: "DefaultParams", Doc: "default parameters that are applied prior to user-set parameters -- these are useful for specific layer functionality in specialized brain areas (e.g., Rubicon, BG etc) not associated with a layer type, which otherwise is used to hard-code initial default parameters -- typically just set to a literal map."}, {Name: "ParamsHistory", Doc: "provides a history of parameters applied to the layer"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerIndexes", IDName: "layer-indexes", Doc: "LayerIndexes contains index access into network global arrays for GPU.", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"layerparams"}}, {Tool: "gosl", Directive: "end", Args: []string{"layerparams"}}, {Tool: "gosl", Directive: "start", Args: []string{"layerparams"}}}, Fields: []types.Field{{Name: "LayIndex", Doc: "layer index"}, {Name: "MaxData", Doc: "maximum number of data parallel elements"}, {Name: "PoolSt", Doc: "start of pools for this layer -- first one is always the layer-wide pool"}, {Name: "NeurSt", Doc: "start of neurons for this layer in global array (same as Layer.NeurStIndex)"}, {Name: "NeurN", Doc: "number of neurons in layer"}, {Name: "RecvSt", Doc: "start index into RecvPaths global array"}, {Name: "RecvN", Doc: "number of recv pathways"}, {Name: "SendSt", Doc: "start index into RecvPaths global array"}, {Name: "SendN", Doc: "number of recv pathways"}, {Name: "ExtsSt", Doc: "starting index in network global Exts list of external input for this layer -- only for Input / Target / Compare layer types"}, {Name: "ShpPlY", Doc: "layer shape Pools Y dimension -- 1 for 2D"}, {Name: "ShpPlX", Doc: "layer shape Pools X dimension -- 1 for 2D"}, {Name: "ShpUnY", Doc: "layer shape Units Y dimension"}, {Name: "ShpUnX", Doc: "layer shape Units X dimension"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerInhibIndexes", IDName: "layer-inhib-indexes", Doc: "LayerInhibIndexes contains indexes of layers for between-layer inhibition", Fields: []types.Field{{Name: "Index1", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib1Name if present -- -1 if not used"}, {Name: "Index2", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib2Name if present -- -1 if not used"}, {Name: "Index3", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib3Name if present -- -1 if not used"}, {Name: "Index4", Doc: "idx of Layer to geta layer-level inhibition from -- set during Build from BuildConfig LayInhib4Name if present -- -1 if not used"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerParams", IDName: "layer-params", Doc: "LayerParams contains all of the layer parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []types.Field{{Name: "LayType", Doc: "functional type of layer -- determines functional code path for specialized layer types, and is synchronized with the Layer.Typ value"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "Acts", Doc: "Activation parameters and methods for computing activations"}, {Name: "Inhib", Doc: "Inhibition parameters and methods for computing layer-level inhibition"}, {Name: "LayInhib", Doc: "indexes of layers that contribute between-layer inhibition to this layer -- set these indexes via BuildConfig LayInhibXName (X = 1, 2...)"}, {Name: "Learn", Doc: "Learning parameters and methods that operate at the neuron level"}, {Name: "Bursts", Doc: "BurstParams determine how the 5IB Burst activation is computed from CaSpkP integrated spiking values in Super layers -- thresholded."}, {Name: "CT", Doc: "] params for the CT corticothalamic layer and PTPred layer that generates predictions over the Pulvinar using context -- uses the CtxtGe excitatory input plus stronger NMDA channels to maintain context trace"}, {Name: "Pulv", Doc: "provides parameters for how the plus-phase (outcome) state of Pulvinar thalamic relay cell neurons is computed from the corresponding driver neuron Burst activation (or CaSpkP if not Super)"}, {Name: "Matrix", Doc: "parameters for BG Striatum Matrix MSN layers, which are the main Go / NoGo gating units in BG."}, {Name: "GP", Doc: "type of GP Layer."}, {Name: "LDT", Doc: "parameterizes laterodorsal tegmentum ACh salience neuromodulatory signal, driven by superior colliculus stimulus novelty, US input / absence, and OFC / ACC inhibition"}, {Name: "VTA", Doc: "parameterizes computing overall VTA DA based on LHb PVDA (primary value -- at US time, computed at start of each trial and stored in LHbPVDA global value) and Amygdala (CeM) CS / learned value (LV) activations, which update every cycle."}, {Name: "RWPred", Doc: "parameterizes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework)."}, {Name: "RWDa", Doc: "parameterizes reward prediction dopamine for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework)."}, {Name: "TDInteg", Doc: "parameterizes TD reward integration layer"}, {Name: "TDDa", Doc: "parameterizes dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase."}, {Name: "Indexes", Doc: "recv and send pathway array access info"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerTypes", IDName: "layer-types", Doc: "LayerTypes is an axon-specific layer type enum,\nthat encompasses all the different algorithm types supported.\nClass parameter styles automatically key off of these types.\nThe first entries must be kept synchronized with the emer.LayerType,\nalthough we replace Hidden -> Super."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ActAvgValues", IDName: "act-avg-values", Doc: "ActAvgValues are long-running-average activation levels stored in the LayerValues,\nfor monitoring and adapting inhibition and possibly scaling parameters.\nAll of these integrate over NData within a network, so are the same across them.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"layervals"}}}, Fields: []types.Field{{Name: "ActMAvg", Doc: "running-average minus-phase activity integrated at Dt.LongAvgTau -- used for adapting inhibition relative to target level"}, {Name: "ActPAvg", Doc: "running-average plus-phase activity integrated at Dt.LongAvgTau"}, {Name: "AvgMaxGeM", Doc: "running-average max of minus-phase Ge value across the layer integrated at Dt.LongAvgTau"}, {Name: "AvgMaxGiM", Doc: "running-average max of minus-phase Gi value across the layer integrated at Dt.LongAvgTau"}, {Name: "GiMult", Doc: "multiplier on inhibition -- adapted to maintain target activity level"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CorSimStats", IDName: "cor-sim-stats", Doc: "CorSimStats holds correlation similarity (centered cosine aka normalized dot product)\nstatistics at the layer level", Fields: []types.Field{{Name: "Cor", Doc: "correlation (centered cosine aka normalized dot product) activation difference between ActP and ActM on this alpha-cycle for this layer -- computed by CorSimFromActs called by PlusPhase"}, {Name: "Avg", Doc: "running average of correlation similarity between ActP and ActM -- computed with CorSim.Tau time constant in PlusPhase"}, {Name: "Var", Doc: "running variance of correlation similarity between ActP and ActM -- computed with CorSim.Tau time constant in PlusPhase"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LaySpecialValues", IDName: "lay-special-values", Doc: "LaySpecialValues holds special values used to communicate to other layers\nbased on neural values, used for special algorithms such as RL where\nsome of the computation is done algorithmically.", Fields: []types.Field{{Name: "V1", Doc: "one value"}, {Name: "V2", Doc: "one value"}, {Name: "V3", Doc: "one value"}, {Name: "V4", Doc: "one value"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LayerValues", IDName: "layer-values", Doc: "LayerValues holds extra layer state that is updated per layer.\nIt is sync'd down from the GPU to the CPU after every Cycle.", Fields: []types.Field{{Name: "LayIndex", Doc: "layer index for these vals"}, {Name: "DataIndex", Doc: "data index for these vals"}, {Name: "RT", Doc: "reaction time for this layer in cycles, which is -1 until the Max CaSpkP level (after MaxCycStart) exceeds the Act.Attn.RTThr threshold"}, {Name: "pad"}, {Name: "ActAvg", Doc: "running-average activation levels used for adaptive inhibition, and other adapting values"}, {Name: "CorSim", Doc: "correlation (centered cosine aka normalized dot product) similarity between ActM, ActP states"}, {Name: "Special", Doc: "special values used to communicate to other layers based on neural values computed on the GPU -- special cross-layer computations happen CPU-side and are sent back into the network via Context on the next cycle -- used for special algorithms such as RL / DA etc"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.CaLrnParams", IDName: "ca-lrn-params", Doc: "CaLrnParams parameterizes the neuron-level calcium signals driving learning:\nCaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or\nuse the more complex and dynamic VGCC channel directly.\nCaLrn is then integrated in a cascading manner at multiple time scales:\nCaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase).", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"learn_neur"}}, {Tool: "gosl", Directive: "end", Args: []string{"learn_neur"}}, {Tool: "gosl", Directive: "start", Args: []string{"learn_neur"}}}, Fields: []types.Field{{Name: "Norm", Doc: "denomenator used for normalizing CaLrn, so the max is roughly 1 - 1.5 or so, which works best in terms of previous standard learning rules, and overall learning performance"}, {Name: "SpkVGCC", Doc: "use spikes to generate VGCC instead of actual VGCC current -- see SpkVGCCa for calcium contribution from each spike"}, {Name: "SpkVgccCa", Doc: "multiplier on spike for computing Ca contribution to CaLrn in SpkVGCC mode"}, {Name: "VgccTau", Doc: "time constant of decay for VgccCa calcium -- it is highly transient around spikes, so decay and diffusion factors are more important than for long-lasting NMDA factor.  VgccCa is integrated separately int VgccCaInt prior to adding into NMDA Ca in CaLrn"}, {Name: "Dt", Doc: "time constants for integrating CaLrn across M, P and D cascading levels"}, {Name: "UpdateThr", Doc: "Threshold on CaSpkP CaSpkD value for updating synapse-level Ca values (SynCa) -- this is purely a performance optimization that excludes random infrequent spikes -- 0.05 works well on larger networks but not smaller, which require the .01 default."}, {Name: "VgccDt", Doc: "rate = 1 / tau"}, {Name: "NormInv", Doc: "= 1 / Norm"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TrgAvgActParams", IDName: "trg-avg-act-params", Doc: "TrgAvgActParams govern the target and actual long-term average activity in neurons.\nTarget value is adapted by neuron-wise error and difference in actual vs. target.\ndrives synaptic scaling at a slow timescale (Network.SlowInterval).", Fields: []types.Field{{Name: "GiBaseInit", Doc: "if this is > 0, then each neuron's GiBase is initialized as this proportion of TrgRange.Max - TrgAvg -- gives neurons differences in intrinsic inhibition / leak as a starting bias.  This is independent of using the target values to scale synaptic weights."}, {Name: "RescaleOn", Doc: "whether to use target average activity mechanism to rescale synaptic weights, so that activity tracks the target values"}, {Name: "ErrLRate", Doc: "learning rate for adjustments to Trg value based on unit-level error signal.  Population TrgAvg values are renormalized to fixed overall average in TrgRange. Generally, deviating from the default doesn't make much difference."}, {Name: "SynScaleRate", Doc: "rate parameter for how much to scale synaptic weights in proportion to the AvgDif between target and actual proportion activity -- this determines the effective strength of the constraint, and larger models may need more than the weaker default value."}, {Name: "SubMean", Doc: "amount of mean trg change to subtract -- 1 = full zero sum.  1 works best in general -- but in some cases it may be better to start with 0 and then increase using network SetSubMean method at a later point."}, {Name: "Permute", Doc: "permute the order of TrgAvg values within layer -- otherwise they are just assigned in order from highest to lowest for easy visualization -- generally must be true if any topographic weights are being used"}, {Name: "Pool", Doc: "use pool-level target values if pool-level inhibition and 4D pooled layers are present -- if pool sizes are relatively small, then may not be useful to distribute targets just within pool"}, {Name: "pad"}, {Name: "TrgRange", Doc: "range of target normalized average activations -- individual neurons are assigned values within this range to TrgAvg, and clamped within this range."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLRateParams", IDName: "rl-rate-params", Doc: "RLRateParams are recv neuron learning rate modulation parameters.\nHas two factors: the derivative of the sigmoid based on CaSpkD\nactivity levels, and based on the phase-wise differences in activity (Diff).", Fields: []types.Field{{Name: "On", Doc: "use learning rate modulation"}, {Name: "SigmoidLinear", Doc: "use a linear sigmoid function: if act > .5: 1-act; else act\notherwise use the actual sigmoid derivative which is squared: a(1-a)"}, {Name: "SigmoidMin", Doc: "minimum learning rate multiplier for sigmoidal act (1-act) factor,\nwhich prevents lrate from going too low for extreme values.\nSet to 1 to disable Sigmoid derivative factor, which is default for Target layers."}, {Name: "Diff", Doc: "modulate learning rate as a function of plus - minus differences"}, {Name: "SpkThr", Doc: "threshold on Max(CaSpkP, CaSpkD) below which Min lrate applies.\nmust be > 0 to prevent div by zero."}, {Name: "DiffThr", Doc: "threshold on recv neuron error delta, i.e., |CaSpkP - CaSpkD| below which lrate is at Min value"}, {Name: "Min", Doc: "for Diff component, minimum learning rate value when below ActDiffThr"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnNeurParams", IDName: "learn-neur-params", Doc: "axon.LearnNeurParams manages learning-related parameters at the neuron-level.\nThis is mainly the running average activations that drive learning", Fields: []types.Field{{Name: "CaLearn", Doc: "parameterizes the neuron-level calcium signals driving learning: CaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or use the more complex and dynamic VGCC channel directly.  CaLrn is then integrated in a cascading manner at multiple time scales: CaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase)."}, {Name: "CaSpk", Doc: "parameterizes the neuron-level spike-driven calcium signals, starting with CaSyn that is integrated at the neuron level, and drives synapse-level, pre * post Ca integration, which provides the Tr trace that multiplies error signals, and drives learning directly for Target layers. CaSpk* values are integrated separately at the Neuron level and used for UpdateThr and RLRate as a proxy for the activation (spiking) based learning signal."}, {Name: "LrnNMDA", Doc: "NMDA channel parameters used for learning, vs. the ones driving activation -- allows exploration of learning parameters independent of their effects on active maintenance contributions of NMDA, and may be supported by different receptor subtypes"}, {Name: "TrgAvgAct", Doc: "synaptic scaling parameters for regulating overall average activity compared to neuron's own target level"}, {Name: "RLRate", Doc: "recv neuron learning rate modulation params -- an additional error-based modulation of learning for receiver side: RLRate = |CaSpkP - CaSpkD| / Max(CaSpkP, CaSpkD)"}, {Name: "NeuroMod", Doc: "neuromodulation effects on learning rate and activity, as a function of layer-level DA and ACh values, which are updated from global Context values, and computed from reinforcement learning algorithms"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtInitParams", IDName: "s-wt-init-params", Doc: "SWtInitParams for initial SWt values", Fields: []types.Field{{Name: "SPct", Doc: "how much of the initial random weights are captured in the SWt values -- rest goes into the LWt values.  1 gives the strongest initial biasing effect, for larger models that need more structural support. 0.5 should work for most models where stronger constraints are not needed."}, {Name: "Mean", Doc: "target mean weight values across receiving neuron's pathway -- the mean SWt values are constrained to remain at this value.  some pathways may benefit from lower mean of .4"}, {Name: "Var", Doc: "initial variance in weight values, prior to constraints."}, {Name: "Sym", Doc: "symmetrize the initial weight values with those in reciprocal pathway -- typically true for bidirectional excitatory connections"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtAdaptParams", IDName: "s-wt-adapt-params", Doc: "SWtAdaptParams manages adaptation of SWt values", Fields: []types.Field{{Name: "On", Doc: "if true, adaptation is active -- if false, SWt values are not updated, in which case it is generally good to have Init.SPct=0 too."}, {Name: "LRate", Doc: "learning rate multiplier on the accumulated DWt values (which already have fast LRate applied) to incorporate into SWt during slow outer loop updating -- lower values impose stronger constraints, for larger networks that need more structural support, e.g., 0.001 is better after 1,000 epochs in large models.  0.1 is fine for smaller models."}, {Name: "SubMean", Doc: "amount of mean to subtract from SWt delta when updating -- generally best to set to 1"}, {Name: "SigGain", Doc: "gain of sigmoidal constrast enhancement function used to transform learned, linear LWt values into Wt values"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SWtParams", IDName: "s-wt-params", Doc: "SWtParams manages structural, slowly adapting weight values (SWt),\nin terms of initialization and updating over course of learning.\nSWts impose initial and slowly adapting constraints on neuron connectivity\nto encourage differentiation of neuron representations and overall good behavior\nin terms of not hogging the representational space.\nThe TrgAvg activity constraint is not enforced through SWt -- it needs to be\nmore dynamic and supported by the regular learned weights.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []types.Field{{Name: "Init", Doc: "initialization of SWt values"}, {Name: "Adapt", Doc: "adaptation of SWt values in response to LWt learning"}, {Name: "Limit", Doc: "range limits for SWt values"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateParams", IDName: "l-rate-params", Doc: "LRateParams manages learning rate parameters", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []types.Field{{Name: "Base", Doc: "base learning rate for this pathway -- can be modulated by other factors below -- for larger networks, use slower rates such as 0.04, smaller networks can use faster 0.2."}, {Name: "Sched", Doc: "scheduled learning rate multiplier, simulating reduction in plasticity over aging"}, {Name: "Mod", Doc: "dynamic learning rate modulation due to neuromodulatory or other such factors"}, {Name: "Eff", Doc: "effective actual learning rate multiplier used in computing DWt: Eff = eMod * Sched * Base"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TraceParams", IDName: "trace-params", Doc: "TraceParams manages parameters associated with temporal trace learning", Fields: []types.Field{{Name: "Tau", Doc: "time constant for integrating trace over theta cycle timescales.\ngoverns the decay rate of syanptic trace"}, {Name: "SubMean", Doc: "amount of the mean dWt to subtract, producing a zero-sum effect -- 1.0 = full zero-sum dWt -- only on non-zero DWts.  typically set to 0 for standard trace learning pathways, although some require it for stability over the long haul.  can use SetSubMean to set to 1 after significant early learning has occurred with 0.  Some special path types (e.g., Hebb) benefit from SubMean = 1 always"}, {Name: "LearnThr", Doc: "threshold for learning, depending on different algorithms -- in Matrix and VSPatch it applies to normalized GeIntNorm value -- setting this relatively high encourages sparser representations"}, {Name: "Dt", Doc: "rate = 1 / tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LRateMod", IDName: "l-rate-mod", Doc: "LRateMod implements global learning rate modulation, based on a performance-based\nfactor, for example error.  Increasing levels of the factor = higher learning rate.\nThis can be added to a Sim and called prior to DWt() to dynamically change lrate\nbased on overall network performance.", Fields: []types.Field{{Name: "On", Doc: "toggle use of this modulation factor"}, {Name: "Base", Doc: "baseline learning rate -- what you get for correct cases"}, {Name: "pad"}, {Name: "pad1"}, {Name: "Range", Doc: "defines the range over which modulation occurs for the modulator factor -- Min and below get the Base level of learning rate modulation, Max and above get a modulation of 1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.HebbParams", IDName: "hebb-params", Doc: "HebbParams for optional hebbian learning that replaces the\ndefault learning rule, based on S = sending activity,\nR = receiving activity", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []types.Field{{Name: "On", Doc: "if On, then the standard learning rule is replaced with a Hebbian learning rule"}, {Name: "Up", Doc: "strength multiplier for hebbian increases, based on R * S * (1-LWt)"}, {Name: "Down", Doc: "strength multiplier for hebbian decreases, based on R * (1 - S) * LWt"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LearnSynParams", IDName: "learn-syn-params", Doc: "LearnSynParams manages learning-related parameters at the synapse-level.", Fields: []types.Field{{Name: "Learn", Doc: "enable learning for this pathway"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "LRate", Doc: "learning rate parameters, supporting two levels of modulation on top of base learning rate."}, {Name: "Trace", Doc: "trace-based learning parameters"}, {Name: "KinaseCa", Doc: "kinase calcium Ca integration parameters: using linear regression parameters"}, {Name: "Hebb", Doc: "hebbian learning option, which overrides the default learning rules"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Network", IDName: "network", Doc: "axon.Network implements the Axon spiking model.", Methods: []types.Method{{Name: "InitWeights", Doc: "InitWeights initializes synaptic weights and all other associated long-term state variables\nincluding running-average state values (e.g., layer running average activations etc)", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- not automatically called", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "ShowAllGlobals", Doc: "ShowAllGlobals shows a listing of all Global variables and values.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}}, {Name: "Build", Doc: "Build constructs the layer and pathway state based on the layer shapes\nand patterns of interconnectivity. Configures threading using heuristics based\non final network size.  Must set UseGPUOrder properly prior to calling.\nConfigures the given Context object used in the simulation with the memory\naccess strides for this network -- must be set properly -- see SetCtxStrides.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"simCtx"}, Returns: []string{"error"}}, {Name: "SaveWeightsJSON", Doc: "SaveWeightsJSON saves network weights (and any other state that adapts with learning)\nto a JSON-formatted file.  If filename has .gz extension, then file is gzip compressed.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"filename"}, Returns: []string{"error"}}, {Name: "OpenWeightsJSON", Doc: "OpenWeightsJSON opens network weights (and any other state that adapts with learning)\nfrom a JSON-formatted file.  If filename has .gz extension, then file is gzip uncompressed.", Directives: []types.Directive{{Tool: "types", Directive: "add"}}, Args: []string{"filename"}, Returns: []string{"error"}}}, Embeds: []types.Field{{Name: "NetworkBase"}}, Fields: []types.Field{{Name: "Rubicon", Doc: "Rubicon system for goal-driven motivated behavior,\nincluding Rubicon phasic dopamine signaling.\nManages internal drives, US outcomes. Core LHb (lateral habenula)\nand VTA (ventral tegmental area) dopamine are computed\nin equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nRenders USLayer, PVLayer, DrivesLayer representations\nbased on state updated here."}, {Name: "NetIndex", Doc: "network index in global Networks list of networks.\nNeeded for GPU shader kernel compatible network variable\naccess functions (e.g., NrnV, SynV etc) in CPU mode."}, {Name: "MaxDelay", Doc: "maximum synaptic delay across any pathway in the network.\nused for sizing the GBuf accumulation buffer."}, {Name: "MaxData", Doc: "maximum number of data inputs that can be processed\nin parallel in one pass of the network.\nNeuron storage is allocated to hold this amount during\nBuild process, and this value reflects that."}, {Name: "NNeurons", Doc: "total number of neurons."}, {Name: "NSyns", Doc: "total number of synapses."}, {Name: "Globals", Doc: "storage for global vars."}, {Name: "Layers", Doc: "array of layers."}, {Name: "LayParams", Doc: "array of layer parameters, in 1-to-1 correspondence with Layers."}, {Name: "LayValues", Doc: "array of layer values, with extra per data."}, {Name: "Pools", Doc: "array of inhibitory pools for all layers."}, {Name: "Neurons", Doc: "network's allocation of neuron variables,\naccessed via NrnV function with flexible striding."}, {Name: "NeuronAvgs", Doc: "network's allocation of neuron average avariables,\naccessed via NrnAvgV function with flexible striding."}, {Name: "NeuronIxs", Doc: "network's allocation of neuron index variables,\naccessed via NrnI function with flexible striding."}, {Name: "Paths", Doc: "pointers to all pathways in the network, sender-based."}, {Name: "PathParams", Doc: "array of pathway parameters, in 1-to-1 correspondence\nwith Paths, sender-based."}, {Name: "SynapseIxs", Doc: "network's allocation of synapse idx vars, organized\nsender-based, with flexible striding, accessed via SynI function."}, {Name: "Synapses", Doc: "network's allocation of synapses, organized sender-based,\nwith flexible striding, accessed via SynV function."}, {Name: "SynapseCas", Doc: "network's allocation of synapse Ca vars, organized sender-based,\nwith flexible striding, accessed via SynCaV function."}, {Name: "PathSendCon", Doc: "starting offset and N cons for each sending neuron,\nfor indexing into the Syns synapses, which are organized sender-based."}, {Name: "PathRecvCon", Doc: "starting offset and N cons for each recv neuron,\nfor indexing into the RecvSynIndex array of indexes into the\nSyns synapses, which are organized sender-based."}, {Name: "PathGBuf", Doc: "conductance buffer for accumulating spikes.\nsubslices are allocated to each pathway.\nuses int-encoded float values for faster GPU atomic integration."}, {Name: "PathGSyns", Doc: "synaptic conductance integrated over time per pathway\nper recv neurons. spikes come in via PathBuf.\nsubslices are allocated to each pathway."}, {Name: "RecvPathIndexes", Doc: "indexes into Paths (organized by SendPath) organized\nby recv pathways. needed for iterating through recv\npaths efficiently on GPU."}, {Name: "RecvSynIndexes", Doc: "indexes into Synapses for each recv neuron, organized\ninto blocks according to PathRecvCon, for receiver-based access."}, {Name: "Exts", Doc: "external input values for all Input / Target / Compare layers\nin the network. The ApplyExt methods write to this per layer,\nand it is then actually applied in one consistent method."}, {Name: "Ctx", Doc: "context used only for accessing neurons for display.\n NetIndexes.NData in here is copied from active context in NewState."}, {Name: "NThreads", Doc: "number of threads to use for parallel processing."}, {Name: "GPU", Doc: "GPU implementation."}, {Name: "RecFunTimes", Doc: "record function timer information."}, {Name: "FunTimes", Doc: "timers for each major function (step of processing)."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DAModTypes", IDName: "da-mod-types", Doc: "DAModTypes are types of dopamine modulation of neural activity."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.ValenceTypes", IDName: "valence-types", Doc: "ValenceTypes are types of valence coding: positive or negative."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuroModParams", IDName: "neuro-mod-params", Doc: "NeuroModParams specifies the effects of neuromodulators on neural\nactivity and learning rate.  These can apply to any neuron type,\nand are applied in the core cycle update equations.", Fields: []types.Field{{Name: "DAMod", Doc: "dopamine receptor-based effects of dopamine modulation on excitatory and inhibitory conductances: D1 is excitatory, D2 is inhibitory as a function of increasing dopamine"}, {Name: "Valence", Doc: "valence coding of this layer -- may affect specific layer types but does not directly affect neuromodulators currently"}, {Name: "DAModGain", Doc: "dopamine modulation of excitatory and inhibitory conductances (i.e., \"performance dopamine\" effect -- this does NOT affect learning dopamine modulation in terms of RLrate): g *= 1 + (DAModGain * DA)"}, {Name: "DALRateSign", Doc: "modulate the sign of the learning rate factor according to the DA sign, taking into account the DAMod sign reversal for D2Mod, also using BurstGain and DipGain to modulate DA value -- otherwise, only the magnitude of the learning rate is modulated as a function of raw DA magnitude according to DALRateMod (without additional gain factors)"}, {Name: "DALRateMod", Doc: "if not using DALRateSign, this is the proportion of maximum learning rate that Abs(DA) magnitude can modulate -- e.g., if 0.2, then DA = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChLRateMod", Doc: "proportion of maximum learning rate that ACh can modulate -- e.g., if 0.2, then ACh = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChDisInhib", Doc: "amount of extra Gi inhibition added in proportion to 1 - ACh level -- makes ACh disinhibitory"}, {Name: "BurstGain", Doc: "multiplicative gain factor applied to positive dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign!"}, {Name: "DipGain", Doc: "multiplicative gain factor applied to negative dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign! should be small for acq, but roughly equal to burst for ext"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronFlags", IDName: "neuron-flags", Doc: "NeuronFlags are bit-flags encoding relevant binary state for neurons"})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronVars", IDName: "neuron-vars", Doc: "NeuronVars are the neuron variables representing current active state,\nspecific to each input data state.\nSee NeuronAvgVars for vars shared across data."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVars", IDName: "neuron-avg-vars", Doc: "NeuronAvgVars are mostly neuron variables involved in longer-term average activity\nwhich is aggregated over time and not specific to each input data state,\nalong with any other state that is not input data specific."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronIndexes", IDName: "neuron-indexes", Doc: "NeuronIndexes are the neuron indexes and other uint32 values.\nThere is only one of these per neuron -- not data parallel.\nnote: Flags are encoded in Vars because they are data parallel and\nwritable, whereas indexes are read-only."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronVarStrides", IDName: "neuron-var-strides", Doc: "NeuronVarStrides encodes the stride offsets for neuron variable access\ninto network float32 array.  Data is always the inner-most variable.", Directives: []types.Directive{{Tool: "gosl", Directive: "end", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "end", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "start", Args: []string{"neuron"}}}, Fields: []types.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVarStrides", IDName: "neuron-avg-var-strides", Doc: "NeuronAvgVarStrides encodes the stride offsets for neuron variable access\ninto network float32 array.  Data is always the inner-most variable.", Fields: []types.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.NeuronIndexStrides", IDName: "neuron-index-strides", Doc: "NeuronIndexStrides encodes the stride offsets for neuron index access\ninto network uint32 array.", Fields: []types.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Idx", Doc: "index value level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Path", IDName: "path", Doc: "axon.Path is a basic Axon pathway with synaptic learning parameters", Embeds: []types.Field{{Name: "PathBase"}}, Fields: []types.Field{{Name: "Params", Doc: "all path-level parameters -- these must remain constant once configured"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathBase", IDName: "path-base", Doc: "PathBase contains the basic structural information for specifying a pathway of synaptic\nconnections between two layers, and maintaining all the synaptic connection-level data.\nThe same struct token is added to the Recv and Send layer path lists, and it manages everything\nabout the connectivity, and methods on the Path handle all the relevant computation.\nThe Base does not have algorithm-specific methods and parameters, so it can be easily\nreused for different algorithms, and cleanly separates the algorithm-specific code.\nAny dependency on the algorithm-level Path can be captured in the AxonPath interface,\naccessed via the AxonPth field.", Fields: []types.Field{{Name: "AxonPth", Doc: "we need a pointer to ourselves as an AxonPath, which can always be used to extract the true underlying type of object when path is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Off", Doc: "inactivate this pathway -- allows for easy experimentation"}, {Name: "Cls", Doc: "Class is for applying parameter styles, can be space separated multple tags"}, {Name: "Notes", Doc: "can record notes about this pathway here"}, {Name: "Send", Doc: "sending layer for this pathway"}, {Name: "Recv", Doc: "receiving layer for this pathway"}, {Name: "Pat", Doc: "pattern of connectivity"}, {Name: "Typ", Doc: "type of pathway:  Forward, Back, Lateral, or extended type in specialized algorithms.\nMatches against .Cls parameter styles (e.g., .Back etc)"}, {Name: "DefaultParams", Doc: "default parameters that are applied prior to user-set parameters.\nthese are useful for specific functionality in specialized brain areas\n(e.g., Rubicon, BG etc) not associated with a path type, which otherwise\nis used to hard-code initial default parameters.\nTypically just set to a literal map."}, {Name: "ParamsHistory", Doc: "provides a history of parameters applied to the layer"}, {Name: "RecvConNAvgMax", Doc: "average and maximum number of recv connections in the receiving layer"}, {Name: "SendConNAvgMax", Doc: "average and maximum number of sending connections in the sending layer"}, {Name: "SynStIndex", Doc: "start index into global Synapse array:"}, {Name: "NSyns", Doc: "number of synapses in this pathway"}, {Name: "RecvCon", Doc: "starting offset and N cons for each recv neuron, for indexing into the RecvSynIndex array of indexes into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathRecvCons slice for GPU usage."}, {Name: "RecvSynIndex", Doc: "index into Syns synaptic state for each sending unit and connection within that, for the sending pathway which does not own the synapses, and instead indexes into recv-ordered list"}, {Name: "RecvConIndex", Doc: "for each recv synapse, this is index of *sending* neuron  It is generally preferable to use the Synapse SendIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "SendCon", Doc: "starting offset and N cons for each sending neuron, for indexing into the Syns synapses, which are organized sender-based.  This is locally managed during build process, but also copied to network global PathSendCons slice for GPU usage."}, {Name: "SendConIndex", Doc: "index of other neuron that receives the sender's synaptic input, ordered by the sending layer's order of units as the outer loop, and SendCon.N receiving units within that.  It is generally preferable to use the Synapse RecvIndex where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "GBuf", Doc: "Ge or Gi conductance ring buffer for each neuron, accessed through Params.Com.ReadIndex, WriteIndex -- scale * weight is added with Com delay offset -- a subslice from network PathGBuf. Uses int-encoded float values for faster GPU atomic integration"}, {Name: "GSyns", Doc: "pathway-level synaptic conductance values, integrated by path before being integrated at the neuron level, which enables the neuron to perform non-linear integration as needed -- a subslice from network PathGSyn."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.StartN", IDName: "start-n", Doc: "StartN holds a starting offset index and a number of items\narranged from Start to Start+N (exclusive).\nThis is not 16 byte padded and only for use on CPU side.", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"pathparams"}}, {Tool: "gosl", Directive: "end", Args: []string{"pathparams"}}, {Tool: "gosl", Directive: "start", Args: []string{"pathparams"}}}, Fields: []types.Field{{Name: "Start", Doc: "starting offset"}, {Name: "N", Doc: "number of items --"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathIndexes", IDName: "path-indexes", Doc: "PathIndexes contains path-level index information into global memory arrays", Fields: []types.Field{{Name: "PathIndex"}, {Name: "RecvLay"}, {Name: "RecvNeurSt"}, {Name: "RecvNeurN"}, {Name: "SendLayer"}, {Name: "SendNeurSt"}, {Name: "SendNeurN"}, {Name: "SynapseSt"}, {Name: "SendConSt"}, {Name: "RecvConSt"}, {Name: "RecvSynSt"}, {Name: "GBufSt"}, {Name: "GSynSt"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GScaleValues", IDName: "g-scale-values", Doc: "GScaleValues holds the conductance scaling values.\nThese are computed once at start and remain constant thereafter,\nand therefore belong on Params and not on PathValues.", Fields: []types.Field{{Name: "Scale", Doc: "scaling factor for integrating synaptic input conductances (G's), originally computed as a function of sending layer activity and number of connections, and typically adapted from there -- see Path.PathScale adapt params"}, {Name: "Rel", Doc: "normalized relative proportion of total receiving conductance for this pathway: PathScale.Rel / sum(PathScale.Rel across relevant paths)"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathParams", IDName: "path-params", Doc: "PathParams contains all of the path parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []types.Field{{Name: "PathType", Doc: "functional type of path, which determines functional code path\nfor specialized layer types, and is synchronized with the Path.Typ value"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "Indexes", Doc: "recv and send neuron-level pathway index array access info"}, {Name: "Com", Doc: "synaptic communication parameters: delay, probability of failure"}, {Name: "PathScale", Doc: "pathway scaling parameters for computing GScale:\nmodulates overall strength of pathway, using both\nabsolute and relative factors, with adaptation option to maintain target max conductances"}, {Name: "SWts", Doc: "slowly adapting, structural weight value parameters,\nwhich control initial weight values and slower outer-loop adjustments"}, {Name: "Learn", Doc: "synaptic-level learning parameters for learning in the fast LWt values."}, {Name: "GScale", Doc: "conductance scaling values"}, {Name: "RLPred", Doc: "Params for RWPath and TDPredPath for doing dopamine-modulated learning\nfor reward prediction: Da * Send activity.\nUse in RWPredLayer or TDPredLayer typically to generate reward predictions.\nIf the Da sign is positive, the first recv unit learns fully; for negative,\nsecond one learns fully.\nLower lrate applies for opposite cases.  Weights are positive-only."}, {Name: "Matrix", Doc: "for trace-based learning in the MatrixPath. A trace of synaptic co-activity\nis formed, and then modulated by dopamine whenever it occurs.\nThis bridges the temporal gap between gating activity and subsequent activity,\nand is based biologically on synaptic tags.\nTrace is reset at time of reward based on ACh level from CINs."}, {Name: "BLA", Doc: "Basolateral Amygdala pathway parameters."}, {Name: "Hip", Doc: "Hip bench parameters."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PathTypes", IDName: "path-types", Doc: "PathTypes is an axon-specific path type enum,\nthat encompasses all the different algorithm types supported.\nClass parameter styles automatically key off of these types.\nThe first entries must be kept synchronized with the emer.PathType."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.MatrixParams", IDName: "matrix-params", Doc: "MatrixParams has parameters for BG Striatum Matrix MSN layers\nThese are the main Go / NoGo gating units in BG.\nDA, ACh learning rate modulation is pre-computed on the recv neuron\nRLRate variable via NeuroMod.  Also uses Pool.Gated for InvertNoGate,\nupdated in PlusPhase prior to DWt call.\nMust set Learn.NeuroMod.DAMod = D1Mod or D2Mod via SetBuildConfig(\"DAMod\").", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pcore_layers"}}}, Fields: []types.Field{{Name: "GateThr", Doc: "threshold on layer Avg SpkMax for Matrix Go and VThal layers to count as having gated"}, {Name: "IsVS", Doc: "is this a ventral striatum (VS) matrix layer?  if true, the gating status of this layer is recorded in the Global state, and used for updating effort and other factors."}, {Name: "OtherMatrixIndex", Doc: "index of other matrix (Go if we are NoGo and vice-versa).    Set during Build from BuildConfig OtherMatrixName"}, {Name: "ThalLay1Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay2Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay2Name if present -- -1 if not used"}, {Name: "ThalLay3Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay3Name if present -- -1 if not used"}, {Name: "ThalLay4Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay4Name if present -- -1 if not used"}, {Name: "ThalLay5Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay5Name if present -- -1 if not used"}, {Name: "ThalLay6Index", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay6Name if present -- -1 if not used"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPLayerTypes", IDName: "gp-layer-types", Doc: "GPLayerTypes is a GPLayer axon-specific layer type enum."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GPParams", IDName: "gp-params", Doc: "GPLayer represents a globus pallidus layer, including:\nGPePr, GPeAk (arkypallidal), and GPi (see GPType for type).\nTypically just a single unit per Pool representing a given stripe.", Fields: []types.Field{{Name: "GPType", Doc: "type of GP Layer -- must set during config using SetBuildConfig of GPType."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.MatrixPathParams", IDName: "matrix-path-params", Doc: "MatrixPathParams for trace-based learning in the MatrixPath.\nA trace of synaptic co-activity is formed, and then modulated by dopamine\nwhenever it occurs.  This bridges the temporal gap between gating activity\nand subsequent activity, and is based biologically on synaptic tags.\nTrace is applied to DWt and reset at the time of reward.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pcore_paths"}}}, Fields: []types.Field{{Name: "Credit", Doc: "proportion of trace activity driven by the basic credit assignment factor\nbased on the PF modulatory inputs and activity of the receiving neuron,\nrelative to the delta factor which is generally going to be smaller\nbecause it is an activity difference."}, {Name: "BasePF", Doc: "baseline amount of PF activity that modulates credit assignment learning,\nfor neurons with zero PF modulatory activity.\nThese were not part of the actual motor action, but can still get some\nsmaller amount of credit learning."}, {Name: "Delta", Doc: "weight for trace activity that is a function of the minus-plus delta\nactivity signal on the receiving MSN neuron, independent of PF modulation.\nThis should always be 1 except for testing disabling: adjust NonDelta\nrelative to it, and the overall learning rate."}, {Name: "VSRewLearn", Doc: "for ventral striatum, learn based on activity at time of reward,\nin inverse proportion to the GoalMaint activity: i.e., if there was no\ngoal maintenance, learn at reward to encourage goal engagement next time,\nbut otherwise, do not further reinforce at time of reward, because the\nactual goal gating learning trace is a better learning signal.\nOtherwise, only uses accumulated trace but doesn't include rew-time activity,\ne.g., for testing cases that do not have GoalMaint."}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxPhases", IDName: "avg-max-phases", Doc: "AvgMaxPhases contains the average and maximum values over a Pool of neurons,\nat different time scales within a standard ThetaCycle of updating.\nIt is much more efficient on the GPU to just grab everything in one pass at\nthe cycle level, and then take snapshots from there.\nAll of the cycle level values are updated at the *start* of the cycle\nbased on values from the prior cycle -- thus are 1 cycle behind in general.", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"pool"}}, {Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "start", Args: []string{"pool"}}}, Fields: []types.Field{{Name: "Cycle", Doc: "updated every cycle -- this is the source of all subsequent time scales"}, {Name: "Minus", Doc: "at the end of the minus phase"}, {Name: "Plus", Doc: "at the end of the plus phase"}, {Name: "Prev", Doc: "at the end of the previous plus phase"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.PoolAvgMax", IDName: "pool-avg-max", Doc: "PoolAvgMax contains the average and maximum values over a Pool of neurons\nfor different variables of interest, at Cycle, Minus and Plus phase timescales.\nAll of the cycle level values are updated at the *start* of the cycle\nbased on values from the prior cycle -- thus are 1 cycle behind in general.", Fields: []types.Field{{Name: "CaSpkP", Doc: "avg and maximum CaSpkP (continuously updated at roughly 40 msec integration window timescale, ends up capturing potentiation, plus-phase signal) -- this is the primary variable to use for tracking overall pool activity"}, {Name: "CaSpkD", Doc: "avg and maximum CaSpkD longer-term depression / DAPK1 signal in layer"}, {Name: "SpkMax", Doc: "avg and maximum SpkMax value (based on CaSpkP) -- reflects peak activity at any point across the cycle"}, {Name: "Act", Doc: "avg and maximum Act firing rate value"}, {Name: "GeInt", Doc: "avg and maximum GeInt integrated running-average excitatory conductance value"}, {Name: "GiInt", Doc: "avg and maximum GiInt integrated running-average inhibitory conductance value"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Pool", IDName: "pool", Doc: "Pool contains computed values for FS-FFFB inhibition,\nand various other state values for layers\nand pools (unit groups) that can be subject to inhibition", Directives: []types.Directive{{Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"pool"}}, {Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "start", Args: []string{"pool"}}}, Fields: []types.Field{{Name: "StIndex", Doc: "starting and ending (exlusive) layer-wise indexes for the list of neurons in this pool"}, {Name: "EdIndex", Doc: "starting and ending (exlusive) layer-wise indexes for the list of neurons in this pool"}, {Name: "LayIndex", Doc: "layer index in global layer list"}, {Name: "DataIndex", Doc: "data parallel index (innermost index per layer)"}, {Name: "PoolIndex", Doc: "pool index in global pool list:"}, {Name: "IsLayPool", Doc: "is this a layer-wide pool?  if not, it represents a sub-pool of units within a 4D layer"}, {Name: "Gated", Doc: "for special types where relevant (e.g., MatrixLayer, BGThalLayer), indicates if the pool was gated"}, {Name: "pad"}, {Name: "Inhib", Doc: "fast-slow FFFB inhibition values"}, {Name: "AvgMax", Doc: "average and max values for relevant variables in this pool, at different time scales"}, {Name: "AvgDif", Doc: "absolute value of AvgDif differences from actual neuron ActPct relative to TrgAvg"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RandFunIndex", IDName: "rand-fun-index", Directives: []types.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"axonrand"}}, {Tool: "gosl", Directive: "end", Args: []string{"axonrand"}}, {Tool: "gosl", Directive: "start", Args: []string{"axonrand"}}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWPredParams", IDName: "rw-pred-params", Doc: "RWPredParams parameterizes reward prediction for a simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rl_layers"}}}, Fields: []types.Field{{Name: "PredRange", Doc: "default 0.1..0.99 range of predictions that can be represented -- having a truncated range preserves some sensitivity in dopamine at the extremes of good or poor performance"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RWDaParams", IDName: "rw-da-params", Doc: "RWDaParams computes a dopamine (DA) signal using simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the Rubicon framework).", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "RWPredLayIndex", Doc: "idx of RWPredLayer to get reward prediction from -- set during Build from BuildConfig RWPredLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDIntegParams", IDName: "td-integ-params", Doc: "TDIntegParams are params for reward integrator layer", Fields: []types.Field{{Name: "Discount", Doc: "discount factor -- how much to discount the future prediction from TDPred"}, {Name: "PredGain", Doc: "gain factor on TD rew pred activations"}, {Name: "TDPredLayIndex", Doc: "idx of TDPredLayer to get reward prediction from -- set during Build from BuildConfig TDPredLayName"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.TDDaParams", IDName: "td-da-params", Doc: "TDDaParams are params for dopamine (DA) signal as the temporal difference (TD)\nbetween the TDIntegLayer activations in the minus and plus phase.", Fields: []types.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "TDIntegLayIndex", Doc: "idx of TDIntegLayer to get reward prediction from -- set during Build from BuildConfig TDIntegLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.RLPredPathParams", IDName: "rl-pred-path-params", Doc: "RLPredPathParams does dopamine-modulated learning for reward prediction: Da * Send.Act\nUsed by RWPath and TDPredPath within corresponding RWPredLayer or TDPredLayer\nto generate reward predictions based on its incoming weights, using linear activation\nfunction. Has no weight bounds or limits on sign etc.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rl_paths"}}}, Fields: []types.Field{{Name: "OppSignLRate", Doc: "how much to learn on opposite DA sign coding neuron (0..1)"}, {Name: "DaTol", Doc: "tolerance on DA -- if below this abs value, then DA goes to zero and there is no learning -- prevents prediction from exactly learning to cancel out reward value, retaining a residual valence of signal"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.DriveParams", IDName: "drive-params", Doc: "DriveParams manages the drive parameters for computing and updating drive state.\nMost of the params are for optional case where drives are automatically\nupdated based on US consumption (which satisfies drives) and time passing\n(which increases drives).", Fields: []types.Field{{Name: "DriveMin", Doc: "minimum effective drive value, which is an automatic baseline ensuring\nthat a positive US results in at least some minimal level of reward.\nUnlike Base values, this is not reflected in the activity of the drive\nvalues, and applies at the time of reward calculation as a minimum baseline."}, {Name: "Base", Doc: "baseline levels for each drive, which is what they naturally trend toward\nin the absence of any input.  Set inactive drives to 0 baseline,\nactive ones typically elevated baseline (0-1 range)."}, {Name: "Tau", Doc: "time constants in ThetaCycle (trial) units for natural update toward\nBase values. 0 values means no natural update (can be updated externally)."}, {Name: "Satisfaction", Doc: "decrement in drive value when US is consumed, thus partially satisfying\nthe drive. Positive values are subtracted from current Drive value."}, {Name: "Dt", Doc: "1/Tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.UrgencyParams", IDName: "urgency-params", Doc: "UrgencyParams has urgency (increasing pressure to do something)\nand parameters for updating it.\nRaw urgency integrates effort when _not_ goal engaged\nwhile effort (negative US 0) integrates when a goal _is_ engaged.", Fields: []types.Field{{Name: "U50", Doc: "value of raw urgency where the urgency activation level is 50%"}, {Name: "Power", Doc: "exponent on the urge factor -- valid numbers are 1,2,4,6"}, {Name: "Thr", Doc: "threshold for urge -- cuts off small baseline values"}, {Name: "DAtonic", Doc: "gain factor for driving tonic DA levels as a function of urgency"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.USParams", IDName: "us-params", Doc: "USParams control how positive and negative USs and Costs are\nweighted and integrated to compute an overall PV primary value.", Fields: []types.Field{{Name: "PVposGain", Doc: "gain factor applied to sum of weighted, drive-scaled positive USs\nto compute PVpos primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust the overall scaling of PVpos reward within 0-1\nnormalized range (see also PVnegGain).\nEach USpos is assumed to be in 0-1 range, with a default of 1."}, {Name: "PVnegGain", Doc: "gain factor applied to sum of weighted negative USs and Costs\nto compute PVneg primary summary value.\nThis is multiplied prior to 1/(1+x) normalization.\nUse this to adjust overall scaling of PVneg within 0-1\nnormalized range (see also PVposGain)."}, {Name: "USnegGains", Doc: "Negative US gain factor for encoding each individual negative US,\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the USneg pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see PVnegWts), and generally must be larger than the weights to leverage\nthe dynamic range within each US pool."}, {Name: "CostGains", Doc: "Cost gain factor for encoding the individual Time, Effort etc costs\nwithin their own separate input pools, multiplied prior to 1/(1+x)\nnormalization of each term for activating the Cost pools.\nThese gains are _not_ applied in computing summary PVneg value\n(see CostWts), and generally must be larger than the weights to use\nthe full dynamic range within each US pool."}, {Name: "PVposWts", Doc: "weight factor applied to each separate positive US on the way to computing\nthe overall PVpos summary value, to control the weighting of each US\nrelative to the others. Each pos US is also multiplied by its dynamic\nDrive factor as well.\nUse PVposGain to control the overall scaling of the PVpos value."}, {Name: "PVnegWts", Doc: "weight factor applied to each separate negative US on the way to computing\nthe overall PVneg summary value, to control the weighting of each US\nrelative to the others, and to the Costs.  These default to 1."}, {Name: "PVcostWts", Doc: "weight factor applied to each separate Cost (Time, Effort, etc) on the\nway to computing the overall PVneg summary value, to control the weighting\nof each Cost relative to the others, and relative to the negative USs.\nThe first pool is Time, second is Effort, and these are typically weighted\nlower (.02) than salient simulation-specific USs (1)."}, {Name: "USposEst", Doc: "computed estimated US values, based on OFCposPT and VSMatrix gating, in PVposEst"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LHbParams", IDName: "l-hb-params", Doc: "LHbParams has values for computing LHb & RMTg which drives dips / pauses in DA firing.\nLHb handles all US-related (PV = primary value) processing.\nPositive net LHb activity drives dips / pauses in VTA DA activity,\ne.g., when predicted pos > actual or actual neg > predicted.\nNegative net LHb activity drives bursts in VTA DA activity,\ne.g., when actual pos > predicted (redundant with LV / Amygdala)\nor \"relief\" burst when actual neg < predicted.", Fields: []types.Field{{Name: "VSPatchNonRewThr", Doc: "threshold on VSPatch prediction during a non-reward trial"}, {Name: "VSPatchGain", Doc: "gain on the VSPatchD1 - D2 difference to drive the net VSPatch DA\nprediction signal, which goes in VSPatchPos and RewPred global variables"}, {Name: "VSPatchVarTau", Doc: "decay time constant for computing the temporal variance in VSPatch\nvalues over time"}, {Name: "NegThr", Doc: "threshold factor that multiplies integrated pvNeg value\nto establish a threshold for whether the integrated pvPos value\nis good enough to drive overall net positive reward.\nIf pvPos wins, it is then multiplicatively discounted by pvNeg;\notherwise, pvNeg is discounted by pvPos."}, {Name: "BurstGain", Doc: "gain multiplier on PVpos for purposes of generating bursts\n(not for discounting negative dips)."}, {Name: "DipGain", Doc: "gain multiplier on PVneg for purposes of generating dips\n(not for discounting positive bursts)."}, {Name: "VSPatchVarDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.GiveUpParams", IDName: "give-up-params", Doc: "GiveUpParams are parameters for computing when to give up,\nbased on Utility, Timing and Progress factors.", Fields: []types.Field{{Name: "ProbThr", Doc: "threshold on GiveUp probability, below which no give up is triggered"}, {Name: "MinGiveUpSum", Doc: "minimum GiveUpSum value, which is the denominator in the sigmoidal function.\nThis minimum prevents division by zero and any other degenerate values."}, {Name: "Utility", Doc: "the factor multiplying utility values: cost and expected positive outcome"}, {Name: "Timing", Doc: "the factor multiplying timing values from VSPatch"}, {Name: "Progress", Doc: "the factor multiplying progress values based on time-integrated progress\ntoward the goal"}, {Name: "MinUtility", Doc: "minimum utility cost and reward estimate values -- when they are below\nthese levels (at the start) then utility is effectively neutral,\nso the other factors take precedence."}, {Name: "VSPatchSumMax", Doc: "maximum VSPatchPosSum for normalizing the value for give-up weighing"}, {Name: "VSPatchVarMax", Doc: "maximum VSPatchPosVar for normalizing the value for give-up weighing"}, {Name: "ProgressRateTau", Doc: "time constant for integrating the ProgressRate\nvalues over time"}, {Name: "ProgressRateDt", Doc: "1/tau"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.Rubicon", IDName: "rubicon", Doc: "Rubicon implements core elements of the Rubicon goal-directed motivational\nmodel, representing the core brainstem-level (hypothalamus) bodily drives\nand resulting dopamine from US (unconditioned stimulus) inputs,\nsubsuming the earlier Rubicon model of primary value (PV)\nand learned value (LV), describing the functions of the Amygala,\nVentral Striatum, VTA and associated midbrain nuclei (LDT, LHb, RMTg).\nCore LHb (lateral habenula) and VTA (ventral tegmental area) dopamine\nare computed in equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nThe Drives, Effort, US and resulting LHb PV dopamine computation all happens at the\nat the start of each trial (NewState, Step).  The LV / CS dopamine is computed\ncycle-by-cycle by the VTA layer using parameters set by the VTA layer.\nRenders USLayer, PVLayer, DrivesLayer representations based on state updated here.", Fields: []types.Field{{Name: "NPosUSs", Doc: "number of possible positive US states and corresponding drives.\nThe first is always reserved for novelty / curiosity.\nMust be set programmatically via SetNUSs method,\nwhich allocates corresponding parameters."}, {Name: "NNegUSs", Doc: "number of possible phasic negative US states (e.g., shock, impact etc).\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "NCosts", Doc: "number of possible costs, typically including accumulated time and effort costs.\nMust be set programmatically via SetNUSs method, which allocates corresponding\nparameters."}, {Name: "Drive", Doc: "parameters and state for built-in drives that form the core motivations\nof the agent, controlled by lateral hypothalamus and associated\nbody state monitoring such as glucose levels and thirst."}, {Name: "Urgency", Doc: "urgency (increasing pressure to do something) and parameters for\n updating it. Raw urgency is incremented by same units as effort,\nbut is only reset with a positive US."}, {Name: "USs", Doc: "controls how positive and negative USs are weighted and integrated to\ncompute an overall PV primary value."}, {Name: "LHb", Doc: "lateral habenula (LHb) parameters and state, which drives\ndipping / pausing in dopamine when the predicted positive\noutcome > actual, or actual negative outcome > predicted.\nCan also drive bursting for the converse, and via matrix phasic firing."}, {Name: "GiveUp", Doc: "parameters for giving up based on PV pos - neg difference"}, {Name: "ValDecode", Doc: "population code decoding parameters for estimates from layers"}, {Name: "decodeActs"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.LDTParams", IDName: "ldt-params", Doc: "LDTParams compute reward salience as ACh global neuromodulatory signal\nas a function of the MAX activation of its inputs from salience detecting\nlayers (e.g., the superior colliculus: SC), and whenever there is an external\nUS outcome input (signalled by the global GvHasRew flag).\nACh from salience inputs is discounted by GoalMaint activity,\nreducing distraction when pursuing a goal, but US ACh activity is not so reduced.\nACh modulates excitability of goal-gating layers.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rubicon_layers"}}}, Fields: []types.Field{{Name: "SrcThr", Doc: "threshold per input source, on absolute value (magnitude), to count as a significant reward event, which then drives maximal ACh -- set to 0 to disable this nonlinear behavior"}, {Name: "Rew", Doc: "use the global Context.NeuroMod.HasRew flag -- if there is some kind of external reward being given, then ACh goes to 1, else 0 for this component"}, {Name: "MaintInhib", Doc: "extent to which active goal maintenance (via Global GoalMaint)\ninhibits ACh signals: when goal engaged, distractability is lower."}, {Name: "SrcLay1Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay1Name if present -- -1 if not used"}, {Name: "SrcLay2Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay2Name if present -- -1 if not used"}, {Name: "SrcLay3Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay3Name if present -- -1 if not used"}, {Name: "SrcLay4Index", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay4Name if present -- -1 if not used"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.VTAParams", IDName: "vta-params", Doc: "VTAParams are for computing overall VTA DA based on LHb PVDA\n(primary value -- at US time, computed at start of each trial\nand stored in LHbPVDA global value)\nand Amygdala (CeM) CS / learned value (LV) activations, which update\nevery cycle.", Fields: []types.Field{{Name: "CeMGain", Doc: "gain on CeM activity difference (CeMPos - CeMNeg) for generating LV CS-driven dopamine values"}, {Name: "LHbGain", Doc: "gain on computed LHb DA (Burst - Dip) -- for controlling DA levels"}, {Name: "AChThr", Doc: "threshold on ACh level required to generate LV CS-driven dopamine burst"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.BLAPathParams", IDName: "bla-path-params", Doc: "BLAPathParams has parameters for basolateral amygdala learning.\nLearning is driven by the Tr trace as function of ACh * Send Act\nrecorded prior to US, and at US, recv unit delta: CaSpkP - SpkPrv\ntimes normalized GeIntNorm for recv unit credit assignment.\nThe Learn.Trace.Tau time constant determines trace updating over trials\nwhen ACh is above threshold -- this determines strength of second-order\nconditioning -- default of 1 means none, but can be increased as needed.", Directives: []types.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rubicon_paths"}}}, Fields: []types.Field{{Name: "NegDeltaLRate", Doc: "use 0.01 for acquisition (don't unlearn) and 1 for extinction -- negative delta learning rate multiplier"}, {Name: "AChThr", Doc: "threshold on this layer's ACh level for trace learning updates"}, {Name: "USTrace", Doc: "proportion of US time stimulus activity to use for the trace component of"}, {Name: "pad"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseVars", IDName: "synapse-vars", Doc: "SynapseVars are the neuron variables representing current synaptic state,\nspecifically weights."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseCaVars", IDName: "synapse-ca-vars", Doc: "SynapseCaVars are synapse variables for calcium involved in learning,\nwhich are data parallel input specific."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseIndexes", IDName: "synapse-indexes", Doc: "SynapseIndexes are the neuron indexes and other uint32 values (flags, etc).\nThere is only one of these per neuron -- not data parallel."})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseVarStrides", IDName: "synapse-var-strides", Doc: "SynapseVarStrides encodes the stride offsets for synapse variable access\ninto network float32 array.", Directives: []types.Directive{{Tool: "gosl", Directive: "end", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "end", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "start", Args: []string{"synapse"}}}, Fields: []types.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseCaStrides", IDName: "synapse-ca-strides", Doc: "SynapseCaStrides encodes the stride offsets for synapse variable access\ninto network float32 array.  Data is always the inner-most variable.", Fields: []types.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Var", Doc: "variable level"}}})

var _ = types.AddType(&types.Type{Name: "github.com/emer/axon/v2/axon.SynapseIndexStrides", IDName: "synapse-index-strides", Doc: "SynapseIndexStrides encodes the stride offsets for synapse index access\ninto network uint32 array.", Fields: []types.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Idx", Doc: "index value level"}, {Name: "pad"}, {Name: "pad1"}}})
