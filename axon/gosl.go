// Code generated by "gosl"; DO NOT EDIT

package axon

import (
	"embed"
	"unsafe"
	"cogentcore.org/core/gpu"
)

//go:embed shaders/*.wgsl
var shaders embed.FS

// ComputeGPU is the compute gpu device
var ComputeGPU *gpu.GPU

// UseGPU indicates whether to use GPU vs. CPU.
var UseGPU bool

// GPUSystem is a GPU compute System with kernels operating on the
// same set of data variables.
var GPUSystem *gpu.ComputeSystem

// GPUVars is an enum for GPU variables, for specifying what to sync.
type GPUVars int32 //enums:enum

const (
	LayersVar GPUVars = 0
	PathsVar GPUVars = 1
	NetworkIxsVar GPUVars = 2
	NeuronIxsVar GPUVars = 3
	SynapseIxsVar GPUVars = 4
	PathSendConVar GPUVars = 5
	RecvPathIxsVar GPUVars = 6
	PathRecvConVar GPUVars = 7
	RecvSynIxsVar GPUVars = 8
	CtxVar GPUVars = 9
	NeuronsVar GPUVars = 10
	NeuronAvgsVar GPUVars = 11
	LayerStatesVar GPUVars = 12
	GlobalScalarsVar GPUVars = 13
	GlobalVectorsVar GPUVars = 14
	ExtsVar GPUVars = 15
	PoolsVar GPUVars = 16
	PoolsIntVar GPUVars = 17
	PathGBufVar GPUVars = 18
	PathGSynsVar GPUVars = 19
	SynapsesVar GPUVars = 20
	SynapseTracesVar GPUVars = 21
)

// GPUInit initializes the GPU compute system,
// configuring system(s), variables and kernels.
// It is safe to call multiple times: detects if already run.
func GPUInit() {
	if ComputeGPU != nil {
		return
	}
	gp := gpu.NewComputeGPU()
	ComputeGPU = gp
	{
		sy := gpu.NewComputeSystem(gp, "Default")
		GPUSystem = sy
		gpu.NewComputePipelineShaderFS(shaders, "shaders/BetweenGi.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/CyclePost.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/MinusPhaseNeuron.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtFromDiSyn.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/GatherSpikes.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/PoolGi.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/CycleInc.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhaseStartNeuron.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhasePool.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtSyn.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/WtFromDWtSyn.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/LayerGi.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/ApplyExtsNeuron.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/GPUTestWrite.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtSubMeanPath.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/CycleNeuron.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/SendSpike.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/MinusPhasePool.wgsl", sy)
		gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhaseNeuron.wgsl", sy)
		vars := sy.Vars()
		{
			sgp := vars.AddGroup(gpu.Storage)
			var vr *gpu.Var
			_ = vr
			vr = sgp.AddStruct("Layers", int(unsafe.Sizeof(LayerParams{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("Paths", int(unsafe.Sizeof(PathParams{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage)
			var vr *gpu.Var
			_ = vr
			vr = sgp.AddStruct("NetworkIxs", int(unsafe.Sizeof(NetworkIndexes{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("NeuronIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("SynapseIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("PathSendCon", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("RecvPathIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("PathRecvCon", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("RecvSynIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage)
			var vr *gpu.Var
			_ = vr
			vr = sgp.AddStruct("Ctx", int(unsafe.Sizeof(Context{})), 1, gpu.ComputeShader)
			vr = sgp.Add("Neurons", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("NeuronAvgs", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("LayerStates", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("GlobalScalars", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("GlobalVectors", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Exts", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage)
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Pools", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("PoolsInt", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("PathGBuf", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("PathGSyns", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Synapses", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		sy.Config()
	}
}

// GPURelease releases the GPU compute system resources.
// Call this at program exit.
func GPURelease() {
	GPUSystem.Release()
	ComputeGPU.Release()
}

// RunDWtSyn runs the DWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtSyn call does Run and Done for a
// single run-and-sync case.
func RunDWtSyn(n int) {
	if UseGPU {
		RunDWtSynGPU(n)
	} else {
		RunDWtSynCPU(n)
	}
}

// RunDWtSynGPU runs the DWtSyn kernel on the GPU. See [RunDWtSyn] for more info.
func RunDWtSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtSynCPU runs the DWtSyn kernel on the CPU.
func RunDWtSynCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtSyn)
}

// RunOneDWtSyn runs the DWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtSynCPU(n)
	}
}
// RunWtFromDWtSyn runs the WtFromDWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneWtFromDWtSyn call does Run and Done for a
// single run-and-sync case.
func RunWtFromDWtSyn(n int) {
	if UseGPU {
		RunWtFromDWtSynGPU(n)
	} else {
		RunWtFromDWtSynCPU(n)
	}
}

// RunWtFromDWtSynGPU runs the WtFromDWtSyn kernel on the GPU. See [RunWtFromDWtSyn] for more info.
func RunWtFromDWtSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["WtFromDWtSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunWtFromDWtSynCPU runs the WtFromDWtSyn kernel on the CPU.
func RunWtFromDWtSynCPU(n int) {
	gpu.VectorizeFunc(0, n, WtFromDWtSyn)
}

// RunOneWtFromDWtSyn runs the WtFromDWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneWtFromDWtSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunWtFromDWtSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunWtFromDWtSynCPU(n)
	}
}
// RunLayerGi runs the LayerGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneLayerGi call does Run and Done for a
// single run-and-sync case.
func RunLayerGi(n int) {
	if UseGPU {
		RunLayerGiGPU(n)
	} else {
		RunLayerGiCPU(n)
	}
}

// RunLayerGiGPU runs the LayerGi kernel on the GPU. See [RunLayerGi] for more info.
func RunLayerGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["LayerGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunLayerGiCPU runs the LayerGi kernel on the CPU.
func RunLayerGiCPU(n int) {
	gpu.VectorizeFunc(0, n, LayerGi)
}

// RunOneLayerGi runs the LayerGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneLayerGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunLayerGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunLayerGiCPU(n)
	}
}
// RunApplyExtsNeuron runs the ApplyExtsNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneApplyExtsNeuron call does Run and Done for a
// single run-and-sync case.
func RunApplyExtsNeuron(n int) {
	if UseGPU {
		RunApplyExtsNeuronGPU(n)
	} else {
		RunApplyExtsNeuronCPU(n)
	}
}

// RunApplyExtsNeuronGPU runs the ApplyExtsNeuron kernel on the GPU. See [RunApplyExtsNeuron] for more info.
func RunApplyExtsNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["ApplyExtsNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunApplyExtsNeuronCPU runs the ApplyExtsNeuron kernel on the CPU.
func RunApplyExtsNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, ApplyExtsNeuron)
}

// RunOneApplyExtsNeuron runs the ApplyExtsNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneApplyExtsNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunApplyExtsNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunApplyExtsNeuronCPU(n)
	}
}
// RunGPUTestWrite runs the GPUTestWrite kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneGPUTestWrite call does Run and Done for a
// single run-and-sync case.
func RunGPUTestWrite(n int) {
	if UseGPU {
		RunGPUTestWriteGPU(n)
	} else {
		RunGPUTestWriteCPU(n)
	}
}

// RunGPUTestWriteGPU runs the GPUTestWrite kernel on the GPU. See [RunGPUTestWrite] for more info.
func RunGPUTestWriteGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["GPUTestWrite"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunGPUTestWriteCPU runs the GPUTestWrite kernel on the CPU.
func RunGPUTestWriteCPU(n int) {
	gpu.VectorizeFunc(0, n, GPUTestWrite)
}

// RunOneGPUTestWrite runs the GPUTestWrite kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneGPUTestWrite(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunGPUTestWriteGPU(n)
		RunDone(syncVars...)
	} else {
		RunGPUTestWriteCPU(n)
	}
}
// RunDWtSubMeanPath runs the DWtSubMeanPath kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtSubMeanPath call does Run and Done for a
// single run-and-sync case.
func RunDWtSubMeanPath(n int) {
	if UseGPU {
		RunDWtSubMeanPathGPU(n)
	} else {
		RunDWtSubMeanPathCPU(n)
	}
}

// RunDWtSubMeanPathGPU runs the DWtSubMeanPath kernel on the GPU. See [RunDWtSubMeanPath] for more info.
func RunDWtSubMeanPathGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtSubMeanPath"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtSubMeanPathCPU runs the DWtSubMeanPath kernel on the CPU.
func RunDWtSubMeanPathCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtSubMeanPath)
}

// RunOneDWtSubMeanPath runs the DWtSubMeanPath kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtSubMeanPath(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtSubMeanPathGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtSubMeanPathCPU(n)
	}
}
// RunCycleNeuron runs the CycleNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCycleNeuron call does Run and Done for a
// single run-and-sync case.
func RunCycleNeuron(n int) {
	if UseGPU {
		RunCycleNeuronGPU(n)
	} else {
		RunCycleNeuronCPU(n)
	}
}

// RunCycleNeuronGPU runs the CycleNeuron kernel on the GPU. See [RunCycleNeuron] for more info.
func RunCycleNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CycleNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCycleNeuronCPU runs the CycleNeuron kernel on the CPU.
func RunCycleNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, CycleNeuron)
}

// RunOneCycleNeuron runs the CycleNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCycleNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCycleNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunCycleNeuronCPU(n)
	}
}
// RunSendSpike runs the SendSpike kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSendSpike call does Run and Done for a
// single run-and-sync case.
func RunSendSpike(n int) {
	if UseGPU {
		RunSendSpikeGPU(n)
	} else {
		RunSendSpikeCPU(n)
	}
}

// RunSendSpikeGPU runs the SendSpike kernel on the GPU. See [RunSendSpike] for more info.
func RunSendSpikeGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SendSpike"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSendSpikeCPU runs the SendSpike kernel on the CPU.
func RunSendSpikeCPU(n int) {
	gpu.VectorizeFunc(0, n, SendSpike)
}

// RunOneSendSpike runs the SendSpike kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSendSpike(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSendSpikeGPU(n)
		RunDone(syncVars...)
	} else {
		RunSendSpikeCPU(n)
	}
}
// RunMinusPhasePool runs the MinusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMinusPhasePool call does Run and Done for a
// single run-and-sync case.
func RunMinusPhasePool(n int) {
	if UseGPU {
		RunMinusPhasePoolGPU(n)
	} else {
		RunMinusPhasePoolCPU(n)
	}
}

// RunMinusPhasePoolGPU runs the MinusPhasePool kernel on the GPU. See [RunMinusPhasePool] for more info.
func RunMinusPhasePoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MinusPhasePool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMinusPhasePoolCPU runs the MinusPhasePool kernel on the CPU.
func RunMinusPhasePoolCPU(n int) {
	gpu.VectorizeFunc(0, n, MinusPhasePool)
}

// RunOneMinusPhasePool runs the MinusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMinusPhasePool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMinusPhasePoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunMinusPhasePoolCPU(n)
	}
}
// RunPlusPhaseNeuron runs the PlusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhaseNeuron call does Run and Done for a
// single run-and-sync case.
func RunPlusPhaseNeuron(n int) {
	if UseGPU {
		RunPlusPhaseNeuronGPU(n)
	} else {
		RunPlusPhaseNeuronCPU(n)
	}
}

// RunPlusPhaseNeuronGPU runs the PlusPhaseNeuron kernel on the GPU. See [RunPlusPhaseNeuron] for more info.
func RunPlusPhaseNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhaseNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhaseNeuronCPU runs the PlusPhaseNeuron kernel on the CPU.
func RunPlusPhaseNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhaseNeuron)
}

// RunOnePlusPhaseNeuron runs the PlusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhaseNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhaseNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhaseNeuronCPU(n)
	}
}
// RunBetweenGi runs the BetweenGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneBetweenGi call does Run and Done for a
// single run-and-sync case.
func RunBetweenGi(n int) {
	if UseGPU {
		RunBetweenGiGPU(n)
	} else {
		RunBetweenGiCPU(n)
	}
}

// RunBetweenGiGPU runs the BetweenGi kernel on the GPU. See [RunBetweenGi] for more info.
func RunBetweenGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["BetweenGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunBetweenGiCPU runs the BetweenGi kernel on the CPU.
func RunBetweenGiCPU(n int) {
	gpu.VectorizeFunc(0, n, BetweenGi)
}

// RunOneBetweenGi runs the BetweenGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneBetweenGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunBetweenGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunBetweenGiCPU(n)
	}
}
// RunCyclePost runs the CyclePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCyclePost call does Run and Done for a
// single run-and-sync case.
func RunCyclePost(n int) {
	if UseGPU {
		RunCyclePostGPU(n)
	} else {
		RunCyclePostCPU(n)
	}
}

// RunCyclePostGPU runs the CyclePost kernel on the GPU. See [RunCyclePost] for more info.
func RunCyclePostGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CyclePost"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCyclePostCPU runs the CyclePost kernel on the CPU.
func RunCyclePostCPU(n int) {
	gpu.VectorizeFunc(0, n, CyclePost)
}

// RunOneCyclePost runs the CyclePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCyclePost(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCyclePostGPU(n)
		RunDone(syncVars...)
	} else {
		RunCyclePostCPU(n)
	}
}
// RunMinusPhaseNeuron runs the MinusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMinusPhaseNeuron call does Run and Done for a
// single run-and-sync case.
func RunMinusPhaseNeuron(n int) {
	if UseGPU {
		RunMinusPhaseNeuronGPU(n)
	} else {
		RunMinusPhaseNeuronCPU(n)
	}
}

// RunMinusPhaseNeuronGPU runs the MinusPhaseNeuron kernel on the GPU. See [RunMinusPhaseNeuron] for more info.
func RunMinusPhaseNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MinusPhaseNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMinusPhaseNeuronCPU runs the MinusPhaseNeuron kernel on the CPU.
func RunMinusPhaseNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, MinusPhaseNeuron)
}

// RunOneMinusPhaseNeuron runs the MinusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMinusPhaseNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMinusPhaseNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunMinusPhaseNeuronCPU(n)
	}
}
// RunPlusPhasePool runs the PlusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhasePool call does Run and Done for a
// single run-and-sync case.
func RunPlusPhasePool(n int) {
	if UseGPU {
		RunPlusPhasePoolGPU(n)
	} else {
		RunPlusPhasePoolCPU(n)
	}
}

// RunPlusPhasePoolGPU runs the PlusPhasePool kernel on the GPU. See [RunPlusPhasePool] for more info.
func RunPlusPhasePoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhasePool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhasePoolCPU runs the PlusPhasePool kernel on the CPU.
func RunPlusPhasePoolCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhasePool)
}

// RunOnePlusPhasePool runs the PlusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhasePool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhasePoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhasePoolCPU(n)
	}
}
// RunDWtFromDiSyn runs the DWtFromDiSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtFromDiSyn call does Run and Done for a
// single run-and-sync case.
func RunDWtFromDiSyn(n int) {
	if UseGPU {
		RunDWtFromDiSynGPU(n)
	} else {
		RunDWtFromDiSynCPU(n)
	}
}

// RunDWtFromDiSynGPU runs the DWtFromDiSyn kernel on the GPU. See [RunDWtFromDiSyn] for more info.
func RunDWtFromDiSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtFromDiSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtFromDiSynCPU runs the DWtFromDiSyn kernel on the CPU.
func RunDWtFromDiSynCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtFromDiSyn)
}

// RunOneDWtFromDiSyn runs the DWtFromDiSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtFromDiSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtFromDiSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtFromDiSynCPU(n)
	}
}
// RunGatherSpikes runs the GatherSpikes kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneGatherSpikes call does Run and Done for a
// single run-and-sync case.
func RunGatherSpikes(n int) {
	if UseGPU {
		RunGatherSpikesGPU(n)
	} else {
		RunGatherSpikesCPU(n)
	}
}

// RunGatherSpikesGPU runs the GatherSpikes kernel on the GPU. See [RunGatherSpikes] for more info.
func RunGatherSpikesGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["GatherSpikes"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunGatherSpikesCPU runs the GatherSpikes kernel on the CPU.
func RunGatherSpikesCPU(n int) {
	gpu.VectorizeFunc(0, n, GatherSpikes)
}

// RunOneGatherSpikes runs the GatherSpikes kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneGatherSpikes(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunGatherSpikesGPU(n)
		RunDone(syncVars...)
	} else {
		RunGatherSpikesCPU(n)
	}
}
// RunPoolGi runs the PoolGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePoolGi call does Run and Done for a
// single run-and-sync case.
func RunPoolGi(n int) {
	if UseGPU {
		RunPoolGiGPU(n)
	} else {
		RunPoolGiCPU(n)
	}
}

// RunPoolGiGPU runs the PoolGi kernel on the GPU. See [RunPoolGi] for more info.
func RunPoolGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PoolGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPoolGiCPU runs the PoolGi kernel on the CPU.
func RunPoolGiCPU(n int) {
	gpu.VectorizeFunc(0, n, PoolGi)
}

// RunOnePoolGi runs the PoolGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePoolGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPoolGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunPoolGiCPU(n)
	}
}
// RunCycleInc runs the CycleInc kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCycleInc call does Run and Done for a
// single run-and-sync case.
func RunCycleInc(n int) {
	if UseGPU {
		RunCycleIncGPU(n)
	} else {
		RunCycleIncCPU(n)
	}
}

// RunCycleIncGPU runs the CycleInc kernel on the GPU. See [RunCycleInc] for more info.
func RunCycleIncGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CycleInc"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCycleIncCPU runs the CycleInc kernel on the CPU.
func RunCycleIncCPU(n int) {
	gpu.VectorizeFunc(0, n, CycleInc)
}

// RunOneCycleInc runs the CycleInc kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCycleInc(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCycleIncGPU(n)
		RunDone(syncVars...)
	} else {
		RunCycleIncCPU(n)
	}
}
// RunPlusPhaseStartNeuron runs the PlusPhaseStartNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhaseStartNeuron call does Run and Done for a
// single run-and-sync case.
func RunPlusPhaseStartNeuron(n int) {
	if UseGPU {
		RunPlusPhaseStartNeuronGPU(n)
	} else {
		RunPlusPhaseStartNeuronCPU(n)
	}
}

// RunPlusPhaseStartNeuronGPU runs the PlusPhaseStartNeuron kernel on the GPU. See [RunPlusPhaseStartNeuron] for more info.
func RunPlusPhaseStartNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhaseStartNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhaseStartNeuronCPU runs the PlusPhaseStartNeuron kernel on the CPU.
func RunPlusPhaseStartNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhaseStartNeuron)
}

// RunOnePlusPhaseStartNeuron runs the PlusPhaseStartNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhaseStartNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhaseStartNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhaseStartNeuronCPU(n)
	}
}
// RunDone must be called after Run* calls to start compute kernels.
// This actually submits the kernel jobs to the GPU, and adds commands
// to synchronize the given variables back from the GPU to the CPU.
// After this function completes, the GPU results will be available in 
// the specified variables.
func RunDone(syncVars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.ComputeEncoder.End()
	ReadFromGPU(syncVars...)
	sy.EndComputePass()
	SyncFromGPU(syncVars...)
}

// ToGPU copies given variables to the GPU for the system.
func ToGPU(vars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			gpu.SetValueFrom(v, Layers)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			gpu.SetValueFrom(v, Paths)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			gpu.SetValueFrom(v, NetworkIxs)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			gpu.SetValueFrom(v, NeuronIxs.Values)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			gpu.SetValueFrom(v, SynapseIxs.Values)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			gpu.SetValueFrom(v, PathSendCon.Values)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			gpu.SetValueFrom(v, RecvPathIxs.Values)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			gpu.SetValueFrom(v, PathRecvCon.Values)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			gpu.SetValueFrom(v, RecvSynIxs.Values)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			gpu.SetValueFrom(v, Ctx)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			gpu.SetValueFrom(v, Neurons.Values)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			gpu.SetValueFrom(v, NeuronAvgs.Values)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			gpu.SetValueFrom(v, LayerStates.Values)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			gpu.SetValueFrom(v, GlobalScalars.Values)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			gpu.SetValueFrom(v, GlobalVectors.Values)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			gpu.SetValueFrom(v, Exts.Values)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(3, "Pools", 0)
			gpu.SetValueFrom(v, Pools.Values)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(3, "PoolsInt", 0)
			gpu.SetValueFrom(v, PoolsInt.Values)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			gpu.SetValueFrom(v, PathGBuf.Values)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			gpu.SetValueFrom(v, PathGSyns.Values)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			gpu.SetValueFrom(v, Synapses.Values)
		case SynapseTracesVar:
			v, _ := syVars.ValueByIndex(3, "SynapseTraces", 0)
			gpu.SetValueFrom(v, SynapseTraces.Values)
		}
	}
}

// ReadFromGPU starts the process of copying vars to the GPU.
func ReadFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			v.GPUToRead(sy.CommandEncoder)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			v.GPUToRead(sy.CommandEncoder)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			v.GPUToRead(sy.CommandEncoder)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			v.GPUToRead(sy.CommandEncoder)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(3, "Pools", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(3, "PoolsInt", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapseTracesVar:
			v, _ := syVars.ValueByIndex(3, "SynapseTraces", 0)
			v.GPUToRead(sy.CommandEncoder)
		}
	}
}

// SyncFromGPU synchronizes vars from the GPU to the actual variable.
func SyncFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Layers)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Paths)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NetworkIxs)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NeuronIxs.Values)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, SynapseIxs.Values)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathSendCon.Values)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, RecvPathIxs.Values)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathRecvCon.Values)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, RecvSynIxs.Values)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Ctx)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Neurons.Values)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NeuronAvgs.Values)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, LayerStates.Values)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, GlobalScalars.Values)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, GlobalVectors.Values)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Exts.Values)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(3, "Pools", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Pools.Values)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(3, "PoolsInt", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PoolsInt.Values)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathGBuf.Values)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathGSyns.Values)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Synapses.Values)
		case SynapseTracesVar:
			v, _ := syVars.ValueByIndex(3, "SynapseTraces", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, SynapseTraces.Values)
		}
	}
}

// GetLayers returns a pointer to the given global variable: 
// [Layers] []LayerParams at given index.
// To ensure that values are updated on the GPU, you must call [SetLayers].
// after all changes have been made.
func GetLayers(idx uint32) *LayerParams {
	return &Layers[idx]
}

// GetPaths returns a pointer to the given global variable: 
// [Paths] []PathParams at given index.
// To ensure that values are updated on the GPU, you must call [SetPaths].
// after all changes have been made.
func GetPaths(idx uint32) *PathParams {
	return &Paths[idx]
}

// GetNetworkIxs returns a pointer to the given global variable: 
// [NetworkIxs] []NetworkIndexes at given index.
// To ensure that values are updated on the GPU, you must call [SetNetworkIxs].
// after all changes have been made.
func GetNetworkIxs(idx uint32) *NetworkIndexes {
	return &NetworkIxs[idx]
}

// GetCtx returns a pointer to the given global variable: 
// [Ctx] []Context at given index.
// To ensure that values are updated on the GPU, you must call [SetCtx].
// after all changes have been made.
func GetCtx(idx uint32) *Context {
	return &Ctx[idx]
}
