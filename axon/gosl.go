// Code generated by "gosl"; DO NOT EDIT

package axon

import (
	"embed"
	"fmt"
	"math"
	"unsafe"
	"cogentcore.org/core/gpu"
	"cogentcore.org/lab/tensor"
)

//go:embed shaders/*.wgsl
var shaders embed.FS

var (
	// ComputeGPU is the compute gpu device
	ComputeGPU *gpu.GPU

	// UseGPU indicates whether to use GPU vs. CPU.
	UseGPU bool
)
// GPUSystem is a GPU compute System with kernels operating on the
// same set of data variables.
var GPUSystem *gpu.ComputeSystem

// GPUVars is an enum for GPU variables, for specifying what to sync.
type GPUVars int32 //enums:enum

const (
	LayersVar GPUVars = 0
	PathsVar GPUVars = 1
	NetworkIxsVar GPUVars = 2
	PoolIxsVar GPUVars = 3
	NeuronIxsVar GPUVars = 4
	SynapseIxsVar GPUVars = 5
	PathSendConVar GPUVars = 6
	RecvPathIxsVar GPUVars = 7
	PathRecvConVar GPUVars = 8
	RecvSynIxsVar GPUVars = 9
	CtxVar GPUVars = 10
	NeuronsVar GPUVars = 11
	NeuronAvgsVar GPUVars = 12
	LayerStatesVar GPUVars = 13
	GlobalScalarsVar GPUVars = 14
	GlobalVectorsVar GPUVars = 15
	ExtsVar GPUVars = 16
	PoolsVar GPUVars = 17
	PoolsIntVar GPUVars = 18
	PathGBufVar GPUVars = 19
	PathGSynsVar GPUVars = 20
	SynapsesVar GPUVars = 21
	SynapseTracesVar GPUVars = 22
)

// Tensor stride variables
var TensorStrides tensor.Uint32

// GPUInit initializes the GPU compute system,
// configuring system(s), variables and kernels.
// It is safe to call multiple times: detects if already run.
func GPUInit() {
	if ComputeGPU != nil {
		return
	}
	gp := gpu.NewComputeGPU()
	ComputeGPU = gp
	_ = fmt.Sprintf("%g",math.NaN()) // keep imports happy
	{
		sy := gpu.NewComputeSystem(gp, "Default")
		GPUSystem = sy
		vars := sy.Vars()
		{
			sgp := vars.AddGroup(gpu.Storage, "Params")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("TensorStrides", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("Layers", int(unsafe.Sizeof(LayerParams{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("Paths", int(unsafe.Sizeof(PathParams{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Indexes")
			var vr *gpu.Var
			_ = vr
			vr = sgp.AddStruct("NetworkIxs", int(unsafe.Sizeof(NetworkIndexes{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("PoolIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("NeuronIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("SynapseIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("PathSendCon", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("RecvPathIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("PathRecvCon", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.Add("RecvSynIxs", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Neurons")
			var vr *gpu.Var
			_ = vr
			vr = sgp.AddStruct("Ctx", int(unsafe.Sizeof(Context{})), 1, gpu.ComputeShader)
			vr = sgp.Add("Neurons", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("NeuronAvgs", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("LayerStates", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("GlobalScalars", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("GlobalVectors", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Exts", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Pools", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("PoolsInt", gpu.Int32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Synapse")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("PathGBuf", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("PathGSyns", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Synapses", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces0", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces1", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces2", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces3", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces4", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces5", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("SynapseTraces6", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		var pl *gpu.ComputePipeline
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/AdaptGiLayer.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/ApplyExtsNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "Exts")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/Beta1Neuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/Beta2Neuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/BetweenGi.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "Pools")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/CycleInc.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/CycleNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(2, "GlobalVectors")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/CyclePost.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(2, "GlobalVectors")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtFromDiSyn.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "SynapseIxs")
		pl.AddVarUsed(3, "SynapseTraces0")
		pl.AddVarUsed(3, "SynapseTraces1")
		pl.AddVarUsed(3, "SynapseTraces2")
		pl.AddVarUsed(3, "SynapseTraces3")
		pl.AddVarUsed(3, "SynapseTraces4")
		pl.AddVarUsed(3, "SynapseTraces5")
		pl.AddVarUsed(3, "SynapseTraces6")
		pl.AddVarUsed(3, "Synapses")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtSubMeanNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(1, "PathRecvCon")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "RecvPathIxs")
		pl.AddVarUsed(1, "RecvSynIxs")
		pl.AddVarUsed(3, "Synapses")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/DWtSyn.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(1, "SynapseIxs")
		pl.AddVarUsed(3, "SynapseTraces0")
		pl.AddVarUsed(3, "SynapseTraces1")
		pl.AddVarUsed(3, "SynapseTraces2")
		pl.AddVarUsed(3, "SynapseTraces3")
		pl.AddVarUsed(3, "SynapseTraces4")
		pl.AddVarUsed(3, "SynapseTraces5")
		pl.AddVarUsed(3, "SynapseTraces6")
		pl.AddVarUsed(3, "Synapses")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/GPUTestWrite.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/GatherSpikes.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(3, "PathGBuf")
		pl.AddVarUsed(3, "PathGSyns")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "PoolsInt")
		pl.AddVarUsed(1, "RecvPathIxs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/InitGBuffsPath.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(3, "PathGBuf")
		pl.AddVarUsed(3, "PathGSyns")
		pl.AddVarUsed(0, "Paths")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/LayerGi.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MinusPhaseNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MinusPhasePool.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/MinusPhasePost.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(2, "GlobalVectors")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/NewStateLayer.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/NewStateNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhaseNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhasePool.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhasePost.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(2, "GlobalVectors")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhaseStartContext.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PlusPhaseStartNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/PoolGi.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SendSpike.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(2, "GlobalScalars")
		pl.AddVarUsed(2, "GlobalVectors")
		pl.AddVarUsed(2, "LayerStates")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(3, "PathGBuf")
		pl.AddVarUsed(1, "PathSendCon")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(1, "SynapseIxs")
		pl.AddVarUsed(3, "Synapses")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SlowAdaptLayer.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(1, "PoolIxs")
		pl.AddVarUsed(2, "Pools")
		pl.AddVarUsed(2, "PoolsInt")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/SlowAdaptNeuron.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(1, "NeuronIxs")
		pl.AddVarUsed(1, "PathRecvCon")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "RecvPathIxs")
		pl.AddVarUsed(1, "RecvSynIxs")
		pl.AddVarUsed(3, "Synapses")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/WtFromDWtLayer.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(0, "Layers")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(2, "NeuronAvgs")
		pl.AddVarUsed(2, "Neurons")
		pl.AddVarUsed(1, "PoolIxs")
		pl = gpu.NewComputePipelineShaderFS(shaders, "shaders/WtFromDWtSyn.wgsl", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl.AddVarUsed(2, "Ctx")
		pl.AddVarUsed(1, "NetworkIxs")
		pl.AddVarUsed(0, "Paths")
		pl.AddVarUsed(1, "SynapseIxs")
		pl.AddVarUsed(3, "Synapses")
		sy.Config()
	}
}

// GPURelease releases the GPU compute system resources.
// Call this at program exit.
func GPURelease() {
	if GPUSystem != nil {
		GPUSystem.Release()
		GPUSystem = nil
	}

	if ComputeGPU != nil {
		ComputeGPU.Release()
		ComputeGPU = nil
	}
}

// RunAdaptGiLayer runs the AdaptGiLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneAdaptGiLayer call does Run and Done for a
// single run-and-sync case.
func RunAdaptGiLayer(n int) {
	if UseGPU {
		RunAdaptGiLayerGPU(n)
	} else {
		RunAdaptGiLayerCPU(n)
	}
}

// RunAdaptGiLayerGPU runs the AdaptGiLayer kernel on the GPU. See [RunAdaptGiLayer] for more info.
func RunAdaptGiLayerGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["AdaptGiLayer"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunAdaptGiLayerCPU runs the AdaptGiLayer kernel on the CPU.
func RunAdaptGiLayerCPU(n int) {
	gpu.VectorizeFunc(0, n, AdaptGiLayer)
}

// RunOneAdaptGiLayer runs the AdaptGiLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneAdaptGiLayer(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunAdaptGiLayerGPU(n)
		RunDone(syncVars...)
	} else {
		RunAdaptGiLayerCPU(n)
	}
}
// RunApplyExtsNeuron runs the ApplyExtsNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneApplyExtsNeuron call does Run and Done for a
// single run-and-sync case.
func RunApplyExtsNeuron(n int) {
	if UseGPU {
		RunApplyExtsNeuronGPU(n)
	} else {
		RunApplyExtsNeuronCPU(n)
	}
}

// RunApplyExtsNeuronGPU runs the ApplyExtsNeuron kernel on the GPU. See [RunApplyExtsNeuron] for more info.
func RunApplyExtsNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["ApplyExtsNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunApplyExtsNeuronCPU runs the ApplyExtsNeuron kernel on the CPU.
func RunApplyExtsNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, ApplyExtsNeuron)
}

// RunOneApplyExtsNeuron runs the ApplyExtsNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneApplyExtsNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunApplyExtsNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunApplyExtsNeuronCPU(n)
	}
}
// RunBeta1Neuron runs the Beta1Neuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneBeta1Neuron call does Run and Done for a
// single run-and-sync case.
func RunBeta1Neuron(n int) {
	if UseGPU {
		RunBeta1NeuronGPU(n)
	} else {
		RunBeta1NeuronCPU(n)
	}
}

// RunBeta1NeuronGPU runs the Beta1Neuron kernel on the GPU. See [RunBeta1Neuron] for more info.
func RunBeta1NeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["Beta1Neuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunBeta1NeuronCPU runs the Beta1Neuron kernel on the CPU.
func RunBeta1NeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, Beta1Neuron)
}

// RunOneBeta1Neuron runs the Beta1Neuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneBeta1Neuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunBeta1NeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunBeta1NeuronCPU(n)
	}
}
// RunBeta2Neuron runs the Beta2Neuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneBeta2Neuron call does Run and Done for a
// single run-and-sync case.
func RunBeta2Neuron(n int) {
	if UseGPU {
		RunBeta2NeuronGPU(n)
	} else {
		RunBeta2NeuronCPU(n)
	}
}

// RunBeta2NeuronGPU runs the Beta2Neuron kernel on the GPU. See [RunBeta2Neuron] for more info.
func RunBeta2NeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["Beta2Neuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunBeta2NeuronCPU runs the Beta2Neuron kernel on the CPU.
func RunBeta2NeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, Beta2Neuron)
}

// RunOneBeta2Neuron runs the Beta2Neuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneBeta2Neuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunBeta2NeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunBeta2NeuronCPU(n)
	}
}
// RunBetweenGi runs the BetweenGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneBetweenGi call does Run and Done for a
// single run-and-sync case.
func RunBetweenGi(n int) {
	if UseGPU {
		RunBetweenGiGPU(n)
	} else {
		RunBetweenGiCPU(n)
	}
}

// RunBetweenGiGPU runs the BetweenGi kernel on the GPU. See [RunBetweenGi] for more info.
func RunBetweenGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["BetweenGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunBetweenGiCPU runs the BetweenGi kernel on the CPU.
func RunBetweenGiCPU(n int) {
	gpu.VectorizeFunc(0, n, BetweenGi)
}

// RunOneBetweenGi runs the BetweenGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneBetweenGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunBetweenGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunBetweenGiCPU(n)
	}
}
// RunCycleInc runs the CycleInc kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCycleInc call does Run and Done for a
// single run-and-sync case.
func RunCycleInc(n int) {
	if UseGPU {
		RunCycleIncGPU(n)
	} else {
		RunCycleIncCPU(n)
	}
}

// RunCycleIncGPU runs the CycleInc kernel on the GPU. See [RunCycleInc] for more info.
func RunCycleIncGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CycleInc"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCycleIncCPU runs the CycleInc kernel on the CPU.
func RunCycleIncCPU(n int) {
	gpu.VectorizeFunc(0, n, CycleInc)
}

// RunOneCycleInc runs the CycleInc kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCycleInc(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCycleIncGPU(n)
		RunDone(syncVars...)
	} else {
		RunCycleIncCPU(n)
	}
}
// RunCycleNeuron runs the CycleNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCycleNeuron call does Run and Done for a
// single run-and-sync case.
func RunCycleNeuron(n int) {
	if UseGPU {
		RunCycleNeuronGPU(n)
	} else {
		RunCycleNeuronCPU(n)
	}
}

// RunCycleNeuronGPU runs the CycleNeuron kernel on the GPU. See [RunCycleNeuron] for more info.
func RunCycleNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CycleNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCycleNeuronCPU runs the CycleNeuron kernel on the CPU.
func RunCycleNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, CycleNeuron)
}

// RunOneCycleNeuron runs the CycleNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCycleNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCycleNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunCycleNeuronCPU(n)
	}
}
// RunCyclePost runs the CyclePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCyclePost call does Run and Done for a
// single run-and-sync case.
func RunCyclePost(n int) {
	if UseGPU {
		RunCyclePostGPU(n)
	} else {
		RunCyclePostCPU(n)
	}
}

// RunCyclePostGPU runs the CyclePost kernel on the GPU. See [RunCyclePost] for more info.
func RunCyclePostGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CyclePost"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCyclePostCPU runs the CyclePost kernel on the CPU.
func RunCyclePostCPU(n int) {
	gpu.VectorizeFunc(0, n, CyclePost)
}

// RunOneCyclePost runs the CyclePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCyclePost(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCyclePostGPU(n)
		RunDone(syncVars...)
	} else {
		RunCyclePostCPU(n)
	}
}
// RunDWtFromDiSyn runs the DWtFromDiSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtFromDiSyn call does Run and Done for a
// single run-and-sync case.
func RunDWtFromDiSyn(n int) {
	if UseGPU {
		RunDWtFromDiSynGPU(n)
	} else {
		RunDWtFromDiSynCPU(n)
	}
}

// RunDWtFromDiSynGPU runs the DWtFromDiSyn kernel on the GPU. See [RunDWtFromDiSyn] for more info.
func RunDWtFromDiSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtFromDiSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtFromDiSynCPU runs the DWtFromDiSyn kernel on the CPU.
func RunDWtFromDiSynCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtFromDiSyn)
}

// RunOneDWtFromDiSyn runs the DWtFromDiSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtFromDiSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtFromDiSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtFromDiSynCPU(n)
	}
}
// RunDWtSubMeanNeuron runs the DWtSubMeanNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtSubMeanNeuron call does Run and Done for a
// single run-and-sync case.
func RunDWtSubMeanNeuron(n int) {
	if UseGPU {
		RunDWtSubMeanNeuronGPU(n)
	} else {
		RunDWtSubMeanNeuronCPU(n)
	}
}

// RunDWtSubMeanNeuronGPU runs the DWtSubMeanNeuron kernel on the GPU. See [RunDWtSubMeanNeuron] for more info.
func RunDWtSubMeanNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtSubMeanNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtSubMeanNeuronCPU runs the DWtSubMeanNeuron kernel on the CPU.
func RunDWtSubMeanNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtSubMeanNeuron)
}

// RunOneDWtSubMeanNeuron runs the DWtSubMeanNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtSubMeanNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtSubMeanNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtSubMeanNeuronCPU(n)
	}
}
// RunDWtSyn runs the DWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDWtSyn call does Run and Done for a
// single run-and-sync case.
func RunDWtSyn(n int) {
	if UseGPU {
		RunDWtSynGPU(n)
	} else {
		RunDWtSynCPU(n)
	}
}

// RunDWtSynGPU runs the DWtSyn kernel on the GPU. See [RunDWtSyn] for more info.
func RunDWtSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DWtSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDWtSynCPU runs the DWtSyn kernel on the CPU.
func RunDWtSynCPU(n int) {
	gpu.VectorizeFunc(0, n, DWtSyn)
}

// RunOneDWtSyn runs the DWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDWtSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDWtSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunDWtSynCPU(n)
	}
}
// RunGPUTestWrite runs the GPUTestWrite kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneGPUTestWrite call does Run and Done for a
// single run-and-sync case.
func RunGPUTestWrite(n int) {
	if UseGPU {
		RunGPUTestWriteGPU(n)
	} else {
		RunGPUTestWriteCPU(n)
	}
}

// RunGPUTestWriteGPU runs the GPUTestWrite kernel on the GPU. See [RunGPUTestWrite] for more info.
func RunGPUTestWriteGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["GPUTestWrite"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunGPUTestWriteCPU runs the GPUTestWrite kernel on the CPU.
func RunGPUTestWriteCPU(n int) {
	gpu.VectorizeFunc(0, n, GPUTestWrite)
}

// RunOneGPUTestWrite runs the GPUTestWrite kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneGPUTestWrite(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunGPUTestWriteGPU(n)
		RunDone(syncVars...)
	} else {
		RunGPUTestWriteCPU(n)
	}
}
// RunGatherSpikes runs the GatherSpikes kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneGatherSpikes call does Run and Done for a
// single run-and-sync case.
func RunGatherSpikes(n int) {
	if UseGPU {
		RunGatherSpikesGPU(n)
	} else {
		RunGatherSpikesCPU(n)
	}
}

// RunGatherSpikesGPU runs the GatherSpikes kernel on the GPU. See [RunGatherSpikes] for more info.
func RunGatherSpikesGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["GatherSpikes"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunGatherSpikesCPU runs the GatherSpikes kernel on the CPU.
func RunGatherSpikesCPU(n int) {
	gpu.VectorizeFunc(0, n, GatherSpikes)
}

// RunOneGatherSpikes runs the GatherSpikes kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneGatherSpikes(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunGatherSpikesGPU(n)
		RunDone(syncVars...)
	} else {
		RunGatherSpikesCPU(n)
	}
}
// RunInitGBuffsPath runs the InitGBuffsPath kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneInitGBuffsPath call does Run and Done for a
// single run-and-sync case.
func RunInitGBuffsPath(n int) {
	if UseGPU {
		RunInitGBuffsPathGPU(n)
	} else {
		RunInitGBuffsPathCPU(n)
	}
}

// RunInitGBuffsPathGPU runs the InitGBuffsPath kernel on the GPU. See [RunInitGBuffsPath] for more info.
func RunInitGBuffsPathGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["InitGBuffsPath"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunInitGBuffsPathCPU runs the InitGBuffsPath kernel on the CPU.
func RunInitGBuffsPathCPU(n int) {
	gpu.VectorizeFunc(0, n, InitGBuffsPath)
}

// RunOneInitGBuffsPath runs the InitGBuffsPath kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneInitGBuffsPath(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunInitGBuffsPathGPU(n)
		RunDone(syncVars...)
	} else {
		RunInitGBuffsPathCPU(n)
	}
}
// RunLayerGi runs the LayerGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneLayerGi call does Run and Done for a
// single run-and-sync case.
func RunLayerGi(n int) {
	if UseGPU {
		RunLayerGiGPU(n)
	} else {
		RunLayerGiCPU(n)
	}
}

// RunLayerGiGPU runs the LayerGi kernel on the GPU. See [RunLayerGi] for more info.
func RunLayerGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["LayerGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunLayerGiCPU runs the LayerGi kernel on the CPU.
func RunLayerGiCPU(n int) {
	gpu.VectorizeFunc(0, n, LayerGi)
}

// RunOneLayerGi runs the LayerGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneLayerGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunLayerGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunLayerGiCPU(n)
	}
}
// RunMinusPhaseNeuron runs the MinusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMinusPhaseNeuron call does Run and Done for a
// single run-and-sync case.
func RunMinusPhaseNeuron(n int) {
	if UseGPU {
		RunMinusPhaseNeuronGPU(n)
	} else {
		RunMinusPhaseNeuronCPU(n)
	}
}

// RunMinusPhaseNeuronGPU runs the MinusPhaseNeuron kernel on the GPU. See [RunMinusPhaseNeuron] for more info.
func RunMinusPhaseNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MinusPhaseNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMinusPhaseNeuronCPU runs the MinusPhaseNeuron kernel on the CPU.
func RunMinusPhaseNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, MinusPhaseNeuron)
}

// RunOneMinusPhaseNeuron runs the MinusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMinusPhaseNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMinusPhaseNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunMinusPhaseNeuronCPU(n)
	}
}
// RunMinusPhasePool runs the MinusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMinusPhasePool call does Run and Done for a
// single run-and-sync case.
func RunMinusPhasePool(n int) {
	if UseGPU {
		RunMinusPhasePoolGPU(n)
	} else {
		RunMinusPhasePoolCPU(n)
	}
}

// RunMinusPhasePoolGPU runs the MinusPhasePool kernel on the GPU. See [RunMinusPhasePool] for more info.
func RunMinusPhasePoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MinusPhasePool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMinusPhasePoolCPU runs the MinusPhasePool kernel on the CPU.
func RunMinusPhasePoolCPU(n int) {
	gpu.VectorizeFunc(0, n, MinusPhasePool)
}

// RunOneMinusPhasePool runs the MinusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMinusPhasePool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMinusPhasePoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunMinusPhasePoolCPU(n)
	}
}
// RunMinusPhasePost runs the MinusPhasePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneMinusPhasePost call does Run and Done for a
// single run-and-sync case.
func RunMinusPhasePost(n int) {
	if UseGPU {
		RunMinusPhasePostGPU(n)
	} else {
		RunMinusPhasePostCPU(n)
	}
}

// RunMinusPhasePostGPU runs the MinusPhasePost kernel on the GPU. See [RunMinusPhasePost] for more info.
func RunMinusPhasePostGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["MinusPhasePost"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunMinusPhasePostCPU runs the MinusPhasePost kernel on the CPU.
func RunMinusPhasePostCPU(n int) {
	gpu.VectorizeFunc(0, n, MinusPhasePost)
}

// RunOneMinusPhasePost runs the MinusPhasePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneMinusPhasePost(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunMinusPhasePostGPU(n)
		RunDone(syncVars...)
	} else {
		RunMinusPhasePostCPU(n)
	}
}
// RunNewStateLayer runs the NewStateLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneNewStateLayer call does Run and Done for a
// single run-and-sync case.
func RunNewStateLayer(n int) {
	if UseGPU {
		RunNewStateLayerGPU(n)
	} else {
		RunNewStateLayerCPU(n)
	}
}

// RunNewStateLayerGPU runs the NewStateLayer kernel on the GPU. See [RunNewStateLayer] for more info.
func RunNewStateLayerGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["NewStateLayer"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunNewStateLayerCPU runs the NewStateLayer kernel on the CPU.
func RunNewStateLayerCPU(n int) {
	gpu.VectorizeFunc(0, n, NewStateLayer)
}

// RunOneNewStateLayer runs the NewStateLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneNewStateLayer(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunNewStateLayerGPU(n)
		RunDone(syncVars...)
	} else {
		RunNewStateLayerCPU(n)
	}
}
// RunNewStateNeuron runs the NewStateNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneNewStateNeuron call does Run and Done for a
// single run-and-sync case.
func RunNewStateNeuron(n int) {
	if UseGPU {
		RunNewStateNeuronGPU(n)
	} else {
		RunNewStateNeuronCPU(n)
	}
}

// RunNewStateNeuronGPU runs the NewStateNeuron kernel on the GPU. See [RunNewStateNeuron] for more info.
func RunNewStateNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["NewStateNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunNewStateNeuronCPU runs the NewStateNeuron kernel on the CPU.
func RunNewStateNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, NewStateNeuron)
}

// RunOneNewStateNeuron runs the NewStateNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneNewStateNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunNewStateNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunNewStateNeuronCPU(n)
	}
}
// RunPlusPhaseNeuron runs the PlusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhaseNeuron call does Run and Done for a
// single run-and-sync case.
func RunPlusPhaseNeuron(n int) {
	if UseGPU {
		RunPlusPhaseNeuronGPU(n)
	} else {
		RunPlusPhaseNeuronCPU(n)
	}
}

// RunPlusPhaseNeuronGPU runs the PlusPhaseNeuron kernel on the GPU. See [RunPlusPhaseNeuron] for more info.
func RunPlusPhaseNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhaseNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhaseNeuronCPU runs the PlusPhaseNeuron kernel on the CPU.
func RunPlusPhaseNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhaseNeuron)
}

// RunOnePlusPhaseNeuron runs the PlusPhaseNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhaseNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhaseNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhaseNeuronCPU(n)
	}
}
// RunPlusPhasePool runs the PlusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhasePool call does Run and Done for a
// single run-and-sync case.
func RunPlusPhasePool(n int) {
	if UseGPU {
		RunPlusPhasePoolGPU(n)
	} else {
		RunPlusPhasePoolCPU(n)
	}
}

// RunPlusPhasePoolGPU runs the PlusPhasePool kernel on the GPU. See [RunPlusPhasePool] for more info.
func RunPlusPhasePoolGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhasePool"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhasePoolCPU runs the PlusPhasePool kernel on the CPU.
func RunPlusPhasePoolCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhasePool)
}

// RunOnePlusPhasePool runs the PlusPhasePool kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhasePool(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhasePoolGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhasePoolCPU(n)
	}
}
// RunPlusPhasePost runs the PlusPhasePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhasePost call does Run and Done for a
// single run-and-sync case.
func RunPlusPhasePost(n int) {
	if UseGPU {
		RunPlusPhasePostGPU(n)
	} else {
		RunPlusPhasePostCPU(n)
	}
}

// RunPlusPhasePostGPU runs the PlusPhasePost kernel on the GPU. See [RunPlusPhasePost] for more info.
func RunPlusPhasePostGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhasePost"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhasePostCPU runs the PlusPhasePost kernel on the CPU.
func RunPlusPhasePostCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhasePost)
}

// RunOnePlusPhasePost runs the PlusPhasePost kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhasePost(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhasePostGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhasePostCPU(n)
	}
}
// RunPlusPhaseStartContext runs the PlusPhaseStartContext kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhaseStartContext call does Run and Done for a
// single run-and-sync case.
func RunPlusPhaseStartContext(n int) {
	if UseGPU {
		RunPlusPhaseStartContextGPU(n)
	} else {
		RunPlusPhaseStartContextCPU(n)
	}
}

// RunPlusPhaseStartContextGPU runs the PlusPhaseStartContext kernel on the GPU. See [RunPlusPhaseStartContext] for more info.
func RunPlusPhaseStartContextGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhaseStartContext"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhaseStartContextCPU runs the PlusPhaseStartContext kernel on the CPU.
func RunPlusPhaseStartContextCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhaseStartContext)
}

// RunOnePlusPhaseStartContext runs the PlusPhaseStartContext kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhaseStartContext(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhaseStartContextGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhaseStartContextCPU(n)
	}
}
// RunPlusPhaseStartNeuron runs the PlusPhaseStartNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePlusPhaseStartNeuron call does Run and Done for a
// single run-and-sync case.
func RunPlusPhaseStartNeuron(n int) {
	if UseGPU {
		RunPlusPhaseStartNeuronGPU(n)
	} else {
		RunPlusPhaseStartNeuronCPU(n)
	}
}

// RunPlusPhaseStartNeuronGPU runs the PlusPhaseStartNeuron kernel on the GPU. See [RunPlusPhaseStartNeuron] for more info.
func RunPlusPhaseStartNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PlusPhaseStartNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPlusPhaseStartNeuronCPU runs the PlusPhaseStartNeuron kernel on the CPU.
func RunPlusPhaseStartNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, PlusPhaseStartNeuron)
}

// RunOnePlusPhaseStartNeuron runs the PlusPhaseStartNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePlusPhaseStartNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPlusPhaseStartNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunPlusPhaseStartNeuronCPU(n)
	}
}
// RunPoolGi runs the PoolGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOnePoolGi call does Run and Done for a
// single run-and-sync case.
func RunPoolGi(n int) {
	if UseGPU {
		RunPoolGiGPU(n)
	} else {
		RunPoolGiCPU(n)
	}
}

// RunPoolGiGPU runs the PoolGi kernel on the GPU. See [RunPoolGi] for more info.
func RunPoolGiGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["PoolGi"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunPoolGiCPU runs the PoolGi kernel on the CPU.
func RunPoolGiCPU(n int) {
	gpu.VectorizeFunc(0, n, PoolGi)
}

// RunOnePoolGi runs the PoolGi kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOnePoolGi(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunPoolGiGPU(n)
		RunDone(syncVars...)
	} else {
		RunPoolGiCPU(n)
	}
}
// RunSendSpike runs the SendSpike kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSendSpike call does Run and Done for a
// single run-and-sync case.
func RunSendSpike(n int) {
	if UseGPU {
		RunSendSpikeGPU(n)
	} else {
		RunSendSpikeCPU(n)
	}
}

// RunSendSpikeGPU runs the SendSpike kernel on the GPU. See [RunSendSpike] for more info.
func RunSendSpikeGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SendSpike"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSendSpikeCPU runs the SendSpike kernel on the CPU.
func RunSendSpikeCPU(n int) {
	gpu.VectorizeFunc(0, n, SendSpike)
}

// RunOneSendSpike runs the SendSpike kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSendSpike(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSendSpikeGPU(n)
		RunDone(syncVars...)
	} else {
		RunSendSpikeCPU(n)
	}
}
// RunSlowAdaptLayer runs the SlowAdaptLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSlowAdaptLayer call does Run and Done for a
// single run-and-sync case.
func RunSlowAdaptLayer(n int) {
	if UseGPU {
		RunSlowAdaptLayerGPU(n)
	} else {
		RunSlowAdaptLayerCPU(n)
	}
}

// RunSlowAdaptLayerGPU runs the SlowAdaptLayer kernel on the GPU. See [RunSlowAdaptLayer] for more info.
func RunSlowAdaptLayerGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SlowAdaptLayer"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSlowAdaptLayerCPU runs the SlowAdaptLayer kernel on the CPU.
func RunSlowAdaptLayerCPU(n int) {
	gpu.VectorizeFunc(0, n, SlowAdaptLayer)
}

// RunOneSlowAdaptLayer runs the SlowAdaptLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSlowAdaptLayer(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSlowAdaptLayerGPU(n)
		RunDone(syncVars...)
	} else {
		RunSlowAdaptLayerCPU(n)
	}
}
// RunSlowAdaptNeuron runs the SlowAdaptNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneSlowAdaptNeuron call does Run and Done for a
// single run-and-sync case.
func RunSlowAdaptNeuron(n int) {
	if UseGPU {
		RunSlowAdaptNeuronGPU(n)
	} else {
		RunSlowAdaptNeuronCPU(n)
	}
}

// RunSlowAdaptNeuronGPU runs the SlowAdaptNeuron kernel on the GPU. See [RunSlowAdaptNeuron] for more info.
func RunSlowAdaptNeuronGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["SlowAdaptNeuron"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunSlowAdaptNeuronCPU runs the SlowAdaptNeuron kernel on the CPU.
func RunSlowAdaptNeuronCPU(n int) {
	gpu.VectorizeFunc(0, n, SlowAdaptNeuron)
}

// RunOneSlowAdaptNeuron runs the SlowAdaptNeuron kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneSlowAdaptNeuron(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunSlowAdaptNeuronGPU(n)
		RunDone(syncVars...)
	} else {
		RunSlowAdaptNeuronCPU(n)
	}
}
// RunWtFromDWtLayer runs the WtFromDWtLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneWtFromDWtLayer call does Run and Done for a
// single run-and-sync case.
func RunWtFromDWtLayer(n int) {
	if UseGPU {
		RunWtFromDWtLayerGPU(n)
	} else {
		RunWtFromDWtLayerCPU(n)
	}
}

// RunWtFromDWtLayerGPU runs the WtFromDWtLayer kernel on the GPU. See [RunWtFromDWtLayer] for more info.
func RunWtFromDWtLayerGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["WtFromDWtLayer"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunWtFromDWtLayerCPU runs the WtFromDWtLayer kernel on the CPU.
func RunWtFromDWtLayerCPU(n int) {
	gpu.VectorizeFunc(0, n, WtFromDWtLayer)
}

// RunOneWtFromDWtLayer runs the WtFromDWtLayer kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneWtFromDWtLayer(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunWtFromDWtLayerGPU(n)
		RunDone(syncVars...)
	} else {
		RunWtFromDWtLayerCPU(n)
	}
}
// RunWtFromDWtSyn runs the WtFromDWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneWtFromDWtSyn call does Run and Done for a
// single run-and-sync case.
func RunWtFromDWtSyn(n int) {
	if UseGPU {
		RunWtFromDWtSynGPU(n)
	} else {
		RunWtFromDWtSynCPU(n)
	}
}

// RunWtFromDWtSynGPU runs the WtFromDWtSyn kernel on the GPU. See [RunWtFromDWtSyn] for more info.
func RunWtFromDWtSynGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["WtFromDWtSyn"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunWtFromDWtSynCPU runs the WtFromDWtSyn kernel on the CPU.
func RunWtFromDWtSynCPU(n int) {
	gpu.VectorizeFunc(0, n, WtFromDWtSyn)
}

// RunOneWtFromDWtSyn runs the WtFromDWtSyn kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneWtFromDWtSyn(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunWtFromDWtSynGPU(n)
		RunDone(syncVars...)
	} else {
		RunWtFromDWtSynCPU(n)
	}
}
// RunDone must be called after Run* calls to start compute kernels.
// This actually submits the kernel jobs to the GPU, and adds commands
// to synchronize the given variables back from the GPU to the CPU.
// After this function completes, the GPU results will be available in 
// the specified variables.
func RunDone(syncVars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.ComputeEncoder.End()
	ReadFromGPU(syncVars...)
	sy.EndComputePass()
	SyncFromGPU(syncVars...)
}

// ToGPU copies given variables to the GPU for the system.
func ToGPU(vars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			gpu.SetValueFrom(v, Layers)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			gpu.SetValueFrom(v, Paths)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			gpu.SetValueFrom(v, NetworkIxs)
		case PoolIxsVar:
			v, _ := syVars.ValueByIndex(1, "PoolIxs", 0)
			gpu.SetValueFrom(v, PoolIxs.Values)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			gpu.SetValueFrom(v, NeuronIxs.Values)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			gpu.SetValueFrom(v, SynapseIxs.Values)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			gpu.SetValueFrom(v, PathSendCon.Values)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			gpu.SetValueFrom(v, RecvPathIxs.Values)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			gpu.SetValueFrom(v, PathRecvCon.Values)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			gpu.SetValueFrom(v, RecvSynIxs.Values)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			gpu.SetValueFrom(v, Ctx)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			gpu.SetValueFrom(v, Neurons.Values)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			gpu.SetValueFrom(v, NeuronAvgs.Values)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			gpu.SetValueFrom(v, LayerStates.Values)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			gpu.SetValueFrom(v, GlobalScalars.Values)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			gpu.SetValueFrom(v, GlobalVectors.Values)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			gpu.SetValueFrom(v, Exts.Values)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(2, "Pools", 0)
			gpu.SetValueFrom(v, Pools.Values)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(2, "PoolsInt", 0)
			gpu.SetValueFrom(v, PoolsInt.Values)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			gpu.SetValueFrom(v, PathGBuf.Values)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			gpu.SetValueFrom(v, PathGSyns.Values)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			gpu.SetValueFrom(v, Synapses.Values)
		case SynapseTracesVar:
			bsz := 536870904
			n := SynapseTraces.Len()
			nb := int(math.Ceil(float64(n) / float64(bsz)))
			for bi := range nb {
				v, _ := syVars.ValueByIndex(3, fmt.Sprintf("SynapseTraces%d", bi), 0)
				st := bsz * bi
				ed := min(bsz * (bi+1), n)
				gpu.SetValueFrom(v, SynapseTraces.Values[st:ed])
			}
		}
	}
}
// RunGPUSync can be called to synchronize data between CPU and GPU.
// Any prior ToGPU* calls will execute to send data to the GPU,
// and any subsequent RunDone* calls will copy data back from the GPU.
func RunGPUSync() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.BeginComputePass()
}

// ToGPUTensorStrides gets tensor strides and starts copying to the GPU.
func ToGPUTensorStrides() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	TensorStrides.SetShapeSizes(190)
	TensorStrides.SetInt1D(PoolIxs.Shape().Strides[0], 0)
	TensorStrides.SetInt1D(PoolIxs.Shape().Strides[1], 1)
	TensorStrides.SetInt1D(NeuronIxs.Shape().Strides[0], 10)
	TensorStrides.SetInt1D(NeuronIxs.Shape().Strides[1], 11)
	TensorStrides.SetInt1D(SynapseIxs.Shape().Strides[0], 20)
	TensorStrides.SetInt1D(SynapseIxs.Shape().Strides[1], 21)
	TensorStrides.SetInt1D(PathSendCon.Shape().Strides[0], 30)
	TensorStrides.SetInt1D(PathSendCon.Shape().Strides[1], 31)
	TensorStrides.SetInt1D(RecvPathIxs.Shape().Strides[0], 40)
	TensorStrides.SetInt1D(PathRecvCon.Shape().Strides[0], 50)
	TensorStrides.SetInt1D(PathRecvCon.Shape().Strides[1], 51)
	TensorStrides.SetInt1D(RecvSynIxs.Shape().Strides[0], 60)
	TensorStrides.SetInt1D(Neurons.Shape().Strides[0], 70)
	TensorStrides.SetInt1D(Neurons.Shape().Strides[1], 71)
	TensorStrides.SetInt1D(Neurons.Shape().Strides[2], 72)
	TensorStrides.SetInt1D(NeuronAvgs.Shape().Strides[0], 80)
	TensorStrides.SetInt1D(NeuronAvgs.Shape().Strides[1], 81)
	TensorStrides.SetInt1D(LayerStates.Shape().Strides[0], 90)
	TensorStrides.SetInt1D(LayerStates.Shape().Strides[1], 91)
	TensorStrides.SetInt1D(LayerStates.Shape().Strides[2], 92)
	TensorStrides.SetInt1D(GlobalScalars.Shape().Strides[0], 100)
	TensorStrides.SetInt1D(GlobalScalars.Shape().Strides[1], 101)
	TensorStrides.SetInt1D(GlobalVectors.Shape().Strides[0], 110)
	TensorStrides.SetInt1D(GlobalVectors.Shape().Strides[1], 111)
	TensorStrides.SetInt1D(GlobalVectors.Shape().Strides[2], 112)
	TensorStrides.SetInt1D(Exts.Shape().Strides[0], 120)
	TensorStrides.SetInt1D(Exts.Shape().Strides[1], 121)
	TensorStrides.SetInt1D(Pools.Shape().Strides[0], 130)
	TensorStrides.SetInt1D(Pools.Shape().Strides[1], 131)
	TensorStrides.SetInt1D(Pools.Shape().Strides[2], 132)
	TensorStrides.SetInt1D(PoolsInt.Shape().Strides[0], 140)
	TensorStrides.SetInt1D(PoolsInt.Shape().Strides[1], 141)
	TensorStrides.SetInt1D(PoolsInt.Shape().Strides[2], 142)
	TensorStrides.SetInt1D(PathGBuf.Shape().Strides[0], 150)
	TensorStrides.SetInt1D(PathGBuf.Shape().Strides[1], 151)
	TensorStrides.SetInt1D(PathGBuf.Shape().Strides[2], 152)
	TensorStrides.SetInt1D(PathGSyns.Shape().Strides[0], 160)
	TensorStrides.SetInt1D(PathGSyns.Shape().Strides[1], 161)
	TensorStrides.SetInt1D(Synapses.Shape().Strides[0], 170)
	TensorStrides.SetInt1D(Synapses.Shape().Strides[1], 171)
	TensorStrides.SetInt1D(SynapseTraces.Shape().Strides[0], 180)
	TensorStrides.SetInt1D(SynapseTraces.Shape().Strides[1], 181)
	TensorStrides.SetInt1D(SynapseTraces.Shape().Strides[2], 182)
	v, _ := syVars.ValueByIndex(0, "TensorStrides", 0)
	gpu.SetValueFrom(v, TensorStrides.Values)
}

// ReadFromGPU starts the process of copying vars to the GPU.
func ReadFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PoolIxsVar:
			v, _ := syVars.ValueByIndex(1, "PoolIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			v.GPUToRead(sy.CommandEncoder)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			v.GPUToRead(sy.CommandEncoder)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			v.GPUToRead(sy.CommandEncoder)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			v.GPUToRead(sy.CommandEncoder)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			v.GPUToRead(sy.CommandEncoder)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(2, "Pools", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(2, "PoolsInt", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			v.GPUToRead(sy.CommandEncoder)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			v.GPUToRead(sy.CommandEncoder)
		case SynapseTracesVar:
			bsz := 536870904
			n := SynapseTraces.Len()
			nb := int(math.Ceil(float64(n) / float64(bsz)))
			for bi := range nb {
				v, _ := syVars.ValueByIndex(3, fmt.Sprintf("SynapseTraces%d", bi), 0)
				v.GPUToRead(sy.CommandEncoder)
			}
		}
	}
}

// SyncFromGPU synchronizes vars from the GPU to the actual variable.
func SyncFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case LayersVar:
			v, _ := syVars.ValueByIndex(0, "Layers", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Layers)
		case PathsVar:
			v, _ := syVars.ValueByIndex(0, "Paths", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Paths)
		case NetworkIxsVar:
			v, _ := syVars.ValueByIndex(1, "NetworkIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NetworkIxs)
		case PoolIxsVar:
			v, _ := syVars.ValueByIndex(1, "PoolIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PoolIxs.Values)
		case NeuronIxsVar:
			v, _ := syVars.ValueByIndex(1, "NeuronIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NeuronIxs.Values)
		case SynapseIxsVar:
			v, _ := syVars.ValueByIndex(1, "SynapseIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, SynapseIxs.Values)
		case PathSendConVar:
			v, _ := syVars.ValueByIndex(1, "PathSendCon", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathSendCon.Values)
		case RecvPathIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvPathIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, RecvPathIxs.Values)
		case PathRecvConVar:
			v, _ := syVars.ValueByIndex(1, "PathRecvCon", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathRecvCon.Values)
		case RecvSynIxsVar:
			v, _ := syVars.ValueByIndex(1, "RecvSynIxs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, RecvSynIxs.Values)
		case CtxVar:
			v, _ := syVars.ValueByIndex(2, "Ctx", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Ctx)
		case NeuronsVar:
			v, _ := syVars.ValueByIndex(2, "Neurons", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Neurons.Values)
		case NeuronAvgsVar:
			v, _ := syVars.ValueByIndex(2, "NeuronAvgs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, NeuronAvgs.Values)
		case LayerStatesVar:
			v, _ := syVars.ValueByIndex(2, "LayerStates", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, LayerStates.Values)
		case GlobalScalarsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalScalars", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, GlobalScalars.Values)
		case GlobalVectorsVar:
			v, _ := syVars.ValueByIndex(2, "GlobalVectors", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, GlobalVectors.Values)
		case ExtsVar:
			v, _ := syVars.ValueByIndex(2, "Exts", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Exts.Values)
		case PoolsVar:
			v, _ := syVars.ValueByIndex(2, "Pools", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Pools.Values)
		case PoolsIntVar:
			v, _ := syVars.ValueByIndex(2, "PoolsInt", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PoolsInt.Values)
		case PathGBufVar:
			v, _ := syVars.ValueByIndex(3, "PathGBuf", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathGBuf.Values)
		case PathGSynsVar:
			v, _ := syVars.ValueByIndex(3, "PathGSyns", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, PathGSyns.Values)
		case SynapsesVar:
			v, _ := syVars.ValueByIndex(3, "Synapses", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Synapses.Values)
		case SynapseTracesVar:
			bsz := 536870904
			n := SynapseTraces.Len()
			nb := int(math.Ceil(float64(n) / float64(bsz)))
			for bi := range nb {
				v, _ := syVars.ValueByIndex(3, fmt.Sprintf("SynapseTraces%d", bi), 0)
				v.ReadSync()
				st := bsz * bi
				ed := min(bsz * (bi+1), n)
				gpu.ReadToBytes(v, SynapseTraces.Values[st:ed])
			}
		}
	}
}

// GetLayers returns a pointer to the given global variable: 
// [Layers] []LayerParams at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetLayers(idx uint32) *LayerParams {
	return &Layers[idx]
}

// GetPaths returns a pointer to the given global variable: 
// [Paths] []PathParams at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetPaths(idx uint32) *PathParams {
	return &Paths[idx]
}

// GetNetworkIxs returns a pointer to the given global variable: 
// [NetworkIxs] []NetworkIndexes at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetNetworkIxs(idx uint32) *NetworkIndexes {
	return &NetworkIxs[idx]
}

// GetCtx returns a pointer to the given global variable: 
// [Ctx] []Context at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetCtx(idx uint32) *Context {
	return &Ctx[idx]
}
