// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"cogentcore.org/core/base/randx"
	"cogentcore.org/core/tensor"
)

// index naming:
// syi =  path-relative synapse index (per existing usage)
// syni = network-relative synapse index -- add SynStIndex to syi

// SetSWtsRPool initializes SWt structural weight values using given tensor
// of values which has unique values for each recv neuron within a given pool.
func (pt *Path) SetSWtsRPool(ctx *Context, swts tensor.Tensor) {
	rNuY := swts.DimSize(0)
	rNuX := swts.DimSize(1)
	rNu := rNuY * rNuX
	rfsz := swts.Len() / rNu

	rsh := pt.Recv.Shape
	rNpY := rsh.DimSize(0)
	rNpX := rsh.DimSize(1)
	r2d := false
	if rsh.NumDims() != 4 {
		r2d = true
		rNpY = 1
		rNpX = 1
	}

	wsz := swts.Len()

	for rpy := 0; rpy < rNpY; rpy++ {
		for rpx := 0; rpx < rNpX; rpx++ {
			for ruy := 0; ruy < rNuY; ruy++ {
				for rux := 0; rux < rNuX; rux++ {
					ri := 0
					if r2d {
						ri = rsh.IndexTo1D(ruy, rux)
					} else {
						ri = rsh.IndexTo1D(rpy, rpx, ruy, rux)
					}
					scst := (ruy*rNuX + rux) * rfsz
					syIndexes := pt.RecvSynIxs(uint32(ri))
					for ci, syi := range syIndexes {
						syni := pt.SynStIndex + syi
						swt := float32(swts.Float1D((scst + ci) % wsz))
						Synapses[SWt, syni] = float32(swt)
						wt := pt.Params.SWts.ClipWt(swt + (Synapses[Wt, syni] - pt.Params.SWts.Init.Mean))
						Synapses[Wt, syni] = wt
						Synapses[LWt, syni] = pt.Params.SWts.LWtFromWts(wt, swt)
					}
				}
			}
		}
	}
}

// SetWeightsFunc initializes synaptic Wt value using given function
// based on receiving and sending unit indexes.
// Strongly suggest calling SWtRescale after.
func (pt *Path) SetWeightsFunc(ctx *Context, wtFun func(si, ri int, send, recv *tensor.Shape) float32) {
	rsh := &pt.Recv.Shape
	rn := rsh.Len()
	ssh := &pt.Send.Shape

	for ri := 0; ri < rn; ri++ {
		syIndexes := pt.RecvSynIxs(uint32(ri))
		for _, syi := range syIndexes {
			syni := pt.SynStIndex + syi
			si := pt.Params.SynSendLayerIndex(syni)
			wt := wtFun(int(si), ri, ssh, rsh)
			Synapses[SWt, syni] = wt
			Synapses[Wt, syni] = wt
			Synapses[LWt, syni] = 0.5
		}
	}
}

// SetSWtsFunc initializes structural SWt values using given function
// based on receiving and sending unit indexes.
func (pt *Path) SetSWtsFunc(ctx *Context, swtFun func(si, ri int, send, recv *tensor.Shape) float32) {
	rsh := &pt.Recv.Shape
	rn := rsh.Len()
	ssh := &pt.Send.Shape

	for ri := 0; ri < rn; ri++ {
		syIndexes := pt.RecvSynIxs(uint32(ri))
		for _, syi := range syIndexes {
			syni := pt.SynStIndex + syi
			si := int(pt.Params.SynSendLayerIndex(syni))
			swt := swtFun(si, ri, ssh, rsh)
			Synapses[SWt, syni] = swt
			wt := pt.Params.SWts.ClipWt(swt + (Synapses[Wt, syni] - pt.Params.SWts.Init.Mean))
			Synapses[Wt, syni] = wt
			Synapses[LWt, syni] = pt.Params.SWts.LWtFromWts(wt, swt)
		}
	}
}

// InitWeightsSyn initializes weight values based on WtInit randomness parameters
// for an individual synapse.
// It also updates the linear weight value based on the sigmoidal weight value.
func (pt *Path) InitWeightsSyn(ctx *Context, syni uint32, rnd randx.Rand, mean, spct float32) {
	pt.Params.SWts.InitWeightsSyn(ctx, syni, rnd, mean, spct)
}

// InitWeights initializes weight values according to SWt params,
// enforcing current constraints.
func (pt *Path) InitWeights(ctx *Context, nt *Network) {
	pt.Params.Learn.LRate.Init()
	pt.Params.InitGBuffs()
	rlay := pt.Recv
	spct := pt.Params.SWts.Init.SPct
	if rlay.Params.IsTarget() {
		pt.Params.SWts.Init.SPct = 0
		spct = 0
	}
	smn := pt.Params.SWts.Init.Mean
	// todo: why is this recv based?  prob important to keep for consistency
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		ni := rlay.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		syIndexes := pt.RecvSynIxs(lni)
		for _, syi := range syIndexes {
			syni := pt.SynStIndex + syi
			pt.InitWeightsSyn(ctx, syni, &nt.Rand, smn, spct)
		}
	}
	if pt.Params.SWts.Adapt.On.IsTrue() && !rlay.Params.IsTarget() {
		pt.SWtRescale(ctx)
	}
}

// SWtRescale rescales the SWt values to preserve the target overall mean value,
// using subtractive normalization.
func (pt *Path) SWtRescale(ctx *Context) {
	rlay := pt.Recv
	smn := pt.Params.SWts.Init.Mean
	for lni := uint32(0); lni < rlay.NNeurons; lni++ {
		ni := rlay.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		var nmin, nmax int
		var sum float32
		syIndexes := pt.RecvSynIxs(lni)
		nCons := len(syIndexes)
		if nCons <= 1 {
			continue
		}
		for _, syi := range syIndexes {
			syni := pt.SynStIndex + syi
			swt := Synapses[SWt, syni]
			sum += swt
			if swt <= pt.Params.SWts.Limit.Min {
				nmin++
			} else if swt >= pt.Params.SWts.Limit.Max {
				nmax++
			}
		}
		amn := sum / float32(nCons)
		mdf := smn - amn // subtractive
		if mdf == 0 {
			continue
		}
		if mdf > 0 { // need to increase
			if nmax > 0 && nmax < nCons {
				amn = sum / float32(nCons-nmax)
				mdf = smn - amn
			}
			for _, syi := range syIndexes {
				syni := pt.SynStIndex + syi
				if Synapses[SWt, syni] <= pt.Params.SWts.Limit.Max {
					swt := pt.Params.SWts.ClipSWt(Synapses[SWt, syni] + mdf)
					Synapses[SWt, syni] = swt
					Synapses[Wt, syni] = pt.Params.SWts.WtValue(swt, Synapses[LWt, syni])
				}
			}
		} else {
			if nmin > 0 && nmin < nCons {
				amn = sum / float32(nCons-nmin)
				mdf = smn - amn
			}
			for _, syi := range syIndexes {
				syni := pt.SynStIndex + syi
				if Synapses[SWt, syni] >= pt.Params.SWts.Limit.Min {
					swt := pt.Params.SWts.ClipSWt(Synapses[SWt, syni] + mdf)
					Synapses[SWt, syni] = swt
					Synapses[Wt, syni] = pt.Params.SWts.WtValue(swt, Synapses[LWt, syni])
				}
			}
		}
	}
}

// sender based version:
//                     only looked at recv layers > send layers -- ri is "above" si
//  for:      ri    <- this is now sender on recip path: recipSi
//           ^ \    <- look for send back to original si, now as a receiver
//          /   v
// start: si == recipRi <- look in sy.RecvIndex of recipSi's sending cons for recipRi == si
//

// InitWtSym initializes weight symmetry.
// Is given the reciprocal pathway where
// the Send and Recv layers are reversed
// (see LayerBase RecipToRecvPath)
func (pt *Path) InitWtSym(ctx *Context, rpj *Path) {
	if len(rpj.SendCon) == 0 {
		return
	}
	slay := pt.Send
	for lni := uint32(0); lni < slay.NNeurons; lni++ {
		scon := pt.SendCon[lni]
		for syi := scon.Start; syi < scon.Start+scon.N; syi++ {
			syni := pt.SynStIndex + syi
			ri := pt.Params.SynRecvLayerIndex(syni) // <- this sends to me, ri
			recipSi := ri                           // reciprocal case is si is now receiver
			recipc := rpj.SendCon[recipSi]
			if recipc.N == 0 {
				continue
			}
			firstSyni := rpj.SynStIndex + recipc.Start
			lastSyni := rpj.SynStIndex + recipc.Start + recipc.N - 1
			firstRi := rpj.Params.SynRecvLayerIndex(firstSyni)
			lastRi := rpj.Params.SynRecvLayerIndex(lastSyni)
			if lni < firstRi || lni > lastRi { // fast reject -- paths are always in order!
				continue
			}
			// start at index proportional to ri relative to rist
			up := int32(0)
			if lastRi > firstRi {
				up = int32(float32(recipc.N) * float32(lni-firstRi) / float32(lastRi-firstRi))
			}
			dn := up - 1

			for {
				doing := false
				if up < int32(recipc.N) {
					doing = true
					recipCi := uint32(recipc.Start) + uint32(up)
					recipSyni := rpj.SynStIndex + recipCi
					recipRi := rpj.Params.SynRecvLayerIndex(recipSyni)
					if recipRi == lni {
						Synapses[Wt, recipSyni] = Synapses[Wt, syni]
						Synapses[LWt, recipSyni] = Synapses[LWt, syni]
						Synapses[SWt, recipSyni] = Synapses[SWt, syni]
						// note: if we support SymFromTop then can have option to go other way
						break
					}
					up++
				}
				if dn >= 0 {
					doing = true
					recipCi := uint32(recipc.Start) + uint32(dn)
					recipSyni := rpj.SynStIndex + recipCi
					recipRi := rpj.Params.SynRecvLayerIndex(recipSyni)
					if recipRi == lni {
						Synapses[Wt, recipSyni] = Synapses[Wt, syni]
						Synapses[LWt, recipSyni] = Synapses[LWt, syni]
						Synapses[SWt, recipSyni] = Synapses[SWt, syni]
						// note: if we support SymFromTop then can have option to go other way
						break
					}
					dn--
				}
				if !doing {
					break
				}
			}
		}
	}
}
