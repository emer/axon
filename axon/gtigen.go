// Code generated by "core generate -add-types"; DO NOT EDIT.

package axon

import (
	"cogentcore.org/core/gti"
)

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SpikeParams", IDName: "spike-params", Doc: "SpikeParams contains spiking activation function params.\nImplements a basic thresholded Vm model, and optionally\nthe AdEx adaptive exponential function (adapt is KNaAdapt)", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"act"}}, {Tool: "gosl", Directive: "end", Args: []string{"act"}}, {Tool: "gosl", Directive: "start", Args: []string{"act"}}}, Fields: []gti.Field{{Name: "Thr", Doc: "threshold value Theta (Q) for firing output activation (.5 is more accurate value based on AdEx biological parameters and normalization"}, {Name: "VmR", Doc: "post-spiking membrane potential to reset to, produces refractory effect if lower than VmInit -- 0.3 is apropriate biologically-based value for AdEx (Brette & Gurstner, 2005) parameters.  See also RTau"}, {Name: "Tr", Doc: "post-spiking explicit refractory period, in cycles -- prevents Vm updating for this number of cycles post firing -- Vm is reduced in exponential steps over this period according to RTau, being fixed at Tr to VmR exactly"}, {Name: "RTau", Doc: "time constant for decaying Vm down to VmR -- at end of Tr it is set to VmR exactly -- this provides a more realistic shape of the post-spiking Vm which is only relevant for more realistic channels that key off of Vm -- does not otherwise affect standard computation"}, {Name: "Exp", Doc: "if true, turn on exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpSlope", Doc: "slope in Vm (2 mV = .02 in normalized units) for extra exponential excitatory current that drives Vm rapidly upward for spiking as it gets past its nominal firing threshold (Thr) -- nicely captures the Hodgkin Huxley dynamics of Na and K channels -- uses Brette & Gurstner 2005 AdEx formulation"}, {Name: "ExpThr", Doc: "membrane potential threshold for actually triggering a spike when using the exponential mechanism"}, {Name: "MaxHz", Doc: "for translating spiking interval (rate) into rate-code activation equivalent, what is the maximum firing rate associated with a maximum activation value of 1"}, {Name: "ISITau", Doc: "constant for integrating the spiking interval in estimating spiking rate"}, {Name: "ISIDt", Doc: "rate = 1 / tau"}, {Name: "RDt", Doc: "rate = 1 / tau"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.DendParams", IDName: "dend-params", Doc: "DendParams are the parameters for updating dendrite-specific dynamics", Fields: []gti.Field{{Name: "GbarExp", Doc: "dendrite-specific strength multiplier of the exponential spiking drive on Vm -- e.g., .5 makes it half as strong as at the soma (which uses Gbar.L as a strength multiplier per the AdEx standard model)"}, {Name: "GbarR", Doc: "dendrite-specific conductance of Kdr delayed rectifier currents, used to reset membrane potential for dendrite -- applied for Tr msec"}, {Name: "SSGi", Doc: "SST+ somatostatin positive slow spiking inhibition level specifically affecting dendritic Vm (VmDend) -- this is important for countering a positive feedback loop from NMDA getting stronger over the course of learning -- also typically requires SubMean = 1 for TrgAvgAct and learning to fully counter this feedback loop."}, {Name: "HasMod", Doc: "set automatically based on whether this layer has any recv projections that have a GType conductance type of Modulatory -- if so, then multiply GeSyn etc by GModSyn"}, {Name: "ModGain", Doc: "multiplicative gain factor on the total modulatory input -- this can also be controlled by the PrjnScale.Abs factor on ModulatoryG inputs, but it is convenient to be able to control on the layer as well."}, {Name: "ModBase", Doc: "baseline modulatory level for modulatory effects -- net modulation is ModBase + ModGain * GModSyn"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ActInitParams", IDName: "act-init-params", Doc: "ActInitParams are initial values for key network state variables.\nInitialized in InitActs called by InitWts, and provides target values for DecayState.", Fields: []gti.Field{{Name: "Vm", Doc: "initial membrane potential -- see Erev.L for the resting potential (typically .3)"}, {Name: "Act", Doc: "initial activation value -- typically 0"}, {Name: "GeBase", Doc: "baseline level of excitatory conductance (net input) -- Ge is initialized to this value, and it is added in as a constant background level of excitatory input -- captures all the other inputs not represented in the model, and intrinsic excitability, etc"}, {Name: "GiBase", Doc: "baseline level of inhibitory conductance (net input) -- Gi is initialized to this value, and it is added in as a constant background level of inhibitory input -- captures all the other inputs not represented in the model"}, {Name: "GeVar", Doc: "variance (sigma) of gaussian distribution around baseline Ge values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "GiVar", Doc: "variance (sigma) of gaussian distribution around baseline Gi values, per unit, to establish variability in intrinsic excitability.  value never goes < 0"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.DecayParams", IDName: "decay-params", Doc: "DecayParams control the decay of activation state in the DecayState function\ncalled in NewState when a new state is to be processed.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"act"}}}, Fields: []gti.Field{{Name: "Act", Doc: "proportion to decay most activation state variables toward initial values at start of every ThetaCycle (except those controlled separately below) -- if 1 it is effectively equivalent to full clear, resetting other derived values.  ISI is reset every AlphaCycle to get a fresh sample of activations (doesn't affect direct computation -- only readout)."}, {Name: "Glong", Doc: "proportion to decay long-lasting conductances, NMDA and GABA, and also the dendritic membrane potential -- when using random stimulus order, it is important to decay this significantly to allow a fresh start -- but set Act to 0 to enable ongoing activity to keep neurons in their sensitive regime."}, {Name: "AHP", Doc: "decay of afterhyperpolarization currents, including mAHP, sAHP, and KNa, Kir -- has a separate decay because often useful to have this not decay at all even if decay is on."}, {Name: "LearnCa", Doc: "decay of Ca variables driven by spiking activity used in learning: CaSpk* and Ca* variables. These are typically not decayed but may need to be in some situations."}, {Name: "OnRew", Doc: "decay layer at end of ThetaCycle when there is a global reward -- true by default for PTPred, PTMaint and PFC Super layers"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.DtParams", IDName: "dt-params", Doc: "DtParams are time and rate constants for temporal derivatives in Axon (Vm, G)", Fields: []gti.Field{{Name: "Integ", Doc: "overall rate constant for numerical integration, for all equations at the unit level -- all time constants are specified in millisecond units, with one cycle = 1 msec -- if you instead want to make one cycle = 2 msec, you can do this globally by setting this integ value to 2 (etc).  However, stability issues will likely arise if you go too high.  For improved numerical stability, you may even need to reduce this value to 0.5 or possibly even lower (typically however this is not necessary).  MUST also coordinate this with network.time_inc variable to ensure that global network.time reflects simulated time accurately"}, {Name: "VmTau", Doc: "membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmDendTau", Doc: "dendritic membrane potential time constant in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life) -- reflects the capacitance of the neuron in principle -- biological default for AdEx spiking model C = 281 pF = 2.81 normalized"}, {Name: "VmSteps", Doc: "number of integration steps to take in computing new Vm value -- this is the one computation that can be most numerically unstable so taking multiple steps with proportionally smaller dt is beneficial"}, {Name: "GeTau", Doc: "time constant for decay of excitatory AMPA receptor conductance."}, {Name: "GiTau", Doc: "time constant for decay of inhibitory GABAa receptor conductance."}, {Name: "IntTau", Doc: "time constant for integrating values over timescale of an individual input state (e.g., roughly 200 msec -- theta cycle), used in computing ActInt, GeInt from Ge, and GiInt from GiSyn -- this is used for scoring performance, not for learning, in cycles, which should be milliseconds typically (tau is roughly how long it takes for value to change significantly -- 1.4x the half-life),"}, {Name: "LongAvgTau", Doc: "time constant for integrating slower long-time-scale averages, such as nrn.ActAvg, Pool.ActsMAvg, ActsPAvg -- computed in NewState when a new input state is present (i.e., not msec but in units of a theta cycle) (tau is roughly how long it takes for value to change significantly) -- set lower for smaller models"}, {Name: "MaxCycStart", Doc: "cycle to start updating the SpkMaxCa, SpkMax values within a theta cycle -- early cycles often reflect prior state"}, {Name: "VmDt", Doc: "nominal rate = Integ / tau"}, {Name: "VmDendDt", Doc: "nominal rate = Integ / tau"}, {Name: "DtStep", Doc: "1 / VmSteps"}, {Name: "GeDt", Doc: "rate = Integ / tau"}, {Name: "GiDt", Doc: "rate = Integ / tau"}, {Name: "IntDt", Doc: "rate = Integ / tau"}, {Name: "LongAvgDt", Doc: "rate = 1 / tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SpikeNoiseParams", IDName: "spike-noise-params", Doc: "SpikeNoiseParams parameterizes background spiking activity impinging on the neuron,\nsimulated using a poisson spiking process.", Fields: []gti.Field{{Name: "On", Doc: "add noise simulating background spiking levels"}, {Name: "GeHz", Doc: "mean frequency of excitatory spikes -- typically 50Hz but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Ge", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "GiHz", Doc: "mean frequency of inhibitory spikes -- typically 100Hz fast spiking but multiple inputs increase rate -- poisson lambda parameter, also the variance"}, {Name: "Gi", Doc: "excitatory conductance per spike -- .001 has minimal impact, .01 can be strong, and .15 is needed to influence timing of clamped inputs"}, {Name: "GeExpInt", Doc: "Exp(-Interval) which is the threshold for GeNoiseP as it is updated"}, {Name: "GiExpInt", Doc: "Exp(-Interval) which is the threshold for GiNoiseP as it is updated"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ClampParams", IDName: "clamp-params", Doc: "ClampParams specify how external inputs drive excitatory conductances\n(like a current clamp) -- either adds or overwrites existing conductances.\nNoise is added in either case.", Fields: []gti.Field{{Name: "IsInput", Doc: "is this a clamped input layer?  set automatically based on layer type at initialization"}, {Name: "IsTarget", Doc: "is this a target layer?  set automatically based on layer type at initialization"}, {Name: "Ge", Doc: "amount of Ge driven for clamping -- generally use 0.8 for Target layers, 1.5 for Input layers"}, {Name: "Add"}, {Name: "ErrThr", Doc: "threshold on neuron Act activity to count as active for computing error relative to target in PctErr method"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AttnParams", IDName: "attn-params", Doc: "AttnParams determine how the Attn modulates Ge", Fields: []gti.Field{{Name: "On", Doc: "is attentional modulation active?"}, {Name: "Min", Doc: "minimum act multiplier if attention is 0"}, {Name: "RTThr", Doc: "threshold on CaSpkP for determining the reaction time for the Layer -- starts after MaxCycStart to ensure that prior trial activity has had a chance to dissipate."}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PopCodeParams", IDName: "pop-code-params", Doc: "PopCodeParams provides an encoding of scalar value using population code,\nwhere a single continuous (scalar) value is encoded as a gaussian bump\nacross a population of neurons (1 dimensional).\nIt can also modulate rate code and number of neurons active according to the value.\nThis is for layers that represent values as in the PVLV system (from Context.PVLV).\nBoth normalized activation values (1 max) and Ge conductance values can be generated.", Fields: []gti.Field{{Name: "On", Doc: "use popcode encoding of variable(s) that this layer represents"}, {Name: "Ge", Doc: "Ge multiplier for driving excitatory conductance based on PopCode -- multiplies normalized activation values"}, {Name: "Min", Doc: "minimum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "Max", Doc: "maximum value representable -- for GaussBump, typically include extra to allow mean with activity on either side to represent the lowest value you want to encode"}, {Name: "MinAct", Doc: "activation multiplier for values at Min end of range, where values at Max end have an activation of 1 -- if this is &lt; 1, then there is a rate code proportional to the value in addition to the popcode pattern -- see also MinSigma, MaxSigma"}, {Name: "MinSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "MaxSigma", Doc: "sigma parameter of a gaussian specifying the tuning width of the coarse-coded units, in normalized 0-1 range -- for Min value -- if MinSigma &lt; MaxSigma then more units are activated for Max values vs. Min values, proportionally"}, {Name: "Clip", Doc: "ensure that encoded and decoded value remains within specified range"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ActParams", IDName: "act-params", Doc: "axon.ActParams contains all the activation computation params and functions\nfor basic Axon, at the neuron level .\nThis is included in axon.Layer to drive the computation.", Fields: []gti.Field{{Name: "Spikes", Doc: "Spiking function parameters"}, {Name: "Dend", Doc: "dendrite-specific parameters"}, {Name: "Init", Doc: "initial values for key network state variables -- initialized in InitActs called by InitWts, and provides target values for DecayState"}, {Name: "Decay", Doc: "amount to decay between AlphaCycles, simulating passage of time and effects of saccades etc, especially important for environments with random temporal structure (e.g., most standard neural net training corpora)"}, {Name: "Dt", Doc: "time and rate constants for temporal derivatives / updating of activation state"}, {Name: "Gbar", Doc: "maximal conductances levels for channels"}, {Name: "Erev", Doc: "reversal potentials for each channel"}, {Name: "Clamp", Doc: "how external inputs drive neural activations"}, {Name: "Noise", Doc: "how, where, when, and how much noise to add"}, {Name: "VmRange", Doc: "range for Vm membrane potential -- -- important to keep just at extreme range of reversal potentials to prevent numerical instability"}, {Name: "Mahp", Doc: "M-type medium time-scale afterhyperpolarization mAHP current -- this is the primary form of adaptation on the time scale of multiple sequences of spikes"}, {Name: "Sahp", Doc: "slow time-scale afterhyperpolarization sAHP current -- integrates CaSpkD at theta cycle intervals and produces a hard cutoff on sustained activity for any neuron"}, {Name: "KNa", Doc: "sodium-gated potassium channel adaptation parameters -- activates a leak-like current as a function of neural activity (firing = Na influx) at two different time-scales (Slick = medium, Slack = slow)"}, {Name: "Kir", Doc: "potassium (K) inwardly rectifying (ir) current, which is similar to GABAB\n(which is a GABA modulated Kir channel).  This channel is off by default\nbut plays a critical role in making medium spiny neurons (MSNs) relatively\nquiet in the striatum."}, {Name: "NMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "MaintNMDA", Doc: "NMDA channel parameters used in computing Gnmda conductance for bistability, and postsynaptic calcium flux used in learning.  Note that Learn.Snmda has distinct parameters used in computing sending NMDA parameters used in learning."}, {Name: "GabaB", Doc: "GABA-B / GIRK channel parameters"}, {Name: "VGCC", Doc: "voltage gated calcium channels -- provide a key additional source of Ca for learning and positive-feedback loop upstate for active neurons"}, {Name: "AK", Doc: "A-type potassium (K) channel that is particularly important for limiting the runaway excitation from VGCC channels"}, {Name: "SKCa", Doc: "small-conductance calcium-activated potassium channel produces the pausing function as a consequence of rapid bursting."}, {Name: "AttnMod", Doc: "Attentional modulation parameters: how Attn modulates Ge"}, {Name: "PopCode", Doc: "provides encoding population codes, used to represent a single continuous (scalar) value, across a population of units / neurons (1 dimensional)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnGTypes", IDName: "prjn-g-types", Doc: "PrjnGTypes represents the conductance (G) effects of a given projection,\nincluding excitatory, inhibitory, and modulatory."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynComParams", IDName: "syn-com-params", Doc: "SynComParams are synaptic communication parameters:\nused in the Prjn parameters.  Includes delay and\nprobability of failure, and Inhib for inhibitory connections,\nand modulatory projections that have multiplicative-like effects.", Fields: []gti.Field{{Name: "GType", Doc: "type of conductance (G) communicated by this projection"}, {Name: "Delay", Doc: "additional synaptic delay in msec for inputs arriving at this projection.  Must be <= MaxDelay which is set during network building based on MaxDelay of any existing Prjn in the network.  Delay = 0 means a spike reaches receivers in the next Cycle, which is the minimum time (1 msec).  Biologically, subtract 1 from biological synaptic delay values to set corresponding Delay value."}, {Name: "MaxDelay", Doc: "maximum value of Delay -- based on MaxDelay values when the BuildGBuf function was called when the network was built -- cannot set it longer than this, except by calling BuildGBuf on network after changing MaxDelay to a larger value in any projection in the network."}, {Name: "PFail", Doc: "probability of synaptic transmission failure -- if > 0, then weights are turned off at random as a function of PFail (times 1-SWt if PFailSwt)"}, {Name: "PFailSWt", Doc: "if true, then probability of failure is inversely proportional to SWt structural / slow weight value (i.e., multiply PFail * (1-SWt)))"}, {Name: "DelLen", Doc: "delay length = actual length of the GBuf buffer per neuron = Delay+1 -- just for speed"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnScaleParams", IDName: "prjn-scale-params", Doc: "PrjnScaleParams are projection scaling parameters: modulates overall strength of projection,\nusing both absolute and relative factors.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"act_prjn"}}}, Fields: []gti.Field{{Name: "Rel", Doc: "relative scaling that shifts balance between different projections -- this is subject to normalization across all other projections into receiving neuron, and determines the GScale.Target for adapting scaling"}, {Name: "Abs", Doc: "absolute multiplier adjustment factor for the prjn scaling -- can be used to adjust for idiosyncrasies not accommodated by the standard scaling based on initial target activation level and relative scaling factors -- any adaptation operates by directly adjusting scaling factor from the initially computed value"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxI32", IDName: "avg-max-i-32", Doc: "AvgMaxI32 holds average and max statistics for float32,\nand values used for computing them incrementally,\nusing a fixed precision int32 based float representation\nthat can be used with GPU-based atomic add and max functions.\nThis ONLY works for positive values with averages around 1, and\nthe N must be set IN ADVANCE to the correct number of items.\nOnce Calc() is called, the incremental values are reset\nvia Init() so it is always ready for updating without a separate\nInit() pass.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"avgmaxi"}}}, Fields: []gti.Field{{Name: "Avg", Doc: "Average, from Calc when last computed as Sum / N"}, {Name: "Max", Doc: "Maximum value, copied from CurMax in Calc"}, {Name: "Sum", Doc: "sum for computing average -- incremented in UpdateVal, reset in Calc"}, {Name: "CurMax", Doc: "current maximum value, updated via UpdateVal, reset in Calc"}, {Name: "N", Doc: "number of items in the sum -- this must be set in advance to a known value and it is used in computing the float <-> int conversion factor to maximize precision."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AxonNetwork", IDName: "axon-network", Doc: "AxonNetwork defines the essential algorithmic API for Axon, at the network level.\nThese are the methods that the user calls in their Sim code:\n* NewState\n* Cycle\n* NewPhase\n* DWt\n* WtFmDwt\nBecause we don't want to have to force the user to use the interface cast in calling\nthese methods, we provide Impl versions here that are the implementations\nwhich the user-facing method calls through the interface cast.\nSpecialized algorithms should thus only change the Impl version, which is what\nis exposed here in this interface.\n\nThere is now a strong constraint that all Cycle level computation takes place\nin one pass at the Layer level, which greatly improves threading efficiency.\n\nAll of the structural API is in emer.Network, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AxonLayer", IDName: "axon-layer", Doc: "AxonLayer defines the essential algorithmic API for Axon, at the layer level.\nThese are the methods that the axon.Network calls on its layers at each step\nof processing.  Other Layer types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Layer, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AxonPrjn", IDName: "axon-prjn", Doc: "AxonPrjn defines the essential algorithmic API for Axon, at the projection level.\nThese are the methods that the axon.Layer calls on its prjns at each step\nof processing.  Other Prjn types can selectively re-implement (override) these methods\nto modify the computation, while inheriting the basic behavior for non-overridden methods.\n\nAll of the structural API is in emer.Prjn, which this interface also inherits for\nconvenience."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AxonPrjns", IDName: "axon-prjns"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NetIdxs", IDName: "net-idxs", Doc: "NetIdxs are indexes and sizes for processing network", Directives: []gti.Directive{{Tool: "gosl", Directive: "end", Args: []string{"context"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"context"}}, {Tool: "gosl", Directive: "end", Args: []string{"context"}}, {Tool: "gosl", Directive: "start", Args: []string{"context"}}}, Fields: []gti.Field{{Name: "NData", Doc: "number of data parallel items to process currently"}, {Name: "NetIdx", Doc: "network index in global Networks list of networks -- needed for GPU shader kernel compatible network variable access functions (e.g., NrnV, SynV etc) in CPU mode"}, {Name: "MaxData", Doc: "maximum amount of data parallel"}, {Name: "NLayers", Doc: "number of layers in the network"}, {Name: "NNeurons", Doc: "total number of neurons"}, {Name: "NPools", Doc: "total number of pools excluding * MaxData factor"}, {Name: "NSyns", Doc: "total number of synapses"}, {Name: "GPUMaxBuffFloats", Doc: "maximum size in float32 (4 bytes) of a GPU buffer -- needed for GPU access"}, {Name: "GPUSynCaBanks", Doc: "total number of SynCa banks of GPUMaxBufferBytes arrays in GPU"}, {Name: "PVLVNPosUSs", Doc: "total number of PVLV Drives / positive USs"}, {Name: "PVLVNNegUSs", Doc: "total number of PVLV Negative USs"}, {Name: "GvUSnegOff", Doc: "offset into GlobalVars for USneg values"}, {Name: "GvUSnegStride", Doc: "stride into GlobalVars for USneg values"}, {Name: "GvUSposOff", Doc: "offset into GlobalVars for USpos, Drive, VSPatch values values"}, {Name: "GvUSposStride", Doc: "stride into GlobalVars for USpos, Drive, VSPatch values"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.Context", IDName: "context", Doc: "Context contains all of the global context state info\nthat is shared across every step of the computation.\nIt is passed around to all relevant computational functions,\nand is updated on the CPU and synced to the GPU after every cycle.\nIt is the *only* mechanism for communication from CPU to GPU.\nIt contains timing, Testing vs. Training mode, random number context,\nglobal neuromodulation, etc.", Fields: []gti.Field{{Name: "Mode", Doc: "current evaluation mode, e.g., Train, Test, etc"}, {Name: "Testing", Doc: "if true, the model is being run in a testing mode, so no weight changes or other associated computations are needed.  this flag should only affect learning-related behavior.  Is automatically updated based on Mode != Train"}, {Name: "Phase", Doc: "phase counter: typicaly 0-1 for minus-plus but can be more phases for other algorithms"}, {Name: "PlusPhase", Doc: "true if this is the plus phase, when the outcome / bursting is occurring, driving positive learning -- else minus phase"}, {Name: "PhaseCycle", Doc: "cycle within current phase -- minus or plus"}, {Name: "Cycle", Doc: "cycle counter: number of iterations of activation updating (settling) on the current state -- this counts time sequentially until reset with NewState"}, {Name: "ThetaCycles", Doc: "length of the theta cycle in terms of 1 msec Cycles -- some network update steps depend on doing something at the end of the theta cycle (e.g., CTCtxtPrjn)."}, {Name: "CyclesTotal", Doc: "total cycle count -- increments continuously from whenever it was last reset -- typically this is number of milliseconds in simulation time -- is int32 and not uint32 b/c used with Synapse CaUpT which needs to have a -1 case for expired update time"}, {Name: "Time", Doc: "accumulated amount of time the network has been running, in simulation-time (not real world time), in seconds"}, {Name: "TrialsTotal", Doc: "total trial count -- increments continuously in NewState call *only in Train mode* from whenever it was last reset -- can be used for synchronizing weight updates across nodes"}, {Name: "TimePerCycle", Doc: "amount of time to increment per cycle"}, {Name: "SlowInterval", Doc: "how frequently to perform slow adaptive processes such as synaptic scaling, inhibition adaptation, associated in the brain with sleep, in the SlowAdapt method.  This should be long enough for meaningful changes to accumulate -- 100 is default but could easily be longer in larger models.  Because SlowCtr is incremented by NData, high NData cases (e.g. 16) likely need to increase this value -- e.g., 400 seems to produce overall consistent results in various models."}, {Name: "SlowCtr", Doc: "counter for how long it has been since last SlowAdapt step.  Note that this is incremented by NData to maintain consistency across different values of this parameter."}, {Name: "SynCaCtr", Doc: "synaptic calcium counter, which drives the CaUpT synaptic value to optimize updating of this computationally expensive factor. It is incremented by 1 for each cycle, and reset at the SlowInterval, at which point the synaptic calcium values are all reset."}, {Name: "pad"}, {Name: "pad1"}, {Name: "NetIdxs", Doc: "indexes and sizes of current network"}, {Name: "NeuronVars", Doc: "stride offsets for accessing neuron variables"}, {Name: "NeuronAvgVars", Doc: "stride offsets for accessing neuron average variables"}, {Name: "NeuronIdxs", Doc: "stride offsets for accessing neuron indexes"}, {Name: "SynapseVars", Doc: "stride offsets for accessing synapse variables"}, {Name: "SynapseCaVars", Doc: "stride offsets for accessing synapse Ca variables"}, {Name: "SynapseIdxs", Doc: "stride offsets for accessing synapse indexes"}, {Name: "RandCtr", Doc: "random counter -- incremented by maximum number of possible random numbers generated per cycle, regardless of how many are actually used -- this is shared across all layers so must encompass all possible param settings."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.BurstParams", IDName: "burst-params", Doc: "BurstParams determine how the 5IB Burst activation is computed from\nCaSpkP integrated spiking values in Super layers -- thresholded.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"deep_layers"}}}, Fields: []gti.Field{{Name: "ThrRel", Doc: "Relative component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  This is the distance between the average and maximum activation values within layer (e.g., 0 = average, 1 = max).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "ThrAbs", Doc: "Absolute component of threshold on superficial activation value, below which it does not drive Burst (and above which, Burst = CaSpkP).  Overall effective threshold is MAX of relative and absolute thresholds."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.CTParams", IDName: "ct-params", Doc: "CTParams control the CT corticothalamic neuron special behavior", Fields: []gti.Field{{Name: "GeGain", Doc: "gain factor for context excitatory input, which is constant as compared to the spiking input from other projections, so it must be downscaled accordingly.  This can make a difference and may need to be scaled up or down."}, {Name: "DecayTau", Doc: "decay time constant for context Ge input -- if > 0, decays over time so intrinsic circuit dynamics have to take over.  For single-step copy-based cases, set to 0, while longer-time-scale dynamics should use 50"}, {Name: "DecayDt", Doc: "1 / tau"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PulvParams", IDName: "pulv-params", Doc: "PulvParams provides parameters for how the plus-phase (outcome)\nstate of Pulvinar thalamic relay cell neurons is computed from\nthe corresponding driver neuron Burst activation (or CaSpkP if not Super)", Fields: []gti.Field{{Name: "DriveScale", Doc: "multiplier on driver input strength, multiplies CaSpkP from driver layer to produce Ge excitatory input to Pulv unit."}, {Name: "FullDriveAct", Doc: "Level of Max driver layer CaSpkP at which the drivers fully drive the burst phase activation.  If there is weaker driver input, then (Max/FullDriveAct) proportion of the non-driver inputs remain and this critically prevents the network from learning to turn activation off, which is difficult and severely degrades learning."}, {Name: "DriveLayIdx", Doc: "index of layer that generates the driving activity into this one -- set via SetBuildConfig(DriveLayName) setting"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GlobalVars", IDName: "global-vars", Doc: "GlobalVars are network-wide variables, such as neuromodulators, reward, drives, etc\nincluding the state for the PVLV phasic dopamine model."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PushOff", IDName: "push-off", Doc: "PushOff has push constants for setting offset into compute shader", Fields: []gti.Field{{Name: "Off", Doc: "offset"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GPU", IDName: "gpu", Doc: "GPU manages all of the GPU-based computation for a given Network.\nLives within the network.", Fields: []gti.Field{{Name: "On", Doc: "if true, actually use the GPU"}, {Name: "RecFunTimes"}, {Name: "CycleByCycle", Doc: "if true, process each cycle one at a time.  Otherwise, 10 cycles at a time are processed in one batch."}, {Name: "Net", Doc: "the network we operate on -- we live under this net"}, {Name: "Ctx", Doc: "the context we use"}, {Name: "Sys", Doc: "the vgpu compute system"}, {Name: "Params", Doc: "VarSet = 0: the uniform LayerParams"}, {Name: "Idxs", Doc: "VarSet = 1: the storage indexes and PrjnParams"}, {Name: "Structs", Doc: "VarSet = 2: the Storage buffer for RW state structs and neuron floats"}, {Name: "Syns", Doc: "Varset = 3: the Storage buffer for synapses"}, {Name: "SynCas", Doc: "Varset = 4: the Storage buffer for SynCa banks"}, {Name: "Semaphores", Doc: "for sequencing commands"}, {Name: "NThreads", Doc: "number of warp threads -- typically 64 -- must update all hlsl files if changed!"}, {Name: "MaxBufferBytes", Doc: "maximum number of bytes per individual storage buffer element, from GPUProps.Limits.MaxStorageBufferRange"}, {Name: "SynapseCas0", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas1", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas2", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas3", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas4", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas5", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas6", Doc: "bank of floats for GPU access"}, {Name: "SynapseCas7", Doc: "bank of floats for GPU access"}, {Name: "DidBind", Doc: "tracks var binding"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.HipConfig", IDName: "hip-config", Doc: "HipConfig have the hippocampus size and connectivity parameters", Fields: []gti.Field{{Name: "EC2Size", Doc: "size of EC2"}, {Name: "EC3NPool", Doc: "number of EC3 pools (outer dimension)"}, {Name: "EC3NNrn", Doc: "number of neurons in one EC3 pool"}, {Name: "CA1NNrn", Doc: "number of neurons in one CA1 pool"}, {Name: "CA3Size", Doc: "size of CA3"}, {Name: "DGRatio", Doc: "size of DG / CA3"}, {Name: "EC3ToEC2PCon", Doc: "percent connectivity from EC3 to EC2"}, {Name: "EC2ToDGPCon", Doc: "percent connectivity from EC2 to DG"}, {Name: "EC2ToCA3PCon", Doc: "percent connectivity from EC2 to CA3"}, {Name: "CA3ToCA1PCon", Doc: "percent connectivity from CA3 to CA1"}, {Name: "DGToCA3PCon", Doc: "percent connectivity into CA3 from DG"}, {Name: "EC2LatRadius", Doc: "lateral radius of connectivity in EC2"}, {Name: "EC2LatSigma", Doc: "lateral gaussian sigma in EC2 for how quickly weights fall off with distance"}, {Name: "MossyDelta", Doc: "proportion of full mossy fiber strength (PrjnScale.Rel) for CA3 EDL in training, applied at the start of a trial to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "MossyDeltaTest", Doc: "proportion of full mossy fiber strength (PrjnScale.Rel) for CA3 EDL in testing, applied during 2nd-3rd quarters to reduce DG -> CA3 strength.  1 = fully reduce strength, .5 = 50% reduction, etc"}, {Name: "ThetaLow", Doc: "low theta modulation value for temporal difference EDL -- sets PrjnScale.Rel on CA1 <-> EC prjns consistent with Theta phase model"}, {Name: "ThetaHigh", Doc: "high theta modulation value for temporal difference EDL -- sets PrjnScale.Rel on CA1 <-> EC prjns consistent with Theta phase model"}, {Name: "EC5Clamp", Doc: "flag for clamping the EC5 from EC5ClampSrc"}, {Name: "EC5ClampSrc", Doc: "source layer for EC5 clamping activations in the plus phase -- biologically it is EC3 but can use an Input layer if available"}, {Name: "EC5ClampTest", Doc: "clamp the EC5 from EC5ClampSrc during testing as well as training -- this will overwrite any target values that might be used in stats (e.g., in the basic hip example), so it must be turned off there"}, {Name: "EC5ClampThr", Doc: "threshold for binarizing EC5 clamp values -- any value above this is clamped to 1, else 0 -- helps produce a cleaner learning signal.  Set to 0 to not perform any binarization."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.HipPrjnParams", IDName: "hip-prjn-params", Doc: "HipPrjnParams define behavior of hippocampus prjns, which have special learning rules", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"hip_prjns"}}}, Fields: []gti.Field{{Name: "Hebb", Doc: "Hebbian learning proportion"}, {Name: "Err", Doc: "EDL proportion"}, {Name: "SAvgCor", Doc: "proportion of correction to apply to sending average activation for hebbian learning component (0=none, 1=all, .5=half, etc)"}, {Name: "SAvgThr", Doc: "threshold of sending average activation below which learning does not occur (prevents learning when there is no input)"}, {Name: "SNominal", Doc: "sending layer Nominal (need to manually set it to be the same as the sending layer)"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ActAvgParams", IDName: "act-avg-params", Doc: "ActAvgParams represents the nominal average activity levels in the layer\nand parameters for adapting the computed Gi inhibition levels to maintain\naverage activity within a target range.", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"inhib"}}, {Tool: "gosl", Directive: "end", Args: []string{"inhib"}}, {Tool: "gosl", Directive: "start", Args: []string{"inhib"}}}, Fields: []gti.Field{{Name: "Nominal", Doc: "nominal estimated average activity level in the layer, which is used in computing the scaling factor on sending projections from this layer.  In general it should roughly match the layer ActAvg.ActMAvg value, which can be logged using the axon.LogAddDiagnosticItems function.  If layers receiving from this layer are not getting enough Ge excitation, then this Nominal level can be lowered to increase projection strength (fewer active neurons means each one contributes more, so scaling factor goes as the inverse of activity level), or vice-versa if Ge is too high.  It is also the basis for the target activity level used for the AdaptGi option -- see the Offset which is added to this value."}, {Name: "AdaptGi", Doc: "enable adapting of layer inhibition Gi multiplier factor (stored in layer GiMult value) to maintain a Target layer level of ActAvg.ActMAvg.  This generally works well and improves the long-term stability of the models.  It is not enabled by default because it depends on having established a reasonable Nominal + Offset target activity level."}, {Name: "Offset", Doc: "offset to add to Nominal for the target average activity that drives adaptation of Gi for this layer.  Typically the Nominal level is good, but sometimes Nominal must be adjusted up or down to achieve desired Ge scaling, so this Offset can compensate accordingly."}, {Name: "HiTol", Doc: "tolerance for higher than Target target average activation as a proportion of that target value (0 = exactly the target, 0.2 = 20% higher than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "LoTol", Doc: "tolerance for lower than Target target average activation as a proportion of that target value (0 = exactly the target, 0.5 = 50% lower than target) -- only once activations move outside this tolerance are inhibitory values adapted."}, {Name: "AdaptRate", Doc: "rate of Gi adaptation as function of AdaptRate * (Target - ActMAvg) / Target -- occurs at spaced intervals determined by Network.SlowInterval value -- slower values such as 0.01 may be needed for large networks and sparse layers."}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.TopoInhibParams", IDName: "topo-inhib-params", Doc: "TopoInhibParams provides for topographic gaussian inhibition integrating over neighborhood.\nTODO: not currently being used", Fields: []gti.Field{{Name: "On", Doc: "use topographic inhibition"}, {Name: "Width", Doc: "half-width of topographic inhibition within layer"}, {Name: "Sigma", Doc: "normalized gaussian sigma as proportion of Width, for gaussian weighting"}, {Name: "Wrap", Doc: "half-width of topographic inhibition within layer"}, {Name: "Gi", Doc: "overall inhibition multiplier for topographic inhibition (generally <= 1)"}, {Name: "FF", Doc: "overall inhibitory contribution from feedforward inhibition -- multiplies average Ge from pools or Ge from neurons"}, {Name: "FB", Doc: "overall inhibitory contribution from feedback inhibition -- multiplies average activation from pools or Act from neurons"}, {Name: "FF0", Doc: "feedforward zero point for Ge per neuron (summed Ge is compared to N * FF0) -- below this level, no FF inhibition is computed, above this it is FF * (Sum Ge - N * FF0)"}, {Name: "WidthWt", Doc: "weight value at width -- to assess the value of Sigma"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.InhibParams", IDName: "inhib-params", Doc: "axon.InhibParams contains all the inhibition computation params and functions for basic Axon\nThis is included in axon.Layer to support computation.\nThis also includes other misc layer-level params such as expected average activation in the layer\nwhich is used for Ge rescaling and potentially for adapting inhibition over time", Fields: []gti.Field{{Name: "ActAvg", Doc: "layer-level and pool-level average activation initial values and updating / adaptation thereof -- initial values help determine initial scaling factors."}, {Name: "Layer", Doc: "inhibition across the entire layer -- inputs generally use Gi = 0.8 or 0.9, 1.3 or higher for sparse layers.  If the layer has sub-pools (4D shape) then this is effectively between-pool inhibition."}, {Name: "Pool", Doc: "inhibition within sub-pools of units, for layers with 4D shape -- almost always need this if the layer has pools."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.Layer", IDName: "layer", Doc: "axon.Layer implements the basic Axon spiking activation function,\nand manages learning in the projections.", Methods: []gti.Method{{Name: "Defaults", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}}, {Name: "InitWts", Doc: "InitWts initializes the weight values in the network, i.e., resetting learning\nAlso calls InitActs", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"ctx", "nt"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- only called automatically during InitWts", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "UnLesionNeurons", Doc: "UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}}, {Name: "LesionNeurons", Doc: "LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer\nreturns number of neurons lesioned.  Emits error if prop > 1 as indication that percent\nmight have been passed", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"prop"}, Returns: []string{"int"}}}, Embeds: []gti.Field{{Name: "LayerBase"}}, Fields: []gti.Field{{Name: "Params", Doc: "all layer-level parameters -- these must remain constant once configured"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerBase", IDName: "layer-base", Doc: "LayerBase manages the structural elements of the layer, which are common\nto any Layer type.\nThe Base does not have algorithm-specific methods and parameters, so it can be easily\nreused for different algorithms, and cleanly separates the algorithm-specific code.\nAny dependency on the algorithm-level Layer can be captured in the AxonLayer interface,\naccessed via the AxonLay field.", Fields: []gti.Field{{Name: "AxonLay", Doc: "we need a pointer to ourselves as an AxonLayer (which subsumes emer.Layer), which can always be used to extract the true underlying type of object when layer is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Network", Doc: "our parent network, in case we need to use it to find other layers etc -- set when added by network"}, {Name: "Nm", Doc: "Name of the layer -- this must be unique within the network, which has a map for quick lookup and layers are typically accessed directly by name"}, {Name: "Cls", Doc: "Class is for applying parameter styles, can be space separated multple tags"}, {Name: "Off", Doc: "inactivate this layer -- allows for easy experimentation"}, {Name: "Shp", Doc: "shape of the layer -- can be 2D for basic layers and 4D for layers with sub-groups (hypercolumns) -- order is outer-to-inner (row major) so Y then X for 2D and for 4D: Y-X unit pools then Y-X neurons within pools"}, {Name: "Typ", Doc: "type of layer -- Hidden, Input, Target, Compare, or extended type in specialized algorithms -- matches against .Class parameter styles (e.g., .Hidden etc)"}, {Name: "Rel", Doc: "Spatial relationship to other layer, determines positioning"}, {Name: "Ps", Doc: "position of lower-left-hand corner of layer in 3D space, computed from Rel.  Layers are in X-Y width - height planes, stacked vertically in Z axis."}, {Name: "Idx", Doc: "a 0..n-1 index of the position of the layer within list of layers in the network. For Axon networks, it only has significance in determining who gets which weights for enforcing initial weight symmetry -- higher layers get weights from lower layers."}, {Name: "NNeurons", Doc: "number of neurons in the layer"}, {Name: "NeurStIdx", Doc: "starting index of neurons for this layer within the global Network list"}, {Name: "NPools", Doc: "number of pools based on layer shape -- at least 1 for layer pool + 4D subpools"}, {Name: "MaxData", Doc: "maximum amount of input data that can be processed in parallel in one pass of the network. Neuron, Pool, Vals storage is allocated to hold this amount."}, {Name: "RepIxs", Doc: "indexes of representative units in the layer, for computationally expensive stats or displays -- also set RepShp"}, {Name: "RepShp", Doc: "shape of representative units in the layer -- if RepIxs is empty or .Shp is nil, use overall layer shape"}, {Name: "RcvPrjns", Doc: "list of receiving projections into this layer from other layers"}, {Name: "SndPrjns", Doc: "list of sending projections from this layer to other layers"}, {Name: "Vals", Doc: "layer-level state values that are updated during computation -- one for each data parallel -- is a sub-slice of network full set"}, {Name: "Pools", Doc: "computes FS-FFFB inhibition and other pooled, aggregate state variables -- has at least 1 for entire layer (lpl = layer pool), and one for each sub-pool if shape supports that (4D) * 1 per data parallel (inner loop).  This is a sub-slice from overall Network Pools slice.  You must iterate over index and use pointer to modify values."}, {Name: "Exts", Doc: "external input values for this layer, allocated from network global Exts slice"}, {Name: "BuildConfig", Doc: "configuration data set when the network is configured, that is used during the network Build() process via PostBuild method, after all the structure of the network has been fully constructed.  In particular, the Params is nil until Build, so setting anything specific in there (e.g., an index to another layer) must be done as a second pass.  Note that Params are all applied after Build and can set user-modifiable params, so this is for more special algorithm structural parameters set during ConfigNet() methods.,"}, {Name: "DefParams", Doc: "default parameters that are applied prior to user-set parameters -- these are useful for specific layer functionality in specialized brain areas (e.g., PVLV, BG etc) not associated with a layer type, which otherwise is used to hard-code initial default parameters -- typically just set to a literal map."}, {Name: "ParamsHistory", Doc: "provides a history of parameters applied to the layer"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerIdxs", IDName: "layer-idxs", Doc: "LayerIdxs contains index access into network global arrays for GPU.", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"layerparams"}}, {Tool: "gosl", Directive: "end", Args: []string{"layerparams"}}, {Tool: "gosl", Directive: "start", Args: []string{"layerparams"}}}, Fields: []gti.Field{{Name: "LayIdx", Doc: "layer index"}, {Name: "MaxData", Doc: "maximum number of data parallel elements"}, {Name: "PoolSt", Doc: "start of pools for this layer -- first one is always the layer-wide pool"}, {Name: "NeurSt", Doc: "start of neurons for this layer in global array (same as Layer.NeurStIdx)"}, {Name: "NeurN", Doc: "number of neurons in layer"}, {Name: "RecvSt", Doc: "start index into RecvPrjns global array"}, {Name: "RecvN", Doc: "number of recv projections"}, {Name: "SendSt", Doc: "start index into RecvPrjns global array"}, {Name: "SendN", Doc: "number of recv projections"}, {Name: "ExtsSt", Doc: "starting index in network global Exts list of external input for this layer -- only for Input / Target / Compare layer types"}, {Name: "ShpPlY", Doc: "layer shape Pools Y dimension -- 1 for 2D"}, {Name: "ShpPlX", Doc: "layer shape Pools X dimension -- 1 for 2D"}, {Name: "ShpUnY", Doc: "layer shape Units Y dimension"}, {Name: "ShpUnX", Doc: "layer shape Units X dimension"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerInhibIdxs", IDName: "layer-inhib-idxs", Doc: "LayerInhibIdxs contains indexes of layers for between-layer inhibition", Fields: []gti.Field{{Name: "Idx1", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib1Name if present -- -1 if not used"}, {Name: "Idx2", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib2Name if present -- -1 if not used"}, {Name: "Idx3", Doc: "idx of Layer to get layer-level inhibition from -- set during Build from BuildConfig LayInhib3Name if present -- -1 if not used"}, {Name: "Idx4", Doc: "idx of Layer to geta layer-level inhibition from -- set during Build from BuildConfig LayInhib4Name if present -- -1 if not used"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerParams", IDName: "layer-params", Doc: "LayerParams contains all of the layer parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []gti.Field{{Name: "LayType", Doc: "functional type of layer -- determines functional code path for specialized layer types, and is synchronized with the Layer.Typ value"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "Acts", Doc: "Activation parameters and methods for computing activations"}, {Name: "Inhib", Doc: "Inhibition parameters and methods for computing layer-level inhibition"}, {Name: "LayInhib", Doc: "indexes of layers that contribute between-layer inhibition to this layer -- set these indexes via BuildConfig LayInhibXName (X = 1, 2...)"}, {Name: "Learn", Doc: "Learning parameters and methods that operate at the neuron level"}, {Name: "Bursts", Doc: "BurstParams determine how the 5IB Burst activation is computed from CaSpkP integrated spiking values in Super layers -- thresholded."}, {Name: "CT", Doc: "] params for the CT corticothalamic layer and PTPred layer that generates predictions over the Pulvinar using context -- uses the CtxtGe excitatory input plus stronger NMDA channels to maintain context trace"}, {Name: "Pulv", Doc: "provides parameters for how the plus-phase (outcome) state of Pulvinar thalamic relay cell neurons is computed from the corresponding driver neuron Burst activation (or CaSpkP if not Super)"}, {Name: "Matrix", Doc: "parameters for BG Striatum Matrix MSN layers, which are the main Go / NoGo gating units in BG."}, {Name: "GP", Doc: "type of GP Layer."}, {Name: "VSPatch", Doc: "parameters for VSPatch learning"}, {Name: "LDT", Doc: "parameterizes laterodorsal tegmentum ACh salience neuromodulatory signal, driven by superior colliculus stimulus novelty, US input / absence, and OFC / ACC inhibition"}, {Name: "VTA", Doc: "parameterizes computing overall VTA DA based on LHb PVDA (primary value -- at US time, computed at start of each trial and stored in LHbPVDA global value) and Amygdala (CeM) CS / learned value (LV) activations, which update every cycle."}, {Name: "RWPred", Doc: "parameterizes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the PVLV framework)."}, {Name: "RWDa", Doc: "parameterizes reward prediction dopamine for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the PVLV framework)."}, {Name: "TDInteg", Doc: "parameterizes TD reward integration layer"}, {Name: "TDDa", Doc: "parameterizes dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase."}, {Name: "Idxs", Doc: "recv and send projection array access info"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerTypes", IDName: "layer-types", Doc: "LayerTypes is an axon-specific layer type enum,\nthat encompasses all the different algorithm types supported.\nClass parameter styles automatically key off of these types.\nThe first entries must be kept synchronized with the emer.LayerType,\nalthough we replace Hidden -> Super."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ActAvgVals", IDName: "act-avg-vals", Doc: "ActAvgVals are long-running-average activation levels stored in the LayerVals,\nfor monitoring and adapting inhibition and possibly scaling parameters.\nAll of these integrate over NData within a network, so are the same across them.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"layervals"}}}, Fields: []gti.Field{{Name: "ActMAvg", Doc: "running-average minus-phase activity integrated at Dt.LongAvgTau -- used for adapting inhibition relative to target level"}, {Name: "ActPAvg", Doc: "running-average plus-phase activity integrated at Dt.LongAvgTau"}, {Name: "AvgMaxGeM", Doc: "running-average max of minus-phase Ge value across the layer integrated at Dt.LongAvgTau"}, {Name: "AvgMaxGiM", Doc: "running-average max of minus-phase Gi value across the layer integrated at Dt.LongAvgTau"}, {Name: "GiMult", Doc: "multiplier on inhibition -- adapted to maintain target activity level"}, {Name: "AdaptThr", Doc: "adaptive threshold -- only used for specialized layers, e.g., VSPatch"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.CorSimStats", IDName: "cor-sim-stats", Doc: "CorSimStats holds correlation similarity (centered cosine aka normalized dot product)\nstatistics at the layer level", Fields: []gti.Field{{Name: "Cor", Doc: "correlation (centered cosine aka normalized dot product) activation difference between ActP and ActM on this alpha-cycle for this layer -- computed by CorSimFmActs called by PlusPhase"}, {Name: "Avg", Doc: "running average of correlation similarity between ActP and ActM -- computed with CorSim.Tau time constant in PlusPhase"}, {Name: "Var", Doc: "running variance of correlation similarity between ActP and ActM -- computed with CorSim.Tau time constant in PlusPhase"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LaySpecialVals", IDName: "lay-special-vals", Doc: "LaySpecialVals holds special values used to communicate to other layers\nbased on neural values, used for special algorithms such as RL where\nsome of the computation is done algorithmically.", Fields: []gti.Field{{Name: "V1", Doc: "one value"}, {Name: "V2", Doc: "one value"}, {Name: "V3", Doc: "one value"}, {Name: "V4", Doc: "one value"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LayerVals", IDName: "layer-vals", Doc: "LayerVals holds extra layer state that is updated per layer.\nIt is sync'd down from the GPU to the CPU after every Cycle.", Fields: []gti.Field{{Name: "LayIdx", Doc: "layer index for these vals"}, {Name: "DataIdx", Doc: "data index for these vals"}, {Name: "RT", Doc: "reaction time for this layer in cycles, which is -1 until the Max CaSpkP level (after MaxCycStart) exceeds the Act.Attn.RTThr threshold"}, {Name: "pad"}, {Name: "ActAvg", Doc: "running-average activation levels used for adaptive inhibition, and other adapting values"}, {Name: "CorSim", Doc: "correlation (centered cosine aka normalized dot product) similarity between ActM, ActP states"}, {Name: "Special", Doc: "special values used to communicate to other layers based on neural values computed on the GPU -- special cross-layer computations happen CPU-side and are sent back into the network via Context on the next cycle -- used for special algorithms such as RL / DA etc"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.CaLrnParams", IDName: "ca-lrn-params", Doc: "CaLrnParams parameterizes the neuron-level calcium signals driving learning:\nCaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or\nuse the more complex and dynamic VGCC channel directly.\nCaLrn is then integrated in a cascading manner at multiple time scales:\nCaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase).", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"learn_neur"}}, {Tool: "gosl", Directive: "end", Args: []string{"learn_neur"}}, {Tool: "gosl", Directive: "start", Args: []string{"learn_neur"}}}, Fields: []gti.Field{{Name: "Norm", Doc: "denomenator used for normalizing CaLrn, so the max is roughly 1 - 1.5 or so, which works best in terms of previous standard learning rules, and overall learning performance"}, {Name: "SpkVGCC", Doc: "use spikes to generate VGCC instead of actual VGCC current -- see SpkVGCCa for calcium contribution from each spike"}, {Name: "SpkVgccCa", Doc: "multiplier on spike for computing Ca contribution to CaLrn in SpkVGCC mode"}, {Name: "VgccTau", Doc: "time constant of decay for VgccCa calcium -- it is highly transient around spikes, so decay and diffusion factors are more important than for long-lasting NMDA factor.  VgccCa is integrated separately int VgccCaInt prior to adding into NMDA Ca in CaLrn"}, {Name: "Dt", Doc: "time constants for integrating CaLrn across M, P and D cascading levels"}, {Name: "UpdtThr", Doc: "Threshold on CaSpkP CaSpkD value for updating synapse-level Ca values (SynCa) -- this is purely a performance optimization that excludes random infrequent spikes -- 0.05 works well on larger networks but not smaller, which require the .01 default."}, {Name: "VgccDt", Doc: "rate = 1 / tau"}, {Name: "NormInv", Doc: "= 1 / Norm"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.CaSpkParams", IDName: "ca-spk-params", Doc: "CaSpkParams parameterizes the neuron-level spike-driven calcium\nsignals, starting with CaSyn that is integrated at the neuron level\nand drives synapse-level, pre * post Ca integration, which provides the Tr\ntrace that multiplies error signals, and drives learning directly for Target layers.\nCaSpk* values are integrated separately at the Neuron level and used for UpdtThr\nand RLRate as a proxy for the activation (spiking) based learning signal.", Fields: []gti.Field{{Name: "SpikeG", Doc: "gain multiplier on spike for computing CaSpk: increasing this directly affects the magnitude of the trace values, learning rate in Target layers, and other factors that depend on CaSpk values: RLRate, UpdtThr.  Prjn.KinaseCa.SpikeG provides an additional gain factor specific to the synapse-level trace factors, without affecting neuron-level CaSpk values.  Larger networks require higher gain factors at the neuron level -- 12, vs 8 for smaller."}, {Name: "SynTau", Doc: "time constant for integrating spike-driven calcium trace at sender and recv neurons, CaSyn, which then drives synapse-level integration of the joint pre * post synapse-level activity, in cycles (msec).  Note: if this param is changed, then there will be a change in effective learning rate that can be compensated for by multiplying PrjnParams.Learn.KinaseCa.SpikeG by sqrt(30 / sqrt(SynTau)"}, {Name: "SynDt", Doc: "rate = 1 / tau"}, {Name: "pad"}, {Name: "Dt", Doc: "time constants for integrating CaSpk across M, P and D cascading levels -- these are typically the same as in CaLrn and Prjn level for synaptic integration, except for the M factor."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.TrgAvgActParams", IDName: "trg-avg-act-params", Doc: "TrgAvgActParams govern the target and actual long-term average activity in neurons.\nTarget value is adapted by neuron-wise error and difference in actual vs. target.\ndrives synaptic scaling at a slow timescale (Network.SlowInterval).", Fields: []gti.Field{{Name: "On", Doc: "whether to use target average activity mechanism to scale synaptic weights"}, {Name: "GiBaseInit", Doc: "if this is > 0, then each neuron's GiBase is initialized as this proportion of TrgRange.Max - TrgAvg -- gives neurons differences in intrinsic inhibition / leak as a starting bias"}, {Name: "ErrLRate", Doc: "learning rate for adjustments to Trg value based on unit-level error signal.  Population TrgAvg values are renormalized to fixed overall average in TrgRange. Generally, deviating from the default doesn't make much difference."}, {Name: "SynScaleRate", Doc: "rate parameter for how much to scale synaptic weights in proportion to the AvgDif between target and actual proportion activity -- this determines the effective strength of the constraint, and larger models may need more than the weaker default value."}, {Name: "SubMean", Doc: "amount of mean trg change to subtract -- 1 = full zero sum.  1 works best in general -- but in some cases it may be better to start with 0 and then increase using network SetSubMean method at a later point."}, {Name: "Permute", Doc: "permute the order of TrgAvg values within layer -- otherwise they are just assigned in order from highest to lowest for easy visualization -- generally must be true if any topographic weights are being used"}, {Name: "Pool", Doc: "use pool-level target values if pool-level inhibition and 4D pooled layers are present -- if pool sizes are relatively small, then may not be useful to distribute targets just within pool"}, {Name: "pad"}, {Name: "TrgRange", Doc: "range of target normalized average activations -- individual neurons are assigned values within this range to TrgAvg, and clamped within this range."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.RLRateParams", IDName: "rl-rate-params", Doc: "RLRateParams are recv neuron learning rate modulation parameters.\nHas two factors: the derivative of the sigmoid based on CaSpkD\nactivity levels, and based on the phase-wise differences in activity (Diff).", Fields: []gti.Field{{Name: "On", Doc: "use learning rate modulation"}, {Name: "SigmoidMin", Doc: "minimum learning rate multiplier for sigmoidal act (1-act) factor -- prevents lrate from going too low for extreme values.  Set to 1 to disable Sigmoid derivative factor, which is default for Target layers."}, {Name: "Diff", Doc: "modulate learning rate as a function of plus - minus differences"}, {Name: "SpkThr", Doc: "threshold on Max(CaSpkP, CaSpkD) below which Min lrate applies -- must be > 0 to prevent div by zero"}, {Name: "DiffThr", Doc: "threshold on recv neuron error delta, i.e., |CaSpkP - CaSpkD| below which lrate is at Min value"}, {Name: "Min", Doc: "for Diff component, minimum learning rate value when below ActDiffThr"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LearnNeurParams", IDName: "learn-neur-params", Doc: "axon.LearnNeurParams manages learning-related parameters at the neuron-level.\nThis is mainly the running average activations that drive learning", Fields: []gti.Field{{Name: "CaLearn", Doc: "parameterizes the neuron-level calcium signals driving learning: CaLrn = NMDA + VGCC Ca sources, where VGCC can be simulated from spiking or use the more complex and dynamic VGCC channel directly.  CaLrn is then integrated in a cascading manner at multiple time scales: CaM (as in calmodulin), CaP (ltP, CaMKII, plus phase), CaD (ltD, DAPK1, minus phase)."}, {Name: "CaSpk", Doc: "parameterizes the neuron-level spike-driven calcium signals, starting with CaSyn that is integrated at the neuron level, and drives synapse-level, pre * post Ca integration, which provides the Tr trace that multiplies error signals, and drives learning directly for Target layers. CaSpk* values are integrated separately at the Neuron level and used for UpdtThr and RLRate as a proxy for the activation (spiking) based learning signal."}, {Name: "LrnNMDA", Doc: "NMDA channel parameters used for learning, vs. the ones driving activation -- allows exploration of learning parameters independent of their effects on active maintenance contributions of NMDA, and may be supported by different receptor subtypes"}, {Name: "TrgAvgAct", Doc: "synaptic scaling parameters for regulating overall average activity compared to neuron's own target level"}, {Name: "RLRate", Doc: "recv neuron learning rate modulation params -- an additional error-based modulation of learning for receiver side: RLRate = |CaSpkP - CaSpkD| / Max(CaSpkP, CaSpkD)"}, {Name: "NeuroMod", Doc: "neuromodulation effects on learning rate and activity, as a function of layer-level DA and ACh values, which are updated from global Context values, and computed from reinforcement learning algorithms"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SWtInitParams", IDName: "s-wt-init-params", Doc: "SWtInitParams for initial SWt values", Fields: []gti.Field{{Name: "SPct", Doc: "how much of the initial random weights are captured in the SWt values -- rest goes into the LWt values.  1 gives the strongest initial biasing effect, for larger models that need more structural support. 0.5 should work for most models where stronger constraints are not needed."}, {Name: "Mean", Doc: "target mean weight values across receiving neuron's projection -- the mean SWt values are constrained to remain at this value.  some projections may benefit from lower mean of .4"}, {Name: "Var", Doc: "initial variance in weight values, prior to constraints."}, {Name: "Sym", Doc: "symmetrize the initial weight values with those in reciprocal projection -- typically true for bidirectional excitatory connections"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SWtAdaptParams", IDName: "s-wt-adapt-params", Doc: "SWtAdaptParams manages adaptation of SWt values", Fields: []gti.Field{{Name: "On", Doc: "if true, adaptation is active -- if false, SWt values are not updated, in which case it is generally good to have Init.SPct=0 too."}, {Name: "LRate", Doc: "learning rate multiplier on the accumulated DWt values (which already have fast LRate applied) to incorporate into SWt during slow outer loop updating -- lower values impose stronger constraints, for larger networks that need more structural support, e.g., 0.001 is better after 1,000 epochs in large models.  0.1 is fine for smaller models."}, {Name: "SubMean", Doc: "amount of mean to subtract from SWt delta when updating -- generally best to set to 1"}, {Name: "SigGain", Doc: "gain of sigmoidal constrast enhancement function used to transform learned, linear LWt values into Wt values"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SWtParams", IDName: "s-wt-params", Doc: "SWtParams manages structural, slowly adapting weight values (SWt),\nin terms of initialization and updating over course of learning.\nSWts impose initial and slowly adapting constraints on neuron connectivity\nto encourage differentiation of neuron representations and overall good behavior\nin terms of not hogging the representational space.\nThe TrgAvg activity constraint is not enforced through SWt -- it needs to be\nmore dynamic and supported by the regular learned weights.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []gti.Field{{Name: "Init", Doc: "initialization of SWt values"}, {Name: "Adapt", Doc: "adaptation of SWt values in response to LWt learning"}, {Name: "Limit", Doc: "range limits for SWt values"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LRateParams", IDName: "l-rate-params", Doc: "LRateParams manages learning rate parameters", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []gti.Field{{Name: "Base", Doc: "base learning rate for this projection -- can be modulated by other factors below -- for larger networks, use slower rates such as 0.04, smaller networks can use faster 0.2."}, {Name: "Sched", Doc: "scheduled learning rate multiplier, simulating reduction in plasticity over aging"}, {Name: "Mod", Doc: "dynamic learning rate modulation due to neuromodulatory or other such factors"}, {Name: "Eff", Doc: "effective actual learning rate multiplier used in computing DWt: Eff = eMod * Sched * Base"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.TraceParams", IDName: "trace-params", Doc: "TraceParams manages learning rate parameters", Fields: []gti.Field{{Name: "Tau", Doc: "time constant for integrating trace over theta cycle timescales -- governs the decay rate of syanptic trace"}, {Name: "SubMean", Doc: "amount of the mean dWt to subtract, producing a zero-sum effect -- 1.0 = full zero-sum dWt -- only on non-zero DWts.  typically set to 0 for standard trace learning projections, although some require it for stability over the long haul.  can use SetSubMean to set to 1 after significant early learning has occurred with 0.  Some special prjn types (e.g., Hebb) benefit from SubMean = 1 always"}, {Name: "LearnThr", Doc: "threshold for learning, depending on different algorithms -- in Matrix and VSPatch it applies to normalized GeIntNorm value -- setting this relatively high encourages sparser representations"}, {Name: "Dt", Doc: "rate = 1 / tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LRateMod", IDName: "l-rate-mod", Doc: "LRateMod implements global learning rate modulation, based on a performance-based\nfactor, for example error.  Increasing levels of the factor = higher learning rate.\nThis can be added to a Sim and called prior to DWt() to dynamically change lrate\nbased on overall network performance.", Fields: []gti.Field{{Name: "On", Doc: "toggle use of this modulation factor"}, {Name: "Base", Doc: "baseline learning rate -- what you get for correct cases"}, {Name: "pad"}, {Name: "pad1"}, {Name: "Range", Doc: "defines the range over which modulation occurs for the modulator factor -- Min and below get the Base level of learning rate modulation, Max and above get a modulation of 1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LearnSynParams", IDName: "learn-syn-params", Doc: "LearnSynParams manages learning-related parameters at the synapse-level.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"learn"}}}, Fields: []gti.Field{{Name: "Learn", Doc: "enable learning for this projection"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "LRate", Doc: "learning rate parameters, supporting two levels of modulation on top of base learning rate."}, {Name: "Trace", Doc: "trace-based learning parameters"}, {Name: "KinaseCa", Doc: "kinase calcium Ca integration parameters"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.Network", IDName: "network", Doc: "axon.Network implements the Axon spiking model,\nbuilding on the algorithm-independent NetworkBase that manages\nall the infrastructure.", Methods: []gti.Method{{Name: "InitWts", Doc: "InitWts initializes synaptic weights and all other associated long-term state variables\nincluding running-average state values (e.g., layer running average activations etc)", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"ctx"}}, {Name: "InitActs", Doc: "InitActs fully initializes activation state -- not automatically called", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"ctx"}}}, Embeds: []gti.Field{{Name: "NetworkBase"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NetworkBase", IDName: "network-base", Doc: "NetworkBase manages the basic structural components of a network (layers).\nThe main Network then can just have the algorithm-specific code.", Methods: []gti.Method{{Name: "ShowAllGlobals", Doc: "ShowAllGlobals shows a listing of all Global variables and values.", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}}, {Name: "Build", Doc: "Build constructs the layer and projection state based on the layer shapes\nand patterns of interconnectivity. Configures threading using heuristics based\non final network size.  Must set UseGPUOrder properly prior to calling.\nConfigures the given Context object used in the simulation with the memory\naccess strides for this network -- must be set properly -- see SetCtxStrides.", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"simCtx"}, Returns: []string{"error"}}, {Name: "SaveWtsJSON", Doc: "SaveWtsJSON saves network weights (and any other state that adapts with learning)\nto a JSON-formatted file.  If filename has .gz extension, then file is gzip compressed.", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"filename"}, Returns: []string{"error"}}, {Name: "OpenWtsJSON", Doc: "OpenWtsJSON opens network weights (and any other state that adapts with learning)\nfrom a JSON-formatted file.  If filename has .gz extension, then file is gzip uncompressed.", Directives: []gti.Directive{{Tool: "gti", Directive: "add"}}, Args: []string{"filename"}, Returns: []string{"error"}}}, Fields: []gti.Field{{Name: "EmerNet", Doc: "we need a pointer to ourselves as an emer.Network, which can always be used to extract the true underlying type of object when network is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Nm", Doc: "overall name of network -- helps discriminate if there are multiple"}, {Name: "WtsFile", Doc: "filename of last weights file loaded or saved"}, {Name: "PVLV", Doc: "PVLV system for phasic dopamine signaling, including internal drives, US outcomes.  Core LHb (lateral habenula) and VTA (ventral tegmental area) dopamine are computed in equations using inputs from specialized network layers (LDTLayer driven by BLA, CeM layers, VSPatchLayer).  Renders USLayer, PVLayer, DrivesLayer representations based on state updated here."}, {Name: "LayMap", Doc: "map of name to layers -- layer names must be unique"}, {Name: "LayClassMap", Doc: "map of layer classes -- made during Build"}, {Name: "MinPos", Doc: "minimum display position in network"}, {Name: "MaxPos", Doc: "maximum display position in network"}, {Name: "MetaData", Doc: "optional metadata that is saved in network weights files -- e.g., can indicate number of epochs that were trained, or any other information about this network that would be useful to save"}, {Name: "UseGPUOrder", Doc: "if true, the neuron and synapse variables will be organized into a gpu-optimized memory order, otherwise cpu-optimized. This must be set before network Build() is called."}, {Name: "NetIdx", Doc: "network index in global Networks list of networks -- needed for GPU shader kernel compatible network variable access functions (e.g., NrnV, SynV etc) in CPU mode"}, {Name: "MaxDelay", Doc: "maximum synaptic delay across any projection in the network -- used for sizing the GBuf accumulation buffer."}, {Name: "MaxData", Doc: "maximum number of data inputs that can be processed in parallel in one pass of the network. Neuron storage is allocated to hold this amount during Build process, and this value reflects that."}, {Name: "NNeurons", Doc: "total number of neurons"}, {Name: "NSyns", Doc: "total number of synapses"}, {Name: "Globals", Doc: "storage for global vars"}, {Name: "Layers", Doc: "array of layers"}, {Name: "LayParams", Doc: "array of layer parameters, in 1-to-1 correspondence with Layers"}, {Name: "LayVals", Doc: "array of layer values, with extra per data"}, {Name: "Pools", Doc: "array of inhibitory pools for all layers."}, {Name: "Neurons", Doc: "entire network's allocation of neuron variables, accessed via NrnV function with flexible striding"}, {Name: "NeuronAvgs", Doc: "] entire network's allocation of neuron average avariables, accessed via NrnAvgV function with flexible striding"}, {Name: "NeuronIxs", Doc: "entire network's allocation of neuron index variables, accessed via NrnI function with flexible striding"}, {Name: "Prjns", Doc: "pointers to all projections in the network, sender-based"}, {Name: "PrjnParams", Doc: "array of projection parameters, in 1-to-1 correspondence with Prjns, sender-based"}, {Name: "SynapseIxs", Doc: "entire network's allocation of synapse idx vars, organized sender-based, with flexible striding, accessed via SynI function"}, {Name: "Synapses", Doc: "entire network's allocation of synapses, organized sender-based, with flexible striding, accessed via SynV function"}, {Name: "SynapseCas", Doc: "entire network's allocation of synapse Ca vars, organized sender-based, with flexible striding, accessed via SynCaV function"}, {Name: "PrjnSendCon", Doc: "starting offset and N cons for each sending neuron, for indexing into the Syns synapses, which are organized sender-based."}, {Name: "PrjnRecvCon", Doc: "starting offset and N cons for each recv neuron, for indexing into the RecvSynIdx array of indexes into the Syns synapses, which are organized sender-based."}, {Name: "PrjnGBuf", Doc: "conductance buffer for accumulating spikes -- subslices are allocated to each projection -- uses int-encoded float values for faster GPU atomic integration"}, {Name: "PrjnGSyns", Doc: "synaptic conductance integrated over time per projection per recv neurons -- spikes come in via PrjnBuf -- subslices are allocated to each projection"}, {Name: "RecvPrjnIdxs", Doc: "indexes into Prjns (organized by SendPrjn) organized by recv projections -- needed for iterating through recv prjns efficiently on GPU."}, {Name: "RecvSynIdxs", Doc: "indexes into Synapses for each recv neuron, organized into blocks according to PrjnRecvCon, for receiver-based access."}, {Name: "Exts", Doc: "external input values for all Input / Target / Compare layers in the network -- the ApplyExt methods write to this per layer, and it is then actually applied in one consistent method."}, {Name: "Ctx", Doc: "context used only for accessing neurons for display -- NetIdxs.NData in here is copied from active context in NewState"}, {Name: "Rand", Doc: "random number generator for the network -- all random calls must use this -- set seed here for weight initialization values"}, {Name: "RndSeed", Doc: "random seed to be set at the start of configuring the network and initializing the weights -- set this to get a different set of weights"}, {Name: "NThreads", Doc: "number of threads to use for parallel processing"}, {Name: "GPU", Doc: "GPU implementation"}, {Name: "RecFunTimes", Doc: "record function timer information"}, {Name: "FunTimes", Doc: "timers for each major function (step of processing)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.DAModTypes", IDName: "da-mod-types", Doc: "DAModTypes are types of dopamine modulation of neural activity."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.ValenceTypes", IDName: "valence-types", Doc: "ValenceTypes are types of valence coding: positive or negative."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuroModParams", IDName: "neuro-mod-params", Doc: "NeuroModParams specifies the effects of neuromodulators on neural\nactivity and learning rate.  These can apply to any neuron type,\nand are applied in the core cycle update equations.", Fields: []gti.Field{{Name: "DAMod", Doc: "dopamine receptor-based effects of dopamine modulation on excitatory and inhibitory conductances: D1 is excitatory, D2 is inhibitory as a function of increasing dopamine"}, {Name: "Valence", Doc: "valence coding of this layer -- may affect specific layer types but does not directly affect neuromodulators currently"}, {Name: "DAModGain", Doc: "multiplicative factor on overall DA modulation specified by DAMod -- resulting overall gain factor is: 1 + DAModGain * DA, where DA is appropriate DA-driven factor"}, {Name: "DALRateSign", Doc: "modulate the sign of the learning rate factor according to the DA sign, taking into account the DAMod sign reversal for D2Mod, also using BurstGain and DipGain to modulate DA value -- otherwise, only the magnitude of the learning rate is modulated as a function of raw DA magnitude according to DALRateMod (without additional gain factors)"}, {Name: "DALRateMod", Doc: "if not using DALRateSign, this is the proportion of maximum learning rate that Abs(DA) magnitude can modulate -- e.g., if 0.2, then DA = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChLRateMod", Doc: "proportion of maximum learning rate that ACh can modulate -- e.g., if 0.2, then ACh = 0 = 80% of std learning rate, 1 = 100%"}, {Name: "AChDisInhib", Doc: "amount of extra Gi inhibition added in proportion to 1 - ACh level -- makes ACh disinhibitory"}, {Name: "BurstGain", Doc: "multiplicative gain factor applied to positive dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign!"}, {Name: "DipGain", Doc: "multiplicative gain factor applied to negative dopamine signals -- this operates on the raw dopamine signal prior to any effect of D2 receptors in reversing its sign! should be small for acq, but roughly equal to burst for ext"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronFlags", IDName: "neuron-flags", Doc: "NeuronFlags are bit-flags encoding relevant binary state for neurons"})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronVars", IDName: "neuron-vars", Doc: "NeuronVars are the neuron variables representing current active state,\nspecific to each input data state.\nSee NeuronAvgVars for vars shared across data."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVars", IDName: "neuron-avg-vars", Doc: "NeuronAvgVars are mostly neuron variables involved in longer-term average activity\nwhich is aggregated over time and not specific to each input data state,\nalong with any other state that is not input data specific."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronIdxs", IDName: "neuron-idxs", Doc: "NeuronIdxs are the neuron indexes and other uint32 values.\nThere is only one of these per neuron -- not data parallel.\nnote: Flags are encoded in Vars because they are data parallel and\nwritable, whereas indexes are read-only."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronVarStrides", IDName: "neuron-var-strides", Doc: "NeuronVarStrides encodes the stride offsets for neuron variable access\ninto network float32 array.  Data is always the inner-most variable.", Directives: []gti.Directive{{Tool: "gosl", Directive: "end", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "end", Args: []string{"neuron"}}, {Tool: "gosl", Directive: "start", Args: []string{"neuron"}}}, Fields: []gti.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronAvgVarStrides", IDName: "neuron-avg-var-strides", Doc: "NeuronAvgVarStrides encodes the stride offsets for neuron variable access\ninto network float32 array.  Data is always the inner-most variable.", Fields: []gti.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.NeuronIdxStrides", IDName: "neuron-idx-strides", Doc: "NeuronIdxStrides encodes the stride offsets for neuron index access\ninto network uint32 array.", Fields: []gti.Field{{Name: "Neuron", Doc: "neuron level"}, {Name: "Index", Doc: "index value level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.MatrixParams", IDName: "matrix-params", Doc: "MatrixParams has parameters for BG Striatum Matrix MSN layers\nThese are the main Go / NoGo gating units in BG.\nDA, ACh learning rate modulation is pre-computed on the recv neuron\nRLRate variable via NeuroMod.  Also uses Pool.Gated for InvertNoGate,\nupdated in PlusPhase prior to DWt call.\nMust set Learn.NeuroMod.DAMod = D1Mod or D2Mod via SetBuildConfig(\"DAMod\").", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pcore_layers"}}}, Fields: []gti.Field{{Name: "GateThr", Doc: "threshold on layer Avg SpkMax for Matrix Go and VThal layers to count as having gated"}, {Name: "IsVS", Doc: "is this a ventral striatum (VS) matrix layer?  if true, the gating status of this layer is recorded in the Global state, and used for updating effort and other factors."}, {Name: "OtherMatrixIdx", Doc: "index of other matrix (Go if we are NoGo and vice-versa).    Set during Build from BuildConfig OtherMatrixName"}, {Name: "ThalLay1Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay1Name if present -- -1 if not used"}, {Name: "ThalLay2Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay2Name if present -- -1 if not used"}, {Name: "ThalLay3Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay3Name if present -- -1 if not used"}, {Name: "ThalLay4Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay4Name if present -- -1 if not used"}, {Name: "ThalLay5Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay5Name if present -- -1 if not used"}, {Name: "ThalLay6Idx", Doc: "index of thalamus layer that we gate.  needed to get gating information.  Set during Build from BuildConfig ThalLay6Name if present -- -1 if not used"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GPLayerTypes", IDName: "gp-layer-types", Doc: "GPLayerTypes is a GPLayer axon-specific layer type enum."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GPParams", IDName: "gp-params", Doc: "GPLayer represents a globus pallidus layer, including:\nGPePr, GPeAk (arkypallidal), and GPi (see GPType for type).\nTypically just a single unit per Pool representing a given stripe.", Fields: []gti.Field{{Name: "GPType", Doc: "type of GP Layer -- must set during config using SetBuildConfig of GPType."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.MatrixPrjnParams", IDName: "matrix-prjn-params", Doc: "MatrixPrjnParams for trace-based learning in the MatrixPrjn.\nA trace of synaptic co-activity is formed, and then modulated by dopamine\nwhenever it occurs.  This bridges the temporal gap between gating activity\nand subsequent activity, and is based biologically on synaptic tags.\nTrace is applied to DWt and reset at the time of reward.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pcore_prjns"}}}, Fields: []gti.Field{{Name: "NoGateLRate", Doc: "learning rate for when ACh was elevated but no gating took place, in proportion to the level of ACh that indicates the salience of the event.  A low level of this learning prevents the highly maladaptive situation where the BG is not gating and thus no learning can occur."}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.AvgMaxPhases", IDName: "avg-max-phases", Doc: "AvgMaxPhases contains the average and maximum values over a Pool of neurons,\nat different time scales within a standard ThetaCycle of updating.\nIt is much more efficient on the GPU to just grab everything in one pass at\nthe cycle level, and then take snapshots from there.\nAll of the cycle level values are updated at the *start* of the cycle\nbased on values from the prior cycle -- thus are 1 cycle behind in general.", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"pool"}}, {Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "start", Args: []string{"pool"}}}, Fields: []gti.Field{{Name: "Cycle", Doc: "updated every cycle -- this is the source of all subsequent time scales"}, {Name: "Minus", Doc: "at the end of the minus phase"}, {Name: "Plus", Doc: "at the end of the plus phase"}, {Name: "Prev", Doc: "at the end of the previous plus phase"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PoolAvgMax", IDName: "pool-avg-max", Doc: "PoolAvgMax contains the average and maximum values over a Pool of neurons\nfor different variables of interest, at Cycle, Minus and Plus phase timescales.\nAll of the cycle level values are updated at the *start* of the cycle\nbased on values from the prior cycle -- thus are 1 cycle behind in general.", Fields: []gti.Field{{Name: "CaSpkP", Doc: "avg and maximum CaSpkP (continuously updated at roughly 40 msec integration window timescale, ends up capturing potentiation, plus-phase signal) -- this is the primary variable to use for tracking overall pool activity"}, {Name: "CaSpkD", Doc: "avg and maximum CaSpkD longer-term depression / DAPK1 signal in layer"}, {Name: "SpkMax", Doc: "avg and maximum SpkMax value (based on CaSpkP) -- reflects peak activity at any point across the cycle"}, {Name: "Act", Doc: "avg and maximum Act firing rate value"}, {Name: "GeInt", Doc: "avg and maximum GeInt integrated running-average excitatory conductance value"}, {Name: "GiInt", Doc: "avg and maximum GiInt integrated running-average inhibitory conductance value"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.Pool", IDName: "pool", Doc: "Pool contains computed values for FS-FFFB inhibition,\nand various other state values for layers\nand pools (unit groups) that can be subject to inhibition", Directives: []gti.Directive{{Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"pool"}}, {Tool: "gosl", Directive: "end", Args: []string{"pool"}}, {Tool: "gosl", Directive: "start", Args: []string{"pool"}}}, Fields: []gti.Field{{Name: "StIdx", Doc: "starting and ending (exlusive) layer-wise indexes for the list of neurons in this pool"}, {Name: "EdIdx", Doc: "starting and ending (exlusive) layer-wise indexes for the list of neurons in this pool"}, {Name: "LayIdx", Doc: "layer index in global layer list"}, {Name: "DataIdx", Doc: "data parallel index (innermost index per layer)"}, {Name: "PoolIdx", Doc: "pool index in global pool list:"}, {Name: "IsLayPool", Doc: "is this a layer-wide pool?  if not, it represents a sub-pool of units within a 4D layer"}, {Name: "Gated", Doc: "for special types where relevant (e.g., MatrixLayer, BGThalLayer), indicates if the pool was gated"}, {Name: "pad"}, {Name: "Inhib", Doc: "fast-slow FFFB inhibition values"}, {Name: "AvgMax", Doc: "average and max values for relevant variables in this pool, at different time scales"}, {Name: "AvgDif", Doc: "absolute value of AvgDif differences from actual neuron ActPct relative to TrgAvg"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.Prjn", IDName: "prjn", Doc: "axon.Prjn is a basic Axon projection with synaptic learning parameters", Embeds: []gti.Field{{Name: "PrjnBase"}}, Fields: []gti.Field{{Name: "Params", Doc: "all prjn-level parameters -- these must remain constant once configured"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnBase", IDName: "prjn-base", Doc: "PrjnBase contains the basic structural information for specifying a projection of synaptic\nconnections between two layers, and maintaining all the synaptic connection-level data.\nThe same struct token is added to the Recv and Send layer prjn lists, and it manages everything\nabout the connectivity, and methods on the Prjn handle all the relevant computation.\nThe Base does not have algorithm-specific methods and parameters, so it can be easily\nreused for different algorithms, and cleanly separates the algorithm-specific code.\nAny dependency on the algorithm-level Prjn can be captured in the AxonPrjn interface,\naccessed via the AxonPrj field.", Fields: []gti.Field{{Name: "AxonPrj", Doc: "we need a pointer to ourselves as an AxonPrjn, which can always be used to extract the true underlying type of object when prjn is embedded in other structs -- function receivers do not have this ability so this is necessary."}, {Name: "Off", Doc: "inactivate this projection -- allows for easy experimentation"}, {Name: "Cls", Doc: "Class is for applying parameter styles, can be space separated multple tags"}, {Name: "Notes", Doc: "can record notes about this projection here"}, {Name: "Send", Doc: "sending layer for this projection"}, {Name: "Recv", Doc: "receiving layer for this projection"}, {Name: "Pat", Doc: "pattern of connectivity"}, {Name: "Typ", Doc: "type of projection -- Forward, Back, Lateral, or extended type in specialized algorithms -- matches against .Cls parameter styles (e.g., .Back etc)"}, {Name: "DefParams", Doc: "default parameters that are applied prior to user-set parameters -- these are useful for specific functionality in specialized brain areas (e.g., PVLV, BG etc) not associated with a prjn type, which otherwise is used to hard-code initial default parameters -- typically just set to a literal map."}, {Name: "ParamsHistory", Doc: "provides a history of parameters applied to the layer"}, {Name: "RecvConNAvgMax", Doc: "average and maximum number of recv connections in the receiving layer"}, {Name: "SendConNAvgMax", Doc: "average and maximum number of sending connections in the sending layer"}, {Name: "SynStIdx", Doc: "start index into global Synapse array:"}, {Name: "NSyns", Doc: "number of synapses in this projection"}, {Name: "RecvCon", Doc: "starting offset and N cons for each recv neuron, for indexing into the RecvSynIdx array of indexes into the Syns synapses, which are organized sender-based.  This is locally-managed during build process, but also copied to network global PrjnRecvCons slice for GPU usage."}, {Name: "RecvSynIdx", Doc: "index into Syns synaptic state for each sending unit and connection within that, for the sending projection which does not own the synapses, and instead indexes into recv-ordered list"}, {Name: "RecvConIdx", Doc: "for each recv synapse, this is index of *sending* neuron  It is generally preferable to use the Synapse SendIdx where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "SendCon", Doc: "starting offset and N cons for each sending neuron, for indexing into the Syns synapses, which are organized sender-based.  This is locally-managed during build process, but also copied to network global PrjnSendCons slice for GPU usage."}, {Name: "SendConIdx", Doc: "index of other neuron that receives the sender's synaptic input, ordered by the sending layer's order of units as the outer loop, and SendCon.N receiving units within that.  It is generally preferable to use the Synapse RecvIdx where needed, instead of this slice, because then the memory access will be close by other values on the synapse."}, {Name: "GBuf", Doc: "Ge or Gi conductance ring buffer for each neuron, accessed through Params.Com.ReadIdx, WriteIdx -- scale * weight is added with Com delay offset -- a subslice from network PrjnGBuf. Uses int-encoded float values for faster GPU atomic integration"}, {Name: "GSyns", Doc: "projection-level synaptic conductance values, integrated by prjn before being integrated at the neuron level, which enables the neuron to perform non-linear integration as needed -- a subslice from network PrjnGSyn."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.StartN", IDName: "start-n", Doc: "StartN holds a starting offset index and a number of items\narranged from Start to Start+N (exclusive).\nThis is not 16 byte padded and only for use on CPU side.", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"prjnparams"}}, {Tool: "gosl", Directive: "end", Args: []string{"prjnparams"}}, {Tool: "gosl", Directive: "start", Args: []string{"prjnparams"}}}, Fields: []gti.Field{{Name: "Start", Doc: "starting offset"}, {Name: "N", Doc: "number of items --"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnIdxs", IDName: "prjn-idxs", Doc: "PrjnIdxs contains prjn-level index information into global memory arrays", Fields: []gti.Field{{Name: "PrjnIdx"}, {Name: "RecvLay"}, {Name: "RecvNeurSt"}, {Name: "RecvNeurN"}, {Name: "SendLay"}, {Name: "SendNeurSt"}, {Name: "SendNeurN"}, {Name: "SynapseSt"}, {Name: "SendConSt"}, {Name: "RecvConSt"}, {Name: "RecvSynSt"}, {Name: "GBufSt"}, {Name: "GSynSt"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GScaleVals", IDName: "g-scale-vals", Doc: "GScaleVals holds the conductance scaling values.\nThese are computed once at start and remain constant thereafter,\nand therefore belong on Params and not on PrjnVals.", Fields: []gti.Field{{Name: "Scale", Doc: "scaling factor for integrating synaptic input conductances (G's), originally computed as a function of sending layer activity and number of connections, and typically adapted from there -- see Prjn.PrjnScale adapt params"}, {Name: "Rel", Doc: "normalized relative proportion of total receiving conductance for this projection: PrjnScale.Rel / sum(PrjnScale.Rel across relevant prjns)"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnParams", IDName: "prjn-params", Doc: "PrjnParams contains all of the prjn parameters.\nThese values must remain constant over the course of computation.\nOn the GPU, they are loaded into a uniform.", Fields: []gti.Field{{Name: "PrjnType", Doc: "functional type of prjn, which determines functional code path\nfor specialized layer types, and is synchronized with the Prjn.Typ value"}, {Name: "pad"}, {Name: "pad1"}, {Name: "pad2"}, {Name: "Idxs", Doc: "recv and send neuron-level projection index array access info"}, {Name: "Com", Doc: "synaptic communication parameters: delay, probability of failure"}, {Name: "PrjnScale", Doc: "projection scaling parameters for computing GScale:\nmodulates overall strength of projection, using both\nabsolute and relative factors, with adaptation option to maintain target max conductances"}, {Name: "SWts", Doc: "slowly adapting, structural weight value parameters,\nwhich control initial weight values and slower outer-loop adjustments"}, {Name: "Learn", Doc: "synaptic-level learning parameters for learning in the fast LWt values."}, {Name: "GScale", Doc: "conductance scaling values"}, {Name: "RLPred", Doc: "Params for RWPrjn and TDPredPrjn for doing dopamine-modulated learning\nfor reward prediction: Da * Send activity.\nUse in RWPredLayer or TDPredLayer typically to generate reward predictions.\nIf the Da sign is positive, the first recv unit learns fully; for negative,\nsecond one learns fully.\nLower lrate applies for opposite cases.  Weights are positive-only."}, {Name: "Matrix", Doc: "for trace-based learning in the MatrixPrjn. A trace of synaptic co-activity\nis formed, and then modulated by dopamine whenever it occurs.\nThis bridges the temporal gap between gating activity and subsequent activity,\nand is based biologically on synaptic tags.\nTrace is reset at time of reward based on ACh level from CINs."}, {Name: "BLA", Doc: "Basolateral Amygdala projection parameters."}, {Name: "Hip", Doc: "Hip bench parameters."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PrjnTypes", IDName: "prjn-types", Doc: "PrjnTypes is an axon-specific prjn type enum,\nthat encompasses all the different algorithm types supported.\nClass parameter styles automatically key off of these types.\nThe first entries must be kept synchronized with the emer.PrjnType."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.DriveParams", IDName: "drive-params", Doc: "DriveParams manages the drive parameters for computing and updating drive state.\nMost of the params are for optional case where drives are automatically\nupdated based on US consumption (which satisfies drives) and time passing\n(which increases drives).", Fields: []gti.Field{{Name: "DriveMin", Doc: "minimum effective drive value -- this is an automatic baseline ensuring that a positive US results in at least some minimal level of reward.  Unlike Base values, this is not reflected in the activity of the drive values -- applies at the time of reward calculation as a minimum baseline."}, {Name: "Base", Doc: "baseline levels for each drive -- what they naturally trend toward in the absence of any input.  Set inactive drives to 0 baseline, active ones typically elevated baseline (0-1 range)."}, {Name: "Tau", Doc: "time constants in ThetaCycle (trial) units for natural update toward Base values -- 0 values means no natural update."}, {Name: "Satisfaction", Doc: "decrement in drive value when US is consumed, thus partially satisfying the drive -- positive values are subtracted from current Drive value."}, {Name: "Dt", Doc: "1/Tau"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.UrgencyParams", IDName: "urgency-params", Doc: "UrgencyParams has urgency (increasing pressure to do something)\nand parameters for updating it.\nRaw urgency integrates effort when _not_ goal engaged\nwhile effort (negative US 0) integrates when a goal _is_ engaged.", Fields: []gti.Field{{Name: "U50", Doc: "value of raw urgency where the urgency activation level is 50%"}, {Name: "Power", Doc: "exponent on the urge factor -- valid numbers are 1,2,4,6"}, {Name: "Thr", Doc: "threshold for urge -- cuts off small baseline values"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.USParams", IDName: "us-params", Doc: "USParams control how positive and negative USs are\nweighted and integrated to compute an overall PV primary value.", Fields: []gti.Field{{Name: "NegUSOutcomeThr", Doc: "threshold for a negative US increment, _after_ multiplying by the USnegGains factor for that US (to allow for normalized input magnitudes that may translate into different magnitude of effects), to drive a phasic ACh response and associated VSMatrix gating and dopamine firing -- i.e., a full negative US outcome event (global NegUSOutcome flag is set)"}, {Name: "PVposGain", Doc: "gain factor applied to sum of weighted, drive-scaled positive USs to compute PVpos primary value summary -- multiplied prior to 1/(1+x) normalization.  Use this to adjust the overall scaling of PVpos reward within 0-1 normalized range (see also PVnegGain).  Each USpos is assumed to be in 0-1 range, default 1."}, {Name: "PVnegGain", Doc: "gain factor applied to sum of weighted negative USs to compute PVneg primary value summary -- multiplied prior to 1/(1+x) normalization.  Use this to adjust overall scaling of PVneg within 0-1 normalized range (see also PVposGain)."}, {Name: "USnegGains", Doc: "gain factor for each individual negative US, multiplied prior to 1/(1+x) normalization of each term for activating the OFCnegUS pools.  These gains are _not_ applied in computing summary PVneg value (see PVnegWts), and generally must be larger than the weights to leverage the dynamic range within each US pool."}, {Name: "PVposWts", Doc: "weight factor applied to each separate positive US on the way to computing the overall PVpos summary value, to control the weighting of each US relative to the others. Each pos US is also multiplied by its dynamic Drive factor as well.  Use PVposGain to control the overall scaling of the PVpos value."}, {Name: "PVnegWts", Doc: "weight factor applied to each separate negative US on the way to computing the overall PVneg summary value, to control the weighting of each US relative to the others.  The first pool is Time, second is Effort, and these are typically weighted lower (.02) than salient simulation-specific USs (1)."}, {Name: "USposEst", Doc: "computed estimated US values, based on OFCposUSPT and VSMatrix gating, in PVposEst"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LHbParams", IDName: "l-hb-params", Doc: "LHbParams has values for computing LHb & RMTg which drives dips / pauses in DA firing.\nLHb handles all US-related (PV = primary value) processing.\nPositive net LHb activity drives dips / pauses in VTA DA activity,\ne.g., when predicted pos > actual or actual neg > predicted.\nNegative net LHb activity drives bursts in VTA DA activity,\ne.g., when actual pos > predicted (redundant with LV / Amygdala)\nor \"relief\" burst when actual neg < predicted.", Fields: []gti.Field{{Name: "NegThr", Doc: "threshold factor that multiplies integrated pvNeg value to establish a threshold for whether the integrated pvPos value is good enough to drive overall net positive reward"}, {Name: "BurstGain", Doc: "gain multiplier on PVpos for purposes of generating bursts (not for  discounting negative dips) -- 4 renormalizes for typical ~.5 values (.5 * .5 = .25)"}, {Name: "DipGain", Doc: "gain multiplier on PVneg for purposes of generating dips (not for  discounting positive bursts) -- 4 renormalizes for typical ~.5 values (.5 * .5 = .25)"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.GiveUpParams", IDName: "give-up-params", Doc: "GiveUpParams are parameters for computing when to give up", Fields: []gti.Field{{Name: "NegThr", Doc: "threshold factor that multiplies integrated pvNeg value to establish a threshold for whether the integrated pvPos value is good enough to drive overall net positive reward"}, {Name: "Gain", Doc: "multiplier on pos - neg for logistic probability function -- higher gain values produce more binary give up behavior and lower values produce more graded stochastic behavior around the threshold"}, {Name: "MinPVposEst", Doc: "minimum estimated PVpos value -- deals with any errors in the estimation process to make sure that erroneous GiveUp doesn't happen."}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.PVLV", IDName: "pvlv", Doc: "PVLV represents the core brainstem-level (hypothalamus) bodily drives\nand resulting dopamine from US (unconditioned stimulus) inputs,\nas computed by the PVLV model of primary value (PV)\nand learned value (LV), describing the functions of the Amygala,\nVentral Striatum, VTA and associated midbrain nuclei (LDT, LHb, RMTg).\nCore LHb (lateral habenula) and VTA (ventral tegmental area) dopamine\nare computed in equations using inputs from specialized network layers\n(LDTLayer driven by BLA, CeM layers, VSPatchLayer).\nThe Drives, Effort, US and resulting LHb PV dopamine computation all happens at the\nat the start of each trial (NewState, Step).  The LV / CS dopamine is computed\ncycle-by-cycle by the VTA layer using parameters set by the VTA layer.\nRenders USLayer, PVLayer, DrivesLayer representations based on state updated here.", Fields: []gti.Field{{Name: "NPosUSs", Doc: "number of possible positive US states and corresponding drives -- the first is always reserved for novelty / curiosity.  Must be set programmatically via SetNUSs method, which allocates corresponding parameters."}, {Name: "NNegUSs", Doc: "number of possible negative US states -- is reserved for accumulated time, the accumulated effort cost.  Must be set programmatically via SetNUSs method, which allocates corresponding parameters."}, {Name: "Drive", Doc: "parameters and state for built-in drives that form the core motivations of agent, controlled by lateral hypothalamus and associated body state monitoring such as glucose levels and thirst."}, {Name: "Urgency", Doc: "urgency (increasing pressure to do something) and parameters for updating it. Raw urgency is incremented by same units as effort, but is only reset with a positive US."}, {Name: "USs", Doc: "controls how positive and negative USs are weighted and integrated to compute an overall PV primary value."}, {Name: "LHb", Doc: "lateral habenula (LHb) parameters and state, which drives dipping / pausing in dopamine when the predicted positive outcome > actual, or actual negative outcome > predicted.  Can also drive bursting for the converse, and via matrix phasic firing"}, {Name: "GiveUp", Doc: "parameters for giving up based on PV pos - neg difference"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.LDTParams", IDName: "ldt-params", Doc: "LDTParams compute reward salience as ACh global neuromodulatory signal\nas a function of the MAX activation of its inputs.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pvlv_layers"}}}, Fields: []gti.Field{{Name: "SrcThr", Doc: "threshold per input source, on absolute value (magnitude), to count as a significant reward event, which then drives maximal ACh -- set to 0 to disable this nonlinear behavior"}, {Name: "Rew", Doc: "use the global Context.NeuroMod.HasRew flag -- if there is some kind of external reward being given, then ACh goes to 1, else 0 for this component"}, {Name: "MaintInhib", Doc: "extent to which active maintenance (via Context.NeuroMod.NotMaint PTNotMaintLayer activity) inhibits ACh signals -- when goal engaged, distractability is lower."}, {Name: "NotMaintMax", Doc: "maximum NeuroMod.NotMaint activity for computing Maint as 1-NotMaint -- when NotMaint is >= NotMaintMax, then Maint = 0."}, {Name: "SrcLay1Idx", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay1Name if present -- -1 if not used"}, {Name: "SrcLay2Idx", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay2Name if present -- -1 if not used"}, {Name: "SrcLay3Idx", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay3Name if present -- -1 if not used"}, {Name: "SrcLay4Idx", Doc: "idx of Layer to get max activity from -- set during Build from BuildConfig SrcLay4Name if present -- -1 if not used"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.VSPatchParams", IDName: "vs-patch-params", Doc: "VSPatchParams parameters for VSPatch learning", Fields: []gti.Field{{Name: "Gain", Doc: "multiplier applied after Thr threshold"}, {Name: "ThrInit", Doc: "initial value for overall threshold, which adapts over time -- stored in LayerVals.ActAvgVals.AdaptThr"}, {Name: "ThrLRate", Doc: "learning rate for the threshold -- moves in proportion to same predictive error signal that drives synaptic learning"}, {Name: "ThrNonRew", Doc: "extra gain factor for non-reward trials, which is the most critical"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.VTAParams", IDName: "vta-params", Doc: "VTAParams are for computing overall VTA DA based on LHb PVDA\n(primary value -- at US time, computed at start of each trial\nand stored in LHbPVDA global value)\nand Amygdala (CeM) CS / learned value (LV) activations, which update\nevery cycle.", Fields: []gti.Field{{Name: "CeMGain", Doc: "gain on CeM activity difference (CeMPos - CeMNeg) for generating LV CS-driven dopamine values"}, {Name: "LHbGain", Doc: "gain on computed LHb DA (Burst - Dip) -- for controlling DA levels"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.BLAPrjnParams", IDName: "bla-prjn-params", Doc: "BLAPrjnParams has parameters for basolateral amygdala learning.\nLearning is driven by the Tr trace as function of ACh * Send Act\nrecorded prior to US, and at US, recv unit delta: CaSpkP - SpkPrv\ntimes normalized GeIntNorm for recv unit credit assignment.\nThe Learn.Trace.Tau time constant determines trace updating over trials\nwhen ACh is above threshold -- this determines strength of second-order\nconditioning -- default of 1 means none, but can be increased as needed.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"pvlv_prjns"}}}, Fields: []gti.Field{{Name: "NegDeltaLRate", Doc: "use 0.01 for acquisition (don't unlearn) and 1 for extinction -- negative delta learning rate multiplier"}, {Name: "AChThr", Doc: "threshold on this layer's ACh level for trace learning updates"}, {Name: "USTrace", Doc: "proportion of US time stimulus activity to use for the trace component of"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.RandFunIdx", IDName: "rand-fun-idx", Directives: []gti.Directive{{Tool: "gosl", Directive: "hlsl", Args: []string{"axonrand"}}, {Tool: "gosl", Directive: "end", Args: []string{"axonrand"}}, {Tool: "gosl", Directive: "start", Args: []string{"axonrand"}}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.RWPredParams", IDName: "rw-pred-params", Doc: "RWPredParams parameterizes reward prediction for a simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the PVLV framework).", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rl_layers"}}}, Fields: []gti.Field{{Name: "PredRange", Doc: "default 0.1..0.99 range of predictions that can be represented -- having a truncated range preserves some sensitivity in dopamine at the extremes of good or poor performance"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.RWDaParams", IDName: "rw-da-params", Doc: "RWDaParams computes a dopamine (DA) signal using simple Rescorla-Wagner\nlearning dynamic (i.e., PV learning in the PVLV framework).", Fields: []gti.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "RWPredLayIdx", Doc: "idx of RWPredLayer to get reward prediction from -- set during Build from BuildConfig RWPredLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.TDIntegParams", IDName: "td-integ-params", Doc: "TDIntegParams are params for reward integrator layer", Fields: []gti.Field{{Name: "Discount", Doc: "discount factor -- how much to discount the future prediction from TDPred"}, {Name: "PredGain", Doc: "gain factor on TD rew pred activations"}, {Name: "TDPredLayIdx", Doc: "idx of TDPredLayer to get reward prediction from -- set during Build from BuildConfig TDPredLayName"}, {Name: "pad"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.TDDaParams", IDName: "td-da-params", Doc: "TDDaParams are params for dopamine (DA) signal as the temporal difference (TD)\nbetween the TDIntegLayer activations in the minus and plus phase.", Fields: []gti.Field{{Name: "TonicGe", Doc: "tonic baseline Ge level for DA = 0 -- +/- are between 0 and 2*TonicGe -- just for spiking display of computed DA value"}, {Name: "TDIntegLayIdx", Doc: "idx of TDIntegLayer to get reward prediction from -- set during Build from BuildConfig TDIntegLayName"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.RLPredPrjnParams", IDName: "rl-pred-prjn-params", Doc: "RLPredPrjnParams does dopamine-modulated learning for reward prediction: Da * Send.Act\nUsed by RWPrjn and TDPredPrjn within corresponding RWPredLayer or TDPredLayer\nto generate reward predictions based on its incoming weights, using linear activation\nfunction. Has no weight bounds or limits on sign etc.", Directives: []gti.Directive{{Tool: "gosl", Directive: "start", Args: []string{"rl_prjns"}}}, Fields: []gti.Field{{Name: "OppSignLRate", Doc: "how much to learn on opposite DA sign coding neuron (0..1)"}, {Name: "DaTol", Doc: "tolerance on DA -- if below this abs value, then DA goes to zero and there is no learning -- prevents prediction from exactly learning to cancel out reward value, retaining a residual valence of signal"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseVars", IDName: "synapse-vars", Doc: "SynapseVars are the neuron variables representing current synaptic state,\nspecifically weights."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseCaVars", IDName: "synapse-ca-vars", Doc: "SynapseCaVars are synapse variables for calcium involved in learning,\nwhich are data parallel input specific."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseIdxs", IDName: "synapse-idxs", Doc: "SynapseIdxs are the neuron indexes and other uint32 values (flags, etc).\nThere is only one of these per neuron -- not data parallel."})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseVarStrides", IDName: "synapse-var-strides", Doc: "SynapseVarStrides encodes the stride offsets for synapse variable access\ninto network float32 array.", Directives: []gti.Directive{{Tool: "gosl", Directive: "end", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "hlsl", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "end", Args: []string{"synapse"}}, {Tool: "gosl", Directive: "start", Args: []string{"synapse"}}}, Fields: []gti.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Var", Doc: "variable level"}, {Name: "pad"}, {Name: "pad1"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseCaStrides", IDName: "synapse-ca-strides", Doc: "SynapseCaStrides encodes the stride offsets for synapse variable access\ninto network float32 array.  Data is always the inner-most variable.", Fields: []gti.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Var", Doc: "variable level"}}})

var _ = gti.AddType(&gti.Type{Name: "github.com/emer/axon/v2/axon.SynapseIdxStrides", IDName: "synapse-idx-strides", Doc: "SynapseIdxStrides encodes the stride offsets for synapse index access\ninto network uint32 array.", Fields: []gti.Field{{Name: "Synapse", Doc: "synapse level"}, {Name: "Index", Doc: "index value level"}, {Name: "pad"}, {Name: "pad1"}}})
