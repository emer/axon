// Code generated by "core generate -add-types -gosl"; DO NOT EDIT.

package axon

import (
	"cogentcore.org/core/enums"
)

var _PathGTypesValues = []PathGTypes{0, 1, 2, 3, 4}

// PathGTypesN is the highest valid value for type PathGTypes, plus one.
//
//gosl:start
const PathGTypesN PathGTypes = 5

//gosl:end

var _PathGTypesValueMap = map[string]PathGTypes{`ExcitatoryG`: 0, `InhibitoryG`: 1, `ModulatoryG`: 2, `MaintG`: 3, `ContextG`: 4}

var _PathGTypesDescMap = map[PathGTypes]string{0: `Excitatory pathways drive Ge conductance on receiving neurons, which send to GiRaw and GiSyn neuron variables.`, 1: `Inhibitory pathways drive Gi inhibitory conductance, which send to GiRaw and GiSyn neuron variables.`, 2: `Modulatory pathways have a multiplicative effect on other inputs, which send to GModRaw and GModSyn neuron variables.`, 3: `Maintenance pathways drive unique set of NMDA channels that support strong active maintenance abilities. Send to GMaintRaw and GMaintSyn neuron variables.`, 4: `Context pathways are for inputs to CT layers, which update only at the end of the plus phase, and send to CtxtGe.`}

var _PathGTypesMap = map[PathGTypes]string{0: `ExcitatoryG`, 1: `InhibitoryG`, 2: `ModulatoryG`, 3: `MaintG`, 4: `ContextG`}

// String returns the string representation of this PathGTypes value.
func (i PathGTypes) String() string { return enums.String(i, _PathGTypesMap) }

// SetString sets the PathGTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *PathGTypes) SetString(s string) error {
	return enums.SetString(i, s, _PathGTypesValueMap, "PathGTypes")
}

// Int64 returns the PathGTypes value as an int64.
func (i PathGTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the PathGTypes value from an int64.
func (i *PathGTypes) SetInt64(in int64) { *i = PathGTypes(in) }

// Desc returns the description of the PathGTypes value.
func (i PathGTypes) Desc() string { return enums.Desc(i, _PathGTypesDescMap) }

// PathGTypesValues returns all possible values for the type PathGTypes.
func PathGTypesValues() []PathGTypes { return _PathGTypesValues }

// Values returns all possible values for the type PathGTypes.
func (i PathGTypes) Values() []enums.Enum { return enums.Values(_PathGTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i PathGTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *PathGTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "PathGTypes")
}

var _GlobalScalarVarsValues = []GlobalScalarVars{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57}

// GlobalScalarVarsN is the highest valid value for type GlobalScalarVars, plus one.
//
//gosl:start
const GlobalScalarVarsN GlobalScalarVars = 58

//gosl:end

var _GlobalScalarVarsValueMap = map[string]GlobalScalarVars{`GvRew`: 0, `GvHasRew`: 1, `GvRewPred`: 2, `GvPrevPred`: 3, `GvHadRew`: 4, `GvDA`: 5, `GvDAtonic`: 6, `GvACh`: 7, `GvNE`: 8, `GvSer`: 9, `GvAChRaw`: 10, `GvGoalMaint`: 11, `GvVSMatrixJustGated`: 12, `GvVSMatrixHasGated`: 13, `GvCuriosityPoolGated`: 14, `GvTime`: 15, `GvEffort`: 16, `GvUrgencyRaw`: 17, `GvUrgency`: 18, `GvHasPosUS`: 19, `GvHadPosUS`: 20, `GvNegUSOutcome`: 21, `GvHadNegUSOutcome`: 22, `GvPVposSum`: 23, `GvPVpos`: 24, `GvPVnegSum`: 25, `GvPVneg`: 26, `GvPVposEst`: 27, `GvPVposVar`: 28, `GvPVnegEst`: 29, `GvPVnegVar`: 30, `GvGoalDistEst`: 31, `GvGoalDistPrev`: 32, `GvProgressRate`: 33, `GvGiveUpUtility`: 34, `GvContUtility`: 35, `GvGiveUpTiming`: 36, `GvContTiming`: 37, `GvGiveUpProgress`: 38, `GvContProgress`: 39, `GvGiveUpSum`: 40, `GvContSum`: 41, `GvGiveUpProb`: 42, `GvGiveUp`: 43, `GvGaveUp`: 44, `GvVSPatchPos`: 45, `GvVSPatchPosThr`: 46, `GvVSPatchPosRPE`: 47, `GvVSPatchPosSum`: 48, `GvVSPatchPosPrev`: 49, `GvVSPatchPosVar`: 50, `GvLHbDip`: 51, `GvLHbBurst`: 52, `GvLHbPVDA`: 53, `GvCeMpos`: 54, `GvCeMneg`: 55, `GvVtaDA`: 56, `GvCaBinWts`: 57}

var _GlobalScalarVarsDescMap = map[GlobalScalarVars]string{0: `Rew is the external reward value. Must also set HasRew flag when Rew is set, otherwise it is ignored. This is computed by the Rubicon algorithm from US inputs set by Net.Rubicon methods, and can be directly set in simpler RL cases.`, 1: `HasRew must be set to true (1) when an external reward / US input is present, otherwise Rew is ignored. This is also set when Rubicon BOA model gives up. This drives ACh release in the Rubicon model.`, 2: `RewPred is the reward prediction, computed by a special reward prediction layer, e.g., the VSPatch layer in the Rubicon algorithm.`, 3: `PrevPred is previous time step reward prediction, e.g., for TDPredLayer`, 4: `HadRew is HasRew state from the previous trial, copied from HasRew in NewState. Used for updating Effort, Urgency at start of new trial.`, 5: `DA is phasic dopamine that drives learning moreso than performance, representing reward prediction error, signaled as phasic increases or decreases in activity relative to a tonic baseline, which is represented by a value of 0. Released by the VTA (ventral tegmental area), or SNc (substantia nigra pars compacta).`, 6: `DAtonic is tonic dopamine, which has modulatory instead of learning effects. Increases can drive greater propensity to engage in activities by biasing Go vs No pathways in the basal ganglia, for example as a function of Urgency.`, 7: `ACh is acetylcholine, activated by salient events, particularly at the onset of a reward / punishment outcome (US), or onset of a conditioned stimulus (CS). Driven by BLA -&gt; PPtg that detects changes in BLA activity, via LDTLayer type.`, 8: `NE is norepinepherine -- not yet in use`, 9: `Ser is serotonin -- not yet in use`, 10: `AChRaw is raw ACh value used in updating global ACh value by LDTLayer.`, 11: `GoalMaint is the normalized (0-1) goal maintenance activity, set in ApplyRubicon function at start of trial. Drives top-down inhibition of LDT layer / ACh activity.`, 12: `VSMatrixJustGated is VSMatrix just gated (to engage goal maintenance in PFC areas), set at end of plus phase. This excludes any gating happening at time of US.`, 13: `VSMatrixHasGated is VSMatrix has gated since the last time HasRew was set (US outcome received or expected one failed to be received).`, 14: `CuriosityPoolGated is true if VSMatrixJustGated and the first pool representing the curiosity / novelty drive gated. This can change the giving up Effort.Max parameter.`, 15: `Time is the raw time counter, incrementing upward during goal engaged window. This is also copied directly into NegUS[0] which tracks time, but we maintain a separate effort value to make it clearer.`, 16: `Effort is the raw effort counter, incrementing upward for each effort step during goal engaged window. This is also copied directly into NegUS[1] which tracks effort, but we maintain a separate effort value to make it clearer.`, 17: `UrgencyRaw is the raw effort for urgency, incrementing upward from effort increments per step when _not_ goal engaged.`, 18: `Urgency is the overall urgency activity level (normalized 0-1), computed from logistic function of GvUrgencyRaw. This drives DAtonic activity to increasingly bias Go firing.`, 19: `HasPosUS indicates has positive US on this trial, drives goal accomplishment logic and gating.`, 20: `HadPosUS is state from the previous trial (copied from HasPosUS in NewState).`, 21: `NegUSOutcome indicates that a phasic negative US stimulus was experienced, driving phasic ACh, VSMatrix gating to reset current goal engaged plan (if any), and phasic dopamine based on the outcome.`, 22: `HadNegUSOutcome is state from the previous trial (copied from NegUSOutcome in NewState)`, 23: `PVposSum is the total weighted positive valence primary value = sum of Weight * USpos * Drive`, 24: `PVpos is the normalized positive valence primary value = (1 - 1/(1+PVposGain * PVposSum))`, 25: `PVnegSum is the total weighted negative valence primary values including costs = sum of Weight * Cost + Weight * USneg`, 26: `PVpos is the normalized negative valence primary values, including costs = (1 - 1/(1+PVnegGain * PVnegSum))`, 27: `PVposEst is the estimated PVpos final outcome value decoded from the network PVposFinal layer`, 28: `PVposVar is the estimated variance or uncertainty in the PVpos final outcome value decoded from the network PVposFinal layer.`, 29: `PVnegEst is the estimated PVneg final outcome value decoded from the network PVnegFinal layer.`, 30: `PVnegVar is the estimated variance or uncertainty in the PVneg final outcome value decoded from the network PVnegFinal layer.`, 31: `GoalDistEst is the estimate of distance to the goal, in trial step units, decreasing down to 0 as the goal approaches.`, 32: `GoalDistPrev is the previous estimate of distance to the goal, in trial step units, decreasing down to 0 as the goal approaches.`, 33: `ProgressRate is the negative time average change in GoalDistEst, i.e., positive values indicate continued approach to the goal, while negative values represent moving away from the goal.`, 34: `GiveUpUtility is total GiveUp weight as a function of Cost.`, 35: `ContUtility is total Continue weight as a function of expected positive outcome PVposEst.`, 36: `GiveUpTiming is total GiveUp weight as a function of VSPatchPosSum * (1 - VSPatchPosVar).`, 37: `ContTiming is total Continue weight as a function of (1 - VSPatchPosSum) * VSPatchPosVar.`, 38: `GiveUpProgress is total GiveUp weight as a function of ProgressRate.`, 39: `ContProgress is total Continue weight as a function of ProgressRate.`, 40: `GiveUpSum is total GiveUp weight: Utility + Timing + Progress.`, 41: `ContSum is total Continue weight: Utility + Timing + Progress.`, 42: `GiveUpProb is the probability of giving up: 1 / (1 + (GvContSum / GvGiveUpSum))`, 43: `GiveUp is true if a reset was triggered probabilistically based on GiveUpProb.`, 44: `GaveUp is copy of GiveUp from previous trial.`, 45: `VSPatchPos is the net shunting input from VSPatch (PosD1, named PVi in original Rubicon) computed as the Max of US-specific VSPatch saved values, subtracting D1 - D2. This is also stored as GvRewPred.`, 46: `VSPatchPosThr is a thresholded version of GvVSPatchPos, applying Rubicon.LHb.VSPatchNonRewThr threshold for non-reward trials. This is the version used for computing DA.`, 47: `VSPatchPosRPE is the reward prediction error for the VSPatchPos reward prediction without any thresholding applied, and only for PV events. This is used to train the VSPatch, assuming a local feedback circuit that does not have the effective thresholding used for the broadcast critic signal that trains the rest of the network.`, 48: `VSPatchPosSum is the sum of VSPatchPos over goal engaged trials, representing the integrated prediction that the US is going to occur`, 49: `VSPatchPosPrev is the previous trial VSPatchPosSum`, 50: `VSPatchPosVar is the integrated temporal variance of VSPatchPos over goal engaged trials, which determines when the VSPatchPosSum has stabilized`, 51: `computed LHb activity level that drives dipping / pausing of DA firing, when VSPatch pos prediction &gt; actual PV reward drive or PVneg &gt; PVpos`, 52: `LHbBurst is computed LHb activity level that drives bursts of DA firing, when actual PV reward drive &gt; VSPatch pos prediction`, 53: `LHbPVDA is GvLHbBurst - GvLHbDip -- the LHb contribution to DA, reflecting PV and VSPatch (PVi), but not the CS (LV) contributions`, 54: `CeMpos is positive valence central nucleus of the amygdala (CeM) LV (learned value) activity, reflecting |BLAposAcqD1 - BLAposExtD2|_+ positively rectified. CeM sets Raw directly. Note that a positive US onset even with no active Drive will be reflected here, enabling learning about unexpected outcomes.`, 55: `CeMneg is negative valence central nucleus of the amygdala (CeM) LV (learned value) activity, reflecting |BLAnegAcqD2 - BLAnegExtD1|_+ positively rectified. CeM sets Raw directly`, 56: `VtaDA is overall dopamine value reflecting all of the different inputs.`, 57: `CaBinWts are NCaBins starting here, of weights for integrating binned spikes to compute synaptic calcium values that drive the trace factor in learning. These are only stored for the first parallel data index di = 0.`}

var _GlobalScalarVarsMap = map[GlobalScalarVars]string{0: `GvRew`, 1: `GvHasRew`, 2: `GvRewPred`, 3: `GvPrevPred`, 4: `GvHadRew`, 5: `GvDA`, 6: `GvDAtonic`, 7: `GvACh`, 8: `GvNE`, 9: `GvSer`, 10: `GvAChRaw`, 11: `GvGoalMaint`, 12: `GvVSMatrixJustGated`, 13: `GvVSMatrixHasGated`, 14: `GvCuriosityPoolGated`, 15: `GvTime`, 16: `GvEffort`, 17: `GvUrgencyRaw`, 18: `GvUrgency`, 19: `GvHasPosUS`, 20: `GvHadPosUS`, 21: `GvNegUSOutcome`, 22: `GvHadNegUSOutcome`, 23: `GvPVposSum`, 24: `GvPVpos`, 25: `GvPVnegSum`, 26: `GvPVneg`, 27: `GvPVposEst`, 28: `GvPVposVar`, 29: `GvPVnegEst`, 30: `GvPVnegVar`, 31: `GvGoalDistEst`, 32: `GvGoalDistPrev`, 33: `GvProgressRate`, 34: `GvGiveUpUtility`, 35: `GvContUtility`, 36: `GvGiveUpTiming`, 37: `GvContTiming`, 38: `GvGiveUpProgress`, 39: `GvContProgress`, 40: `GvGiveUpSum`, 41: `GvContSum`, 42: `GvGiveUpProb`, 43: `GvGiveUp`, 44: `GvGaveUp`, 45: `GvVSPatchPos`, 46: `GvVSPatchPosThr`, 47: `GvVSPatchPosRPE`, 48: `GvVSPatchPosSum`, 49: `GvVSPatchPosPrev`, 50: `GvVSPatchPosVar`, 51: `GvLHbDip`, 52: `GvLHbBurst`, 53: `GvLHbPVDA`, 54: `GvCeMpos`, 55: `GvCeMneg`, 56: `GvVtaDA`, 57: `GvCaBinWts`}

// String returns the string representation of this GlobalScalarVars value.
func (i GlobalScalarVars) String() string { return enums.String(i, _GlobalScalarVarsMap) }

// SetString sets the GlobalScalarVars value from its string representation,
// and returns an error if the string is invalid.
func (i *GlobalScalarVars) SetString(s string) error {
	return enums.SetString(i, s, _GlobalScalarVarsValueMap, "GlobalScalarVars")
}

// Int64 returns the GlobalScalarVars value as an int64.
func (i GlobalScalarVars) Int64() int64 { return int64(i) }

// SetInt64 sets the GlobalScalarVars value from an int64.
func (i *GlobalScalarVars) SetInt64(in int64) { *i = GlobalScalarVars(in) }

// Desc returns the description of the GlobalScalarVars value.
func (i GlobalScalarVars) Desc() string { return enums.Desc(i, _GlobalScalarVarsDescMap) }

// GlobalScalarVarsValues returns all possible values for the type GlobalScalarVars.
func GlobalScalarVarsValues() []GlobalScalarVars { return _GlobalScalarVarsValues }

// Values returns all possible values for the type GlobalScalarVars.
func (i GlobalScalarVars) Values() []enums.Enum { return enums.Values(_GlobalScalarVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i GlobalScalarVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *GlobalScalarVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "GlobalScalarVars")
}

var _GlobalVectorVarsValues = []GlobalVectorVars{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}

// GlobalVectorVarsN is the highest valid value for type GlobalVectorVars, plus one.
//
//gosl:start
const GlobalVectorVarsN GlobalVectorVars = 10

//gosl:end

var _GlobalVectorVarsValueMap = map[string]GlobalVectorVars{`GvCost`: 0, `GvCostRaw`: 1, `GvUSneg`: 2, `GvUSnegRaw`: 3, `GvDrives`: 4, `GvUSpos`: 5, `GvVSPatchD1`: 6, `GvVSPatchD2`: 7, `GvOFCposPTMaint`: 8, `GvVSMatrixPoolGated`: 9}

var _GlobalVectorVarsDescMap = map[GlobalVectorVars]string{0: `Cost are Time, Effort, etc costs, as normalized version of corresponding raw. NCosts of them`, 1: `CostRaw are raw, linearly incremented negative valence US outcomes, this value is also integrated together with all US vals for PVneg`, 2: `USneg are negative valence US outcomes, normalized version of raw. NNegUSs of them`, 3: `USnegRaw are raw, linearly incremented negative valence US outcomes, this value is also integrated together with all US vals for PVneg`, 4: `Drives are current drive state, updated with optional homeostatic exponential return to baseline values.`, 5: `USpos are current positive-valence drive-satisfying input(s) (unconditioned stimuli = US)`, 6: `VSPatch is current reward predicting VSPatch (PosD1) values.`, 7: `VSPatch is current reward predicting VSPatch (PosD2) values.`, 8: `OFCposPTMaint is activity level of given OFCposPT maintenance pool used in anticipating potential USpos outcome value.`, 9: `VSMatrixPoolGated indicates whether given VSMatrix pool gated this is reset after last goal accomplished -- records gating since then.`}

var _GlobalVectorVarsMap = map[GlobalVectorVars]string{0: `GvCost`, 1: `GvCostRaw`, 2: `GvUSneg`, 3: `GvUSnegRaw`, 4: `GvDrives`, 5: `GvUSpos`, 6: `GvVSPatchD1`, 7: `GvVSPatchD2`, 8: `GvOFCposPTMaint`, 9: `GvVSMatrixPoolGated`}

// String returns the string representation of this GlobalVectorVars value.
func (i GlobalVectorVars) String() string { return enums.String(i, _GlobalVectorVarsMap) }

// SetString sets the GlobalVectorVars value from its string representation,
// and returns an error if the string is invalid.
func (i *GlobalVectorVars) SetString(s string) error {
	return enums.SetString(i, s, _GlobalVectorVarsValueMap, "GlobalVectorVars")
}

// Int64 returns the GlobalVectorVars value as an int64.
func (i GlobalVectorVars) Int64() int64 { return int64(i) }

// SetInt64 sets the GlobalVectorVars value from an int64.
func (i *GlobalVectorVars) SetInt64(in int64) { *i = GlobalVectorVars(in) }

// Desc returns the description of the GlobalVectorVars value.
func (i GlobalVectorVars) Desc() string { return enums.Desc(i, _GlobalVectorVarsDescMap) }

// GlobalVectorVarsValues returns all possible values for the type GlobalVectorVars.
func GlobalVectorVarsValues() []GlobalVectorVars { return _GlobalVectorVarsValues }

// Values returns all possible values for the type GlobalVectorVars.
func (i GlobalVectorVars) Values() []enums.Enum { return enums.Values(_GlobalVectorVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i GlobalVectorVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *GlobalVectorVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "GlobalVectorVars")
}

var _GPUVarsValues = []GPUVars{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}

// GPUVarsN is the highest valid value for type GPUVars, plus one.
//
//gosl:start
const GPUVarsN GPUVars = 23

//gosl:end

var _GPUVarsValueMap = map[string]GPUVars{`LayersVar`: 0, `PathsVar`: 1, `NetworkIxsVar`: 2, `PoolIxsVar`: 3, `NeuronIxsVar`: 4, `SynapseIxsVar`: 5, `PathSendConVar`: 6, `RecvPathIxsVar`: 7, `PathRecvConVar`: 8, `RecvSynIxsVar`: 9, `CtxVar`: 10, `NeuronsVar`: 11, `NeuronAvgsVar`: 12, `LayerStatesVar`: 13, `GlobalScalarsVar`: 14, `GlobalVectorsVar`: 15, `ExtsVar`: 16, `PoolsVar`: 17, `PoolsIntVar`: 18, `PathGBufVar`: 19, `PathGSynsVar`: 20, `SynapsesVar`: 21, `SynapseTracesVar`: 22}

var _GPUVarsDescMap = map[GPUVars]string{0: ``, 1: ``, 2: ``, 3: ``, 4: ``, 5: ``, 6: ``, 7: ``, 8: ``, 9: ``, 10: ``, 11: ``, 12: ``, 13: ``, 14: ``, 15: ``, 16: ``, 17: ``, 18: ``, 19: ``, 20: ``, 21: ``, 22: ``}

var _GPUVarsMap = map[GPUVars]string{0: `LayersVar`, 1: `PathsVar`, 2: `NetworkIxsVar`, 3: `PoolIxsVar`, 4: `NeuronIxsVar`, 5: `SynapseIxsVar`, 6: `PathSendConVar`, 7: `RecvPathIxsVar`, 8: `PathRecvConVar`, 9: `RecvSynIxsVar`, 10: `CtxVar`, 11: `NeuronsVar`, 12: `NeuronAvgsVar`, 13: `LayerStatesVar`, 14: `GlobalScalarsVar`, 15: `GlobalVectorsVar`, 16: `ExtsVar`, 17: `PoolsVar`, 18: `PoolsIntVar`, 19: `PathGBufVar`, 20: `PathGSynsVar`, 21: `SynapsesVar`, 22: `SynapseTracesVar`}

// String returns the string representation of this GPUVars value.
func (i GPUVars) String() string { return enums.String(i, _GPUVarsMap) }

// SetString sets the GPUVars value from its string representation,
// and returns an error if the string is invalid.
func (i *GPUVars) SetString(s string) error {
	return enums.SetString(i, s, _GPUVarsValueMap, "GPUVars")
}

// Int64 returns the GPUVars value as an int64.
func (i GPUVars) Int64() int64 { return int64(i) }

// SetInt64 sets the GPUVars value from an int64.
func (i *GPUVars) SetInt64(in int64) { *i = GPUVars(in) }

// Desc returns the description of the GPUVars value.
func (i GPUVars) Desc() string { return enums.Desc(i, _GPUVarsDescMap) }

// GPUVarsValues returns all possible values for the type GPUVars.
func GPUVarsValues() []GPUVars { return _GPUVarsValues }

// Values returns all possible values for the type GPUVars.
func (i GPUVars) Values() []enums.Enum { return enums.Values(_GPUVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i GPUVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *GPUVars) UnmarshalText(text []byte) error { return enums.UnmarshalText(i, text, "GPUVars") }

var _LayerTypesValues = []LayerTypes{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35}

// LayerTypesN is the highest valid value for type LayerTypes, plus one.
//
//gosl:start
const LayerTypesN LayerTypes = 36

//gosl:end

var _LayerTypesValueMap = map[string]LayerTypes{`SuperLayer`: 0, `InputLayer`: 1, `TargetLayer`: 2, `CompareLayer`: 3, `CTLayer`: 4, `PulvinarLayer`: 5, `TRNLayer`: 6, `PTMaintLayer`: 7, `PTPredLayer`: 8, `DSMatrixLayer`: 9, `VSMatrixLayer`: 10, `DSPatchLayer`: 11, `STNLayer`: 12, `GPLayer`: 13, `BGThalLayer`: 14, `VSGatedLayer`: 15, `IOLayer`: 16, `CNeLayer`: 17, `CNiIOLayer`: 18, `CNiUpLayer`: 19, `BLALayer`: 20, `CeMLayer`: 21, `VSPatchLayer`: 22, `LHbLayer`: 23, `DrivesLayer`: 24, `UrgencyLayer`: 25, `USLayer`: 26, `PVLayer`: 27, `LDTLayer`: 28, `VTALayer`: 29, `RewLayer`: 30, `RWPredLayer`: 31, `RWDaLayer`: 32, `TDPredLayer`: 33, `TDIntegLayer`: 34, `TDDaLayer`: 35}

var _LayerTypesDescMap = map[LayerTypes]string{0: `Super is a superficial cortical layer (lamina 2-3-4) which does not receive direct input or targets. In more generic models, it should be used as a Hidden layer, and maps onto the Hidden type in LayerTypes.`, 1: `Input is a layer that receives direct external input in its Ext inputs. Biologically, it can be a primary sensory layer, or a thalamic layer.`, 2: `Target is a layer that receives direct external target inputs used for driving plus-phase learning. Simple target layers are generally not used in more biological models, which instead use predictive learning via Pulvinar or related mechanisms.`, 3: `Compare is a layer that receives external comparison inputs, which drive statistics but do NOT drive activation or learning directly. It is rarely used in axon.`, 4: `CT are layer 6 corticothalamic projecting neurons, which drive &#34;top down&#34; predictions in Pulvinar layers. They maintain information over time via stronger NMDA channels and use maintained prior state information to generate predictions about current states forming on Super layers that then drive PT (5IB) bursting activity, which are the plus-phase drivers of Pulvinar activity.`, 5: `Pulvinar are thalamic relay cell neurons in the higher-order Pulvinar nucleus of the thalamus, and functionally isomorphic neurons in the MD thalamus, and potentially other areas. These cells alternately reflect predictions driven by CT pathways, and actual outcomes driven by 5IB Burst activity from corresponding PT or Super layer neurons that provide strong driving inputs.`, 6: `TRNLayer is thalamic reticular nucleus layer for inhibitory competition within the thalamus.`, 7: `PTMaintLayer implements the subset of pyramidal tract (PT) layer 5 intrinsic bursting (5IB) deep neurons that exhibit robust, stable maintenance of activity over the duration of a goal engaged window, modulated by basal ganglia (BG) disinhibitory gating, supported by strong MaintNMDA channels and recurrent excitation. The lateral PTSelfMaint pathway uses MaintG to drive GMaintRaw input that feeds into the stronger, longer MaintNMDA channels, and the ThalToPT ModulatoryG pathway from BGThalamus multiplicatively modulates the strength of other inputs, such that only at the time of BG gating are these strong enough to drive sustained active maintenance. Use Act.Dend.ModGain to parameterize.`, 8: `PTPredLayer implements the subset of pyramidal tract (PT) layer 5 intrinsic bursting (5IB) deep neurons that combine modulatory input from PTMaintLayer sustained maintenance and CTLayer dynamic predictive learning that helps to predict state changes during the period of active goal maintenance. This layer provides the primary input to VSPatch US-timing prediction layers, and other layers that require predictive dynamic`, 9: `DSMatrixLayer represents the matrisome spiny projection neurons (SPNs, MSNs) that are the main Go / No gating units in BG, and are modulated by phasic dopamine: D1 = Go, D2 = No. These are for dorsal striatum, which interact with matrisomes and receive PF (parafasciculus) feedback signals.`, 10: `VSMatrixLayer represents the matrisome spiny projection neurons (SPNs, MSNs) that are the main Go / No gating units in BG, and are modulated by phasic dopamine: D1 = Go, D2 = No. These are for ventral striatum, which drive goal-selection gating signals through the MD thalamus, and activate instinctive behaviors based on learned inputs projecting to various output pathways.`, 11: `DSPatchLayer represents the dorsolateral striosomal spiny neurons that modulate the activity of SNc dopamine to a given Pool.`, 12: `STNLayer represents subthalamic nucleus neurons, with two subtypes: STNp are more strongly driven and get over bursting threshold, driving strong, rapid activation of the KCa channels, causing a long pause in firing, which creates a window during which GPe dynamics resolve Go vs. No balance. STNs are more weakly driven and thus more slowly activate KCa, resulting in a longer period of activation, during which the GPi is inhibited to prevent premature gating based only MtxGo inhibition -- gating only occurs when GPePr signal has had a chance to integrate its MtxNo inputs.`, 13: `GPLayer represents a globus pallidus layer in the BG, including: GPePr, GPeAk (arkypallidal), and GPi / SNr. Has intrinsic activity.`, 14: `BGThalLayer represents a BG gated thalamic layer, which receives BG gating in the form of an inhibitory pathway from GPi. Located mainly in the Ventral thalamus: VA / VM / VL, and also parts of MD mediodorsal thalamus.`, 15: `VSGated represents explicit coding of VS gating status: JustGated and HasGated (since last US or failed predicted US), For visualization and / or motor action signaling.`, 16: `IOLayer represents a cerebellum inferior olive (IO) layer, which drive learning in associated cerebellar nuclei and Purkinje cells. Receives paired input from the CNiIOLayer inhibitory prediction neurons and specific sensory channels that are being predicted, and a modulatory input from the efferent copy of motor action to initiate it. GaP = integrated GeSyn, GaM = integrated GiSyn, GaD = offset GiSyn, TimeDiff = GaP - GaD, TimePeak = 1 if error spike.`, 17: `CNeLayer represents the cerebellar nuclei excitatory neurons, which have slow learning to maintain a target average firing rate.`, 18: `CNiIOLayer represents the cerebellar nuclei inhibitory prediction neurons, which learn to predict the activity of a specific sensory input, and inhibit it in the corresponding CNeUpLayer`, 19: `CNiUpLayer represents the cerebellar nuclei inhibitory upgoing output neurons, which learn from IOLayer error signals to predict specific sensory inputs based on motor commands, thereby cancelling the effects of self-generated motor commands.`, 20: `BLALayer represents a basolateral amygdala layer which learns to associate arbitrary stimuli (CSs) with behaviorally salient outcomes (USs)`, 21: `CeMLayer represents a central nucleus of the amygdala layer.`, 22: `VSPatchLayer represents a ventral striatum patch layer, which learns to represent the expected amount of dopamine reward and projects both directly with shunting inhibition to the VTA and indirectly via the LHb / RMTg to cancel phasic dopamine firing to expected rewards (i.e., reward prediction error).`, 23: `LHbLayer represents the lateral habenula, which drives dipping in the VTA. It tracks the Global LHb values for visualization purposes -- updated by VTALayer.`, 24: `DrivesLayer represents the Drives in .Rubicon framework. It tracks the Global Drives values for visualization and predictive learning purposes.`, 25: `UrgencyLayer represents the Urgency factor in Rubicon framework. It tracks the Global Urgency.Urge value for visualization and predictive learning purposes.`, 26: `USLayer represents a US unconditioned stimulus layer (USpos or USneg). It tracks the Global USpos or USneg, for visualization and predictive learning purposes. Actual US inputs are set in Rubicon.`, 27: `PVLayer represents a PV primary value layer (PVpos or PVneg) representing the total primary value as a function of US inputs, drives, and effort. It tracks the Global VTA.PVpos, PVneg values for visualization and predictive learning purposes.`, 28: `LDTLayer represents the laterodorsal tegmentum layer, which is the primary limbic ACh (acetylcholine) driver to other ACh: BG cholinergic interneurons (CIN) and nucleus basalis ACh areas. The phasic ACh release signals reward salient inputs from CS, US and US omssion, and it drives widespread disinhibition of BG gating and VTA DA firing. It receives excitation from superior colliculus which computes a temporal derivative (stimulus specific adaptation, SSA) of sensory inputs, and inhibitory input from OFC, ACC driving suppression of distracting inputs during goal-engaged states.`, 29: `VTALayer represents the ventral tegmental area, which releases dopamine. It computes final DA value from Rubicon-computed LHb PVDA (primary value DA), updated at start of each trial from updated US, Effort, etc state, and cycle-by-cycle LV learned value state reflecting CS inputs, in the Amygdala (CeM). Its activity reflects this DA level, which is effectively broadcast vial Global state values to all layers.`, 30: `RewLayer represents positive (first unit) or negative (second unit) reward values, showing spiking rates for each, and Act always represents the signed value.`, 31: `RWPredLayer computes reward prediction for a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework). Activity is computed as linear function of excitatory conductance. The first unit in the layer represents positive reward, second negative. Use with RWPath which does simple delta-rule learning on minus-plus.`, 32: `RWDaLayer computes a dopamine (DA) signal based on a simple Rescorla-Wagner learning dynamic (i.e., PV learning in the Rubicon framework). It computes difference between r(t) and RWPred values. r(t) is accessed directly from a Rew layer -- if no external input then no DA is computed -- critical for effective use of RW only for PV cases. RWPred prediction is also accessed directly from Rew layer to avoid any issues.`, 33: `TDPredLayer is the temporal differences reward prediction layer. It represents estimated value V(t) in the minus phase, and computes estimated V(t+1) based on its learned weights in plus phase, using the TDPredPath pathway type for DA modulated learning. The first unit in the layer represents positive reward, second negative.`, 34: `TDIntegLayer is the temporal differences reward integration layer. It represents estimated value V(t) from prior time step in the minus phase, and estimated discount * V(t+1) + r(t) in the plus phase. It gets Rew, PrevPred from Context.NeuroMod, and Special LayerValues from TDPredLayer. The first unit in the layer represents positive reward, second negative.`, 35: `TDDaLayer computes a dopamine (DA) signal as the temporal difference (TD) between the TDIntegLayer activations in the minus and plus phase. These are retrieved from Special LayerValues.`}

var _LayerTypesMap = map[LayerTypes]string{0: `SuperLayer`, 1: `InputLayer`, 2: `TargetLayer`, 3: `CompareLayer`, 4: `CTLayer`, 5: `PulvinarLayer`, 6: `TRNLayer`, 7: `PTMaintLayer`, 8: `PTPredLayer`, 9: `DSMatrixLayer`, 10: `VSMatrixLayer`, 11: `DSPatchLayer`, 12: `STNLayer`, 13: `GPLayer`, 14: `BGThalLayer`, 15: `VSGatedLayer`, 16: `IOLayer`, 17: `CNeLayer`, 18: `CNiIOLayer`, 19: `CNiUpLayer`, 20: `BLALayer`, 21: `CeMLayer`, 22: `VSPatchLayer`, 23: `LHbLayer`, 24: `DrivesLayer`, 25: `UrgencyLayer`, 26: `USLayer`, 27: `PVLayer`, 28: `LDTLayer`, 29: `VTALayer`, 30: `RewLayer`, 31: `RWPredLayer`, 32: `RWDaLayer`, 33: `TDPredLayer`, 34: `TDIntegLayer`, 35: `TDDaLayer`}

// String returns the string representation of this LayerTypes value.
func (i LayerTypes) String() string { return enums.String(i, _LayerTypesMap) }

// SetString sets the LayerTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *LayerTypes) SetString(s string) error {
	return enums.SetString(i, s, _LayerTypesValueMap, "LayerTypes")
}

// Int64 returns the LayerTypes value as an int64.
func (i LayerTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the LayerTypes value from an int64.
func (i *LayerTypes) SetInt64(in int64) { *i = LayerTypes(in) }

// Desc returns the description of the LayerTypes value.
func (i LayerTypes) Desc() string { return enums.Desc(i, _LayerTypesDescMap) }

// LayerTypesValues returns all possible values for the type LayerTypes.
func LayerTypesValues() []LayerTypes { return _LayerTypesValues }

// Values returns all possible values for the type LayerTypes.
func (i LayerTypes) Values() []enums.Enum { return enums.Values(_LayerTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i LayerTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *LayerTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "LayerTypes")
}

var _LayerVarsValues = []LayerVars{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}

// LayerVarsN is the highest valid value for type LayerVars, plus one.
//
//gosl:start
const LayerVarsN LayerVars = 12

//gosl:end

var _LayerVarsValueMap = map[string]LayerVars{`LayerActMAvg`: 0, `LayerActPAvg`: 1, `LayerAvgMaxGeM`: 2, `LayerAvgMaxGiM`: 3, `LayerGiMult`: 4, `LayerPhaseDiff`: 5, `LayerPhaseDiffAvg`: 6, `LayerPhaseDiffVar`: 7, `LayerRT`: 8, `GatedRT`: 9, `LayerRewPredPos`: 10, `LayerRewPredNeg`: 11}

var _LayerVarsDescMap = map[LayerVars]string{0: `LayerActMAvg is the running-average minus-phase activity integrated at Dt.LongAvgTau, used for adapting inhibition relative to target level.`, 1: `LayerActPAvg is the running-average plus-phase activity integrated at Dt.LongAvgTau.`, 2: `LayerAvgMaxGeM is the running-average max of minus-phase Ge value across the layer integrated at Dt.LongAvgTau.`, 3: `LayerAvgMaxGiM is the running-average max of minus-phase Gi value across the layer integrated at Dt.LongAvgTau.`, 4: `LayerGiMult is a multiplier on layer-level inhibition, which can be adapted to maintain target activity level.`, 5: `LayerPhaseDiff is the phase-wise difference in the activity state between the minus [ActM] and plus [ActP] phases, measured using 1 minus the correlation (centered cosine aka normalized dot product). 0 = no difference, 2 = maximum difference. Computed by PhaseDiffFromActs in the PlusPhase.`, 6: `LayerPhaseDiffAvg is the running average of [LayerPhaseDiff] over time, integrated at Dt.LongAvgTau.`, 7: `LayerPhaseDiffVar is the running variance of [LayerPhaseDiff], integrated at Dt.LongAvgTau.`, 8: `LayerRT is the reaction time for this layer in cycles, which is -1 until the Max CaP level (after MaxCycStart) exceeds the Inhib.ActAvg.RTThr threshold.`, 9: `GatedRT is the reaction time for this layer in cycles, which is -1 until the Layer-level [PoolGated] is true.`, 10: `LayerRewPredPos is the positive-valued Reward Prediction value, for RL specific layers: [RWPredLayer], [TDPredLayer]. For [TDIntegLayer], this is the plus phase current integrated reward prediction.`, 11: `LayerRewPredNeg is the negative-valued Reward Prediction value, for RL specific layers: [RWPredLayer], [TDPredLayer] For [TDIntegLayer], this is the minus phase previous integrated reward prediction.`}

var _LayerVarsMap = map[LayerVars]string{0: `LayerActMAvg`, 1: `LayerActPAvg`, 2: `LayerAvgMaxGeM`, 3: `LayerAvgMaxGiM`, 4: `LayerGiMult`, 5: `LayerPhaseDiff`, 6: `LayerPhaseDiffAvg`, 7: `LayerPhaseDiffVar`, 8: `LayerRT`, 9: `GatedRT`, 10: `LayerRewPredPos`, 11: `LayerRewPredNeg`}

// String returns the string representation of this LayerVars value.
func (i LayerVars) String() string { return enums.String(i, _LayerVarsMap) }

// SetString sets the LayerVars value from its string representation,
// and returns an error if the string is invalid.
func (i *LayerVars) SetString(s string) error {
	return enums.SetString(i, s, _LayerVarsValueMap, "LayerVars")
}

// Int64 returns the LayerVars value as an int64.
func (i LayerVars) Int64() int64 { return int64(i) }

// SetInt64 sets the LayerVars value from an int64.
func (i *LayerVars) SetInt64(in int64) { *i = LayerVars(in) }

// Desc returns the description of the LayerVars value.
func (i LayerVars) Desc() string { return enums.Desc(i, _LayerVarsDescMap) }

// LayerVarsValues returns all possible values for the type LayerVars.
func LayerVarsValues() []LayerVars { return _LayerVarsValues }

// Values returns all possible values for the type LayerVars.
func (i LayerVars) Values() []enums.Enum { return enums.Values(_LayerVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i LayerVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *LayerVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "LayerVars")
}

var _ViewTimesValues = []ViewTimes{0, 1, 2, 3, 4, 5, 6}

// ViewTimesN is the highest valid value for type ViewTimes, plus one.
//
//gosl:start
const ViewTimesN ViewTimes = 7

//gosl:end

var _ViewTimesValueMap = map[string]ViewTimes{`Cycle`: 0, `FastSpike`: 1, `Gamma`: 2, `Beta`: 3, `Alpha`: 4, `Phase`: 5, `Theta`: 6}

var _ViewTimesDescMap = map[ViewTimes]string{0: `Cycle is an update of neuron state, equivalent to 1 msec of real time.`, 1: `FastSpike is 10 cycles (msec) or 100hz. This is the fastest spiking time generally observed in the neocortex.`, 2: `Gamma is 25 cycles (msec) or 40hz. Neocortical activity often exhibits synchrony peaks in this range.`, 3: `Beta is 50 cycles (msec) or 20 hz (two Gammas). Gating in the basal ganglia and associated updating in prefrontal cortex occurs at this frequency.`, 4: `Alpha is 100 cycle (msec) or 10 hz (two Betas). Posterior neocortex exhibits synchrony peaks in this range, corresponding to the intrinsic bursting frequency of layer 5 IB neurons, and corticothalamic loop resonance.`, 5: `Phase is the Minus or Plus phase, where plus phase is bursting / outcome that drives positive learning relative to prediction in minus phase. Minus phase is at 150 cycles (msec).`, 6: `Theta is 200 cycles (msec) or 5 hz (two Alphas), i.e., a Trial. This is the modal duration of a saccade, the update frequency of medial temporal lobe episodic memory, and the minimal predictive learning cycle (perceive on Alpha 1, predict on 2).`}

var _ViewTimesMap = map[ViewTimes]string{0: `Cycle`, 1: `FastSpike`, 2: `Gamma`, 3: `Beta`, 4: `Alpha`, 5: `Phase`, 6: `Theta`}

// String returns the string representation of this ViewTimes value.
func (i ViewTimes) String() string { return enums.String(i, _ViewTimesMap) }

// SetString sets the ViewTimes value from its string representation,
// and returns an error if the string is invalid.
func (i *ViewTimes) SetString(s string) error {
	return enums.SetString(i, s, _ViewTimesValueMap, "ViewTimes")
}

// Int64 returns the ViewTimes value as an int64.
func (i ViewTimes) Int64() int64 { return int64(i) }

// SetInt64 sets the ViewTimes value from an int64.
func (i *ViewTimes) SetInt64(in int64) { *i = ViewTimes(in) }

// Desc returns the description of the ViewTimes value.
func (i ViewTimes) Desc() string { return enums.Desc(i, _ViewTimesDescMap) }

// ViewTimesValues returns all possible values for the type ViewTimes.
func ViewTimesValues() []ViewTimes { return _ViewTimesValues }

// Values returns all possible values for the type ViewTimes.
func (i ViewTimes) Values() []enums.Enum { return enums.Values(_ViewTimesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i ViewTimes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *ViewTimes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "ViewTimes")
}

var _DAModTypesValues = []DAModTypes{0, 1, 2, 3}

// DAModTypesN is the highest valid value for type DAModTypes, plus one.
//
//gosl:start
const DAModTypesN DAModTypes = 4

//gosl:end

var _DAModTypesValueMap = map[string]DAModTypes{`NoDAMod`: 0, `D1Mod`: 1, `D2Mod`: 2, `D1AbsMod`: 3}

var _DAModTypesDescMap = map[DAModTypes]string{0: `NoDAMod means there is no effect of dopamine on neural activity`, 1: `D1Mod is for neurons that primarily express dopamine D1 receptors, which are excitatory from DA bursts, inhibitory from dips. Cortical neurons can generally use this type, while subcortical populations are more diverse in having both D1 and D2 subtypes.`, 2: `D2Mod is for neurons that primarily express dopamine D2 receptors, which are excitatory from DA dips, inhibitory from bursts.`, 3: `D1AbsMod is like D1Mod, except the absolute value of DA is used instead of the signed value. There are a subset of DA neurons that send increased DA for both negative and positive outcomes, targeting frontal neurons.`}

var _DAModTypesMap = map[DAModTypes]string{0: `NoDAMod`, 1: `D1Mod`, 2: `D2Mod`, 3: `D1AbsMod`}

// String returns the string representation of this DAModTypes value.
func (i DAModTypes) String() string { return enums.String(i, _DAModTypesMap) }

// SetString sets the DAModTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *DAModTypes) SetString(s string) error {
	return enums.SetString(i, s, _DAModTypesValueMap, "DAModTypes")
}

// Int64 returns the DAModTypes value as an int64.
func (i DAModTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the DAModTypes value from an int64.
func (i *DAModTypes) SetInt64(in int64) { *i = DAModTypes(in) }

// Desc returns the description of the DAModTypes value.
func (i DAModTypes) Desc() string { return enums.Desc(i, _DAModTypesDescMap) }

// DAModTypesValues returns all possible values for the type DAModTypes.
func DAModTypesValues() []DAModTypes { return _DAModTypesValues }

// Values returns all possible values for the type DAModTypes.
func (i DAModTypes) Values() []enums.Enum { return enums.Values(_DAModTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i DAModTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *DAModTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "DAModTypes")
}

var _ValenceTypesValues = []ValenceTypes{0, 1, 2}

// ValenceTypesN is the highest valid value for type ValenceTypes, plus one.
//
//gosl:start
const ValenceTypesN ValenceTypes = 3

//gosl:end

var _ValenceTypesValueMap = map[string]ValenceTypes{`Positive`: 0, `Negative`: 1, `Cost`: 2}

var _ValenceTypesDescMap = map[ValenceTypes]string{0: `Positive valence codes for outcomes aligned with drives / goals.`, 1: `Negative valence codes for harmful or aversive outcomes.`, 2: `Cost codes for continous ongoing cost factors such as Time and Effort`}

var _ValenceTypesMap = map[ValenceTypes]string{0: `Positive`, 1: `Negative`, 2: `Cost`}

// String returns the string representation of this ValenceTypes value.
func (i ValenceTypes) String() string { return enums.String(i, _ValenceTypesMap) }

// SetString sets the ValenceTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *ValenceTypes) SetString(s string) error {
	return enums.SetString(i, s, _ValenceTypesValueMap, "ValenceTypes")
}

// Int64 returns the ValenceTypes value as an int64.
func (i ValenceTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the ValenceTypes value from an int64.
func (i *ValenceTypes) SetInt64(in int64) { *i = ValenceTypes(in) }

// Desc returns the description of the ValenceTypes value.
func (i ValenceTypes) Desc() string { return enums.Desc(i, _ValenceTypesDescMap) }

// ValenceTypesValues returns all possible values for the type ValenceTypes.
func ValenceTypesValues() []ValenceTypes { return _ValenceTypesValues }

// Values returns all possible values for the type ValenceTypes.
func (i ValenceTypes) Values() []enums.Enum { return enums.Values(_ValenceTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i ValenceTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *ValenceTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "ValenceTypes")
}

var _NeuronFlagsValues = []NeuronFlags{1, 2, 4, 8}

// NeuronFlagsN is the highest valid value for type NeuronFlags, plus one.
//
//gosl:start
const NeuronFlagsN NeuronFlags = 9

//gosl:end

var _NeuronFlagsValueMap = map[string]NeuronFlags{`NeuronOff`: 1, `NeuronHasExt`: 2, `NeuronHasTarg`: 4, `NeuronHasCmpr`: 8}

var _NeuronFlagsDescMap = map[NeuronFlags]string{1: `NeuronOff flag indicates that this neuron has been turned off (i.e., lesioned).`, 2: `NeuronHasExt means the neuron has external input in its Ext field.`, 4: `NeuronHasTarg means the neuron has external target input in its Target field.`, 8: `NeuronHasCmpr means the neuron has external comparison input in its Target field. Used for computing comparison statistics but does not drive neural activity ever.`}

var _NeuronFlagsMap = map[NeuronFlags]string{1: `NeuronOff`, 2: `NeuronHasExt`, 4: `NeuronHasTarg`, 8: `NeuronHasCmpr`}

// String returns the string representation of this NeuronFlags value.
func (i NeuronFlags) String() string { return enums.String(i, _NeuronFlagsMap) }

// SetString sets the NeuronFlags value from its string representation,
// and returns an error if the string is invalid.
func (i *NeuronFlags) SetString(s string) error {
	return enums.SetString(i, s, _NeuronFlagsValueMap, "NeuronFlags")
}

// Int64 returns the NeuronFlags value as an int64.
func (i NeuronFlags) Int64() int64 { return int64(i) }

// SetInt64 sets the NeuronFlags value from an int64.
func (i *NeuronFlags) SetInt64(in int64) { *i = NeuronFlags(in) }

// Desc returns the description of the NeuronFlags value.
func (i NeuronFlags) Desc() string { return enums.Desc(i, _NeuronFlagsDescMap) }

// NeuronFlagsValues returns all possible values for the type NeuronFlags.
func NeuronFlagsValues() []NeuronFlags { return _NeuronFlagsValues }

// Values returns all possible values for the type NeuronFlags.
func (i NeuronFlags) Values() []enums.Enum { return enums.Values(_NeuronFlagsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i NeuronFlags) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *NeuronFlags) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "NeuronFlags")
}

var _NeuronVarsValues = []NeuronVars{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92}

// NeuronVarsN is the highest valid value for type NeuronVars, plus one.
//
//gosl:start
const NeuronVarsN NeuronVars = 93

//gosl:end

var _NeuronVarsValueMap = map[string]NeuronVars{`Spike`: 0, `Spiked`: 1, `Act`: 2, `ActInt`: 3, `Ge`: 4, `Gi`: 5, `Gk`: 6, `Inet`: 7, `Vm`: 8, `VmDend`: 9, `ISI`: 10, `ISIAvg`: 11, `Ext`: 12, `Target`: 13, `CaM`: 14, `CaP`: 15, `CaD`: 16, `CaDPrev`: 17, `CaSyn`: 18, `LearnCa`: 19, `LearnCaM`: 20, `LearnCaP`: 21, `LearnCaD`: 22, `CaDiff`: 23, `LearnDiff`: 24, `GaM`: 25, `GaP`: 26, `GaD`: 27, `TimeDiff`: 28, `TimePeak`: 29, `TimeCycle`: 30, `LearnNow`: 31, `RLRate`: 32, `ETrace`: 33, `ETrLearn`: 34, `GnmdaSyn`: 35, `Gnmda`: 36, `GnmdaLrn`: 37, `GnmdaMaint`: 38, `NmdaCa`: 39, `Gvgcc`: 40, `VgccM`: 41, `VgccH`: 42, `VgccCa`: 43, `VgccCaInt`: 44, `Burst`: 45, `BurstPrv`: 46, `CtxtGe`: 47, `CtxtGeRaw`: 48, `CtxtGeOrig`: 49, `GgabaB`: 50, `GababM`: 51, `GababX`: 52, `Gak`: 53, `SSGiDend`: 54, `GknaMed`: 55, `GknaSlow`: 56, `Gkir`: 57, `KirM`: 58, `Gsk`: 59, `SKCaIn`: 60, `SKCaR`: 61, `SKCaM`: 62, `Gmahp`: 63, `MahpN`: 64, `Gsahp`: 65, `SahpCa`: 66, `SahpN`: 67, `ActM`: 68, `ActP`: 69, `Beta1`: 70, `Beta2`: 71, `CaPMax`: 72, `CaPMaxCa`: 73, `GeNoise`: 74, `GeNoiseP`: 75, `GiNoise`: 76, `GiNoiseP`: 77, `GeExt`: 78, `GeRaw`: 79, `GeSyn`: 80, `GiRaw`: 81, `GiSyn`: 82, `GeInt`: 83, `GeIntNorm`: 84, `GiInt`: 85, `GModRaw`: 86, `GModSyn`: 87, `SMaintP`: 88, `GMaintRaw`: 89, `GMaintSyn`: 90, `NeurFlags`: 91, `CaBins`: 92}

var _NeuronVarsDescMap = map[NeuronVars]string{0: `Spike is whether neuron has spiked or not on this cycle (0 or 1).`, 1: `Spiked is 1 if neuron has spiked within the last 10 cycles (msecs), corresponding to a nominal max spiking rate of 100 Hz, 0 otherwise. Useful for visualization and computing activity levels in terms of average spiked levels.`, 2: `Act is rate-coded activation value reflecting instantaneous estimated rate of spiking, based on 1 / ISIAvg. It is integrated over time for ActInt which is then used for performance statistics and layer average activations, etc. Should not be used for learning or other computations: just for stats / display.`, 3: `ActInt is integrated running-average activation value computed from Act with time constant Act.Dt.IntTau, to produce a longer-term integrated value reflecting the overall activation state across the ThetaCycle time scale, as the overall response of network to current input state. This is copied to ActM and ActP at the ends of the minus and plus phases, respectively, and used in computing some performance-level statistics (based on ActM). Should not be used for learning or other computations.`, 4: `Ge is total excitatory conductance, including all forms of excitation (e.g., NMDA). Does *not* include the Gbar.E factor.`, 5: `Gi is total inhibitory synaptic conductance, i.e., the net inhibitory input to the neuron. Does *not* include the Gbar.I factor.`, 6: `Gk is total potassium conductance, typically reflecting sodium-gated potassium currents involved in adaptation effects. Does *not* include the Gbar.K factor.`, 7: `Inet is net current produced by all channels, which drives update of Vm.`, 8: `Vm is the membrane potential at the cell body, which integrates Inet current over time, and drives spiking at the axon initial segment of the neuron.`, 9: `VmDend is the dendritic membrane potential, which has a slower time constant than Vm and is not subject to the VmR reset after spiking.`, 10: `ISI is the current inter-spike-interval, which counts up since last spike. Starts at -1 when initialized.`, 11: `ISIAvg is the average inter-spike-interval, i.e., the average time interval between spikes, integrated with ISITau rate constant (relatively fast) to capture something close to an instantaneous spiking rate. Starts at -1 when initialized, and goes to -2 after first spike, and is only valid after the second spike post-initialization.`, 12: `Ext is the external input: drives activation of unit from outside influences (e.g., sensory input).`, 13: `Target is the target value: drives learning to produce this activation value.`, 14: `CaM is the spike-driven calcium trace at the neuron level, which then drives longer time-integrated variables: [CaP] and [CaD]. These variables are used for statistics and display to capture spiking activity at different timescales. They fluctuate more than [Act] and [ActInt], but are closer to the biological variables driving learning. CaM is the exponential integration of SpikeG * Spike using the MTau time constant (typically 5), and simulates a calmodulin (CaM) like signal, at an abstract level.`, 15: `CaP is the continuous cascaded integration of [CaM] using the PTau time constant (typically 40), representing a neuron-level, purely spiking version of the plus, LTP direction of weight change in the Kinase learning rule, dependent on CaMKII. This is not used for learning (see [LearnCaP]), but instead for statistics as a representation of recent activity.`, 16: `CaD is the continuous cascaded integration [CaP] using the DTau time constant (typically 40), representing a neuron-level, purely spiking version of the minus, LTD direction of weight change in the Kinase learning rule, dependent on DAPK1. This is not used for learning (see [LearnCaD]), but instead for statistics as a representation of trial-level activity.`, 17: `CaDPrev is the final [CaD] activation state at the end of previous theta cycle. This is used for specialized learning mechanisms that operate on delayed sending activations.`, 18: `CaSyn is the neuron-level integration of spike-driven calcium, used to approximate synaptic calcium influx as a product of sender and receiver neuron CaSyn values, which are integrated separately because it is computationally much more efficient. CaSyn enters into a Sender * Receiver product at each synapse to give the effective credit assignment factor for learning. This value is driven directly by spikes, with an exponential integration time constant of 30 msec (default), which captures the coincidence window for pre*post firing on NMDA receptor opening. The neuron [CaBins] values record the temporal trajectory of CaSyn over the course of the theta cycle window, and then the pre*post product is integrated over these bins at the synaptic level.`, 19: `LearnCa is the receiving neuron calcium signal, which is integrated up to [LearnCaP] and [LearnCaD], the difference of which is the temporal error component of the kinase cortical learning rule. LearnCa combines NMDA via [NmdaCa] and spiking-driven VGCC [VgccCaInt] calcium sources. The NMDA signal reflects both sending and receiving activity, while the VGCC signal is purely receiver spiking, and a balance of both works best.`, 20: `LearnCaM is the integrated [LearnCa] at the MTau timescale (typically 5), simulating a calmodulin (CaM) like signal, which then drives [LearnCaP], and [LearnCaD] for the delta signal for error-driven learning.`, 21: `LearnCaP is the cascaded integration of [LearnCaM] using the PTau time constant (typically 40), representing the plus, LTP direction of weight change, capturing the function of CaMKII in the Kinase learning rule.`, 22: `LearnCaD is the cascaded integration of [LearnCaP] using the DTau time constant (typically 40), representing the minus, LTD direction of weight change, capturing the function of DAPK1 in the Kinase learning rule.`, 23: `CaDiff is difference between [LearnCaP] - [LearnCaD]. This is the error signal that drives error-driven learning.`, 24: `LearnDiff is the actual difference signal that drives learning, which is computed from [CaDiff] for neocortical neurons, but specifically at the point of learning ([LearnNow]).`, 25: `GaM is first-level integration of all input conductances g_a, which then drives longer time-integrated variables: [GaP] and [GaD]. These variables are used for timing of learning based on bursts of activity change over time: at the minus and plus phases.`, 26: `GaP is the continuous cascaded integration of [GaM] using the PTau time constant (typically 40), representing a neuron-level, all-conductance-based version of the plus, LTP direction of weight change in the Kinase learning rule.`, 27: `GaD is the continuous cascaded integration of [GaP] using the DTau time constant (typically 40), representing a neuron-level, all-conductance-based version of the minus, LTD direction of weight change in the Kinase learning rule.`, 28: `TimeDiff is the running time-average of |P - D| (absolute value), used for determining the timing of learning in terms of onsets of peaks. See [TimePeak]. GaP - GaD is used, as it is smoother and more reliable than LearnCaP - D.`, 29: `TimePeak is the value of the current peak (local maximum) of [TimeDiff]. This typically occurs at the onset of the minus phase, and drives the timing of learning a given number of cycles after that.`, 30: `TimeCycle is the absolute cycle where [TimePeak] occurred.`, 31: `LearnNow is the absolute cycle (ms, CyclesTotal) when the receiving neuron learns. For neocortex, either at end of theta cycle or based on timing computed from [TimeCycle] per [LearnTimingParams].`, 32: `RLRate is recv-unit based learning rate multiplier, reflecting the sigmoid derivative computed from [CaD] of recv unit, and the normalized difference (CaP - CaD) / MAX(CaP - CaD).`, 33: `ETrace is the eligibility trace for this neuron.`, 34: `ETrLearn is the learning factor for the eligibility trace for this neuron. 1 + ETraceScale * [ETrace]`, 35: `GnmdaSyn is the integrated NMDA synaptic current on the receiving neuron. It adds GeRaw and decays with a time constant.`, 36: `Gnmda is the net postsynaptic (receiving) NMDA conductance, after Mg V-gating and Gbar. This is added directly to Ge as it has the same reversal potential.`, 37: `GnmdaLrn is learning version of integrated NMDA recv synaptic current. It adds [GeRaw] and decays with a time constant. This drives [NmdaCa] that then drives [LearnCa] for learning.`, 38: `GnmdaMaint is net postsynaptic maintenance NMDA conductance, computed from [GMaintSyn] and [GMaintRaw], after Mg V-gating and Gbar. This is added directly to Ge as it has the same reversal potential.`, 39: `NmdaCa is NMDA calcium computed from GnmdaLrn, drives learning via CaM.`, 40: `Gvgcc is conductance (via Ca) for VGCC voltage gated calcium channels.`, 41: `VgccM is activation gate of VGCC channels.`, 42: `VgccH inactivation gate of VGCC channels.`, 43: `VgccCa is the instantaneous VGCC calcium flux: can be driven by spiking or directly from Gvgcc.`, 44: `VgccCaInt is the time-integrated VGCC calcium flux. This is actually what drives learning.`, 45: `Burst is the layer 5 IB intrinsic bursting neural activation value, computed by thresholding the [CaP] value in Super superficial layers.`, 46: `BurstPrv is previous Burst bursting activation from prior time step. Used for context-based learning.`, 47: `CtxtGe is context (temporally delayed) excitatory conductance, driven by deep bursting at end of the plus phase, for CT layers.`, 48: `CtxtGeRaw is raw update of context (temporally delayed) excitatory conductance, driven by deep bursting at end of the plus phase, for CT layers.`, 49: `CtxtGeOrig is original CtxtGe value prior to any decay factor. Updates at end of plus phase.`, 50: `GgabaB is net GABA-B conductance, after Vm gating and Gk + Gbase. Applies to Gk, not Gi, for GIRK, with .1 reversal potential.`, 51: `GababM is the GABA-B / GIRK activation, which is a time-integrated value with rise and decay time constants.`, 52: `GababX is GABA-B / GIRK internal drive variable. This gets the raw activation and decays.`, 53: `Gak is the conductance of A-type K potassium channels.`, 54: `SSGiDend is the amount of SST+ somatostatin positive slow spiking inhibition applied to dendritic Vm (VmDend).`, 55: `GknaMed is the conductance of sodium-gated potassium channel (KNa) medium dynamics (Slick), which produces accommodation / adaptation.`, 56: `GknaSlow is the conductance of sodium-gated potassium channel (KNa) slow dynamics (Slack), which produces accommodation / adaptation.`, 57: `Gkir is the conductance of the potassium (K) inwardly rectifying channel, which is strongest at low membrane potentials. Can be modulated by DA.`, 58: `KirM is the Kir potassium (K) inwardly rectifying gating value.`, 59: `Gsk is Calcium-gated potassium channel conductance as a function of Gbar * SKCaM.`, 60: `SKCaIn is intracellular calcium store level, available to be released with spiking as SKCaR, which can bind to SKCa receptors and drive K current. replenishment is a function of spiking activity being below a threshold.`, 61: `SKCaR is the released amount of intracellular calcium, from SKCaIn, as a function of spiking events. This can bind to SKCa channels and drive K currents.`, 62: `SKCaM is the Calcium-gated potassium channel gating factor, driven by SKCaR via a Hill equation as in chans.SKPCaParams.`, 63: `Gmahp is medium time scale AHP conductance.`, 64: `MahpN is accumulating voltage-gated gating value for the medium time scale AHP.`, 65: `Gsahp is slow time scale AHP conductance.`, 66: `SahpCa is slowly accumulating calcium value that drives the slow AHP.`, 67: `SahpN is the sAHP gating value.`, 68: `ActM is ActInt activation state at end of third quarter, representing the posterior-cortical minus phase activation. This is used for statistics and monitoring network performance. Should not be used for learning or other computations.`, 69: `ActP is ActInt activation state at end of fourth quarter, representing the posterior-cortical plus_phase activation. This is used for statistics and monitoring network performance. Should not be used for learning or other computations.`, 70: `Beta1 is the activation state at the first beta cycle within current state processing window (i.e., at 50 msec), as saved by Beta1() function. Used for example in hippocampus for CA3, CA1 learning.`, 71: `Beta2 is the activation state at the second beta cycle within current state processing window (i.e., at 100 msec), as saved by Beta2() function. Used for example in hippocampus for CA3, CA1 learning.`, 72: `CaPMax is the maximum [CaP] across one theta cycle time window (max of CaPMaxCa). It is used for specialized algorithms that have more phasic behavior within a single trial, e.g., BG Matrix layer gating. Also useful for visualization of peak activity of neurons.`, 73: `CaPMaxCa is the Ca integrated like [CaP] but only starting at the MaxCycStart cycle, to prevent inclusion of carryover spiking from prior theta cycle trial. The PTau time constant otherwise results in significant carryover. This is the input to CaPMax.`, 74: `GeNoise is integrated noise excitatory conductance, added into Ge.`, 75: `GeNoiseP is accumulating poisson probability factor for driving excitatory noise spiking. Multiply times uniform random deviate at each time step, until it gets below the target threshold based on poisson lambda as function of noise firing rate.`, 76: `GiNoise is integrated noise inhibitory conductance, added into Gi.`, 77: `GiNoiseP is accumulating poisson probability factor for driving inhibitory noise spiking. Multiply times uniform random deviate at each time step, until it gets below the target threshold based on poisson lambda as a function of noise firing rate.`, 78: `GeExt is extra excitatory conductance added to Ge, from Ext input, GeCtxt etc.`, 79: `GeRaw is the raw excitatory conductance (net input) received from senders = current raw spiking drive.`, 80: `GeSyn is the time-integrated total excitatory (AMPA) synaptic conductance, with an instantaneous rise time from each spike (in GeRaw) and exponential decay with Dt.GeTau, aggregated over pathways. Does *not* include Gbar.E.`, 81: `GiRaw is the raw inhibitory conductance (net input) received from senders = current raw spiking drive.`, 82: `GiSyn is time-integrated total inhibitory synaptic conductance, with an instantaneous rise time from each spike (in GiRaw) and exponential decay with Dt.GiTau, aggregated over pathways -- does *not* include Gbar.I. This is added with computed FFFB inhibition to get the full inhibition in Gi.`, 83: `GeInt is integrated running-average activation value computed from Ge with time constant Act.Dt.IntTau, to produce a longer-term integrated value reflecting the overall Ge level across the ThetaCycle time scale (Ge itself fluctuates considerably). This is useful for stats to set strength of connections etc to get neurons into right range of overall excitatory drive.`, 84: `GeIntNorm is normalized GeInt value (divided by the layer maximum). This is used for learning in layers that require learning on subthreshold activity.`, 85: `GiInt is integrated running-average activation value computed from GiSyn with time constant Act.Dt.IntTau, to produce a longer-term integrated value reflecting the overall synaptic Gi level across the ThetaCycle time scale (Gi itself fluctuates considerably). Useful for stats to set strength of connections etc to get neurons into right range of overall inhibitory drive.`, 86: `GModRaw is raw modulatory conductance, received from GType = ModulatoryG pathways.`, 87: `GModSyn is syn integrated modulatory conductance, received from GType = ModulatoryG pathways.`, 88: `SMaintP is accumulating poisson probability factor for driving self-maintenance by simulating a population of mutually interconnected neurons. Multiply times uniform random deviate at each time step, until it gets below the target threshold based on poisson lambda based on accumulating self maint factor.`, 89: `GMaintRaw is raw maintenance conductance, received from GType = MaintG pathways.`, 90: `GMaintSyn is syn integrated maintenance conductance, integrated using MaintNMDA params.`, 91: `NeurFlags are bit flags for binary state variables, which are converted to / from uint32. These need to be in Vars because they can be differential per data (for ext inputs) and are writable (indexes are read only).`, 92: `CaBins is a vector of values starting here, with aggregated [CaSyn] values in time bins of [CaBinCycles] across two theta cycles, for computing synaptic calcium efficiently. Each bin = Sum(CaSyn / CaBinCycles). Total number of bins = 2 * [Context.ThetaCycles] / CaBinCycles. Use [CaBinForCycle] to access. Synaptic calcium is integrated from sender * receiver CaBins values, with weights for CaP vs CaD that reflect their faster vs. slower time constants, respectively. CaD is used for the credit assignment factor, while CaP - CaD is used directly for error-driven learning at Target layers.`}

var _NeuronVarsMap = map[NeuronVars]string{0: `Spike`, 1: `Spiked`, 2: `Act`, 3: `ActInt`, 4: `Ge`, 5: `Gi`, 6: `Gk`, 7: `Inet`, 8: `Vm`, 9: `VmDend`, 10: `ISI`, 11: `ISIAvg`, 12: `Ext`, 13: `Target`, 14: `CaM`, 15: `CaP`, 16: `CaD`, 17: `CaDPrev`, 18: `CaSyn`, 19: `LearnCa`, 20: `LearnCaM`, 21: `LearnCaP`, 22: `LearnCaD`, 23: `CaDiff`, 24: `LearnDiff`, 25: `GaM`, 26: `GaP`, 27: `GaD`, 28: `TimeDiff`, 29: `TimePeak`, 30: `TimeCycle`, 31: `LearnNow`, 32: `RLRate`, 33: `ETrace`, 34: `ETrLearn`, 35: `GnmdaSyn`, 36: `Gnmda`, 37: `GnmdaLrn`, 38: `GnmdaMaint`, 39: `NmdaCa`, 40: `Gvgcc`, 41: `VgccM`, 42: `VgccH`, 43: `VgccCa`, 44: `VgccCaInt`, 45: `Burst`, 46: `BurstPrv`, 47: `CtxtGe`, 48: `CtxtGeRaw`, 49: `CtxtGeOrig`, 50: `GgabaB`, 51: `GababM`, 52: `GababX`, 53: `Gak`, 54: `SSGiDend`, 55: `GknaMed`, 56: `GknaSlow`, 57: `Gkir`, 58: `KirM`, 59: `Gsk`, 60: `SKCaIn`, 61: `SKCaR`, 62: `SKCaM`, 63: `Gmahp`, 64: `MahpN`, 65: `Gsahp`, 66: `SahpCa`, 67: `SahpN`, 68: `ActM`, 69: `ActP`, 70: `Beta1`, 71: `Beta2`, 72: `CaPMax`, 73: `CaPMaxCa`, 74: `GeNoise`, 75: `GeNoiseP`, 76: `GiNoise`, 77: `GiNoiseP`, 78: `GeExt`, 79: `GeRaw`, 80: `GeSyn`, 81: `GiRaw`, 82: `GiSyn`, 83: `GeInt`, 84: `GeIntNorm`, 85: `GiInt`, 86: `GModRaw`, 87: `GModSyn`, 88: `SMaintP`, 89: `GMaintRaw`, 90: `GMaintSyn`, 91: `NeurFlags`, 92: `CaBins`}

// String returns the string representation of this NeuronVars value.
func (i NeuronVars) String() string { return enums.String(i, _NeuronVarsMap) }

// SetString sets the NeuronVars value from its string representation,
// and returns an error if the string is invalid.
func (i *NeuronVars) SetString(s string) error {
	return enums.SetString(i, s, _NeuronVarsValueMap, "NeuronVars")
}

// Int64 returns the NeuronVars value as an int64.
func (i NeuronVars) Int64() int64 { return int64(i) }

// SetInt64 sets the NeuronVars value from an int64.
func (i *NeuronVars) SetInt64(in int64) { *i = NeuronVars(in) }

// Desc returns the description of the NeuronVars value.
func (i NeuronVars) Desc() string { return enums.Desc(i, _NeuronVarsDescMap) }

// NeuronVarsValues returns all possible values for the type NeuronVars.
func NeuronVarsValues() []NeuronVars { return _NeuronVarsValues }

// Values returns all possible values for the type NeuronVars.
func (i NeuronVars) Values() []enums.Enum { return enums.Values(_NeuronVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i NeuronVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *NeuronVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "NeuronVars")
}

var _NeuronAvgVarsValues = []NeuronAvgVars{0, 1, 2, 3, 4, 5, 6}

// NeuronAvgVarsN is the highest valid value for type NeuronAvgVars, plus one.
//
//gosl:start
const NeuronAvgVarsN NeuronAvgVars = 7

//gosl:end

var _NeuronAvgVarsValueMap = map[string]NeuronAvgVars{`ActAvg`: 0, `AvgPct`: 1, `TrgAvg`: 2, `DTrgAvg`: 3, `AvgDif`: 4, `GeBase`: 5, `GiBase`: 6}

var _NeuronAvgVarsDescMap = map[NeuronAvgVars]string{0: `ActAvg is average activation (of minus phase activation state) over long time intervals (time constant = Dt.LongAvgTau). Useful for finding hog units and seeing overall distribution of activation.`, 1: `AvgPct is ActAvg as a proportion of overall layer activation. This is used for synaptic scaling to match TrgAvg activation, updated at SlowInterval intervals.`, 2: `TrgAvg is neuron&#39;s target average activation as a proportion of overall layer activation, assigned during weight initialization, driving synaptic scaling relative to AvgPct.`, 3: `DTrgAvg is change in neuron&#39;s target average activation as a result of unit-wise error gradient. Acts like a bias weight. MPI needs to share these across processors.`, 4: `AvgDif is AvgPct - TrgAvg, i.e., the error in overall activity level relative to set point for this neuron, which drives synaptic scaling. Updated at SlowInterval intervals.`, 5: `GeBase is baseline level of Ge, added to GeRaw, for intrinsic excitability.`, 6: `GiBase is baseline level of Gi, added to GiRaw, for intrinsic excitability.`}

var _NeuronAvgVarsMap = map[NeuronAvgVars]string{0: `ActAvg`, 1: `AvgPct`, 2: `TrgAvg`, 3: `DTrgAvg`, 4: `AvgDif`, 5: `GeBase`, 6: `GiBase`}

// String returns the string representation of this NeuronAvgVars value.
func (i NeuronAvgVars) String() string { return enums.String(i, _NeuronAvgVarsMap) }

// SetString sets the NeuronAvgVars value from its string representation,
// and returns an error if the string is invalid.
func (i *NeuronAvgVars) SetString(s string) error {
	return enums.SetString(i, s, _NeuronAvgVarsValueMap, "NeuronAvgVars")
}

// Int64 returns the NeuronAvgVars value as an int64.
func (i NeuronAvgVars) Int64() int64 { return int64(i) }

// SetInt64 sets the NeuronAvgVars value from an int64.
func (i *NeuronAvgVars) SetInt64(in int64) { *i = NeuronAvgVars(in) }

// Desc returns the description of the NeuronAvgVars value.
func (i NeuronAvgVars) Desc() string { return enums.Desc(i, _NeuronAvgVarsDescMap) }

// NeuronAvgVarsValues returns all possible values for the type NeuronAvgVars.
func NeuronAvgVarsValues() []NeuronAvgVars { return _NeuronAvgVarsValues }

// Values returns all possible values for the type NeuronAvgVars.
func (i NeuronAvgVars) Values() []enums.Enum { return enums.Values(_NeuronAvgVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i NeuronAvgVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *NeuronAvgVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "NeuronAvgVars")
}

var _NeuronIndexVarsValues = []NeuronIndexVars{0, 1, 2}

// NeuronIndexVarsN is the highest valid value for type NeuronIndexVars, plus one.
//
//gosl:start
const NeuronIndexVarsN NeuronIndexVars = 3

//gosl:end

var _NeuronIndexVarsValueMap = map[string]NeuronIndexVars{`NrnNeurIndex`: 0, `NrnLayIndex`: 1, `NrnSubPool`: 2}

var _NeuronIndexVarsDescMap = map[NeuronIndexVars]string{0: `NrnNeurIndex is the index of this neuron within its owning layer.`, 1: `NrnLayIndex is the index of the layer that this neuron belongs to, needed for neuron-level parallel code.`, 2: `NrnSubPool is the index of the sub-level inhibitory pool for this neuron (only for 4D shapes, the pool (unit-group / hypercolumn) structure level). Indicies start at 1 -- 0 is layer-level pool (is 0 if no sub-pools).`}

var _NeuronIndexVarsMap = map[NeuronIndexVars]string{0: `NrnNeurIndex`, 1: `NrnLayIndex`, 2: `NrnSubPool`}

// String returns the string representation of this NeuronIndexVars value.
func (i NeuronIndexVars) String() string { return enums.String(i, _NeuronIndexVarsMap) }

// SetString sets the NeuronIndexVars value from its string representation,
// and returns an error if the string is invalid.
func (i *NeuronIndexVars) SetString(s string) error {
	return enums.SetString(i, s, _NeuronIndexVarsValueMap, "NeuronIndexVars")
}

// Int64 returns the NeuronIndexVars value as an int64.
func (i NeuronIndexVars) Int64() int64 { return int64(i) }

// SetInt64 sets the NeuronIndexVars value from an int64.
func (i *NeuronIndexVars) SetInt64(in int64) { *i = NeuronIndexVars(in) }

// Desc returns the description of the NeuronIndexVars value.
func (i NeuronIndexVars) Desc() string { return enums.Desc(i, _NeuronIndexVarsDescMap) }

// NeuronIndexVarsValues returns all possible values for the type NeuronIndexVars.
func NeuronIndexVarsValues() []NeuronIndexVars { return _NeuronIndexVarsValues }

// Values returns all possible values for the type NeuronIndexVars.
func (i NeuronIndexVars) Values() []enums.Enum { return enums.Values(_NeuronIndexVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i NeuronIndexVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *NeuronIndexVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "NeuronIndexVars")
}

var _PathTypesValues = []PathTypes{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}

// PathTypesN is the highest valid value for type PathTypes, plus one.
//
//gosl:start
const PathTypesN PathTypes = 15

//gosl:end

var _PathTypesValueMap = map[string]PathTypes{`ForwardPath`: 0, `BackPath`: 1, `LateralPath`: 2, `InhibPath`: 3, `CTCtxtPath`: 4, `DSPatchPath`: 5, `VSPatchPath`: 6, `VSMatrixPath`: 7, `DSMatrixPath`: 8, `CNIOPath`: 9, `CNeUpPath`: 10, `RWPath`: 11, `TDPredPath`: 12, `BLAPath`: 13, `HipPath`: 14}

var _PathTypesDescMap = map[PathTypes]string{0: `Forward is a feedforward, bottom-up pathway from sensory inputs to higher layers`, 1: `Back is a feedback, top-down pathway from higher layers back to lower layers`, 2: `Lateral is a lateral pathway within the same layer / area`, 3: `Inhib is an inhibitory pathway that drives inhibitory synaptic conductances instead of the default excitatory ones.`, 4: `CTCtxt are pathways from Superficial layers to CT layers that send Burst activations drive updating of CtxtGe excitatory conductance, at end of plus (51B Bursting) phase. Biologically, this pathway comes from the PT layer 5IB neurons, but it is simpler to use the Super neurons directly, and PT are optional for most network types. These pathways also use a special learning rule that takes into account the temporal delays in the activation states. Can also add self context from CT for deeper temporal context.`, 5: `DSPatchPath implements the DSPatch learning rule: dW = ACh * DA * X * Y where DA is D1 vs. D2 modulated DA level, X = sending activity factor, Y = receiving activity factor, and ACh provides overall modulation.`, 6: `VSPatchPath implements the VSPatch learning rule: dW = ACh * DA * X * Y where DA is D1 vs. D2 modulated DA level, X = sending activity factor, Y = receiving activity factor, and ACh provides overall modulation.`, 7: `VSMatrixPath is for ventral striatum matrix (SPN / MSN) neurons supporting trace-based learning, where an initial trace of synaptic co-activity is formed, and then modulated by subsequent phasic dopamine &amp; ACh when an outcome occurs. This bridges the temporal gap between gating activity and subsequent outcomes, and is based biologically on synaptic tags. Trace is reset at time of reward based on ACh level (from CINs in biology).`, 8: `DSMatrixPath is for dorsal striatum matrix (SPN / MSN) neurons supporting trace-based learning, where an initial trace of synaptic co-activity is formed, and then modulated by subsequent phasic dopamine &amp; ACh when an outcome occurs. This bridges the temporal gap between gating activity and subsequent outcomes, and is based biologically on synaptic tags. Trace is reset at time of reward based on ACh level (from CINs in biology).`, 9: `CNIOPath is a cerebellar nucleus pathway trained by IO error signals.`, 10: `CNeUpPath is a cerebellar excitatory output neuron pathway, for upbound microzones, which learns drive the output neurons at their target baseline activity level by adapting the inhibitory input strength.`, 11: `RWPath does dopamine-modulated learning for reward prediction: Da * Send.CaP (integrated current spiking activity). Uses RLPredPath parameters. Use in RWPredLayer typically to generate reward predictions. If the Da sign is positive, the first recv unit learns fully; for negative, second one learns fully. Lower lrate applies for opposite cases. Weights are positive-only.`, 12: `TDPredPath does dopamine-modulated learning for reward prediction: DWt = Da * Send.CaDPrev (activity on *previous* timestep) Uses RLPredPath parameters. Use in TDPredLayer typically to generate reward predictions. If the Da sign is positive, the first recv unit learns fully; for negative, second one learns fully. Lower lrate applies for opposite cases. Weights are positive-only.`, 13: `BLAPath implements the Rubicon BLA learning rule: dW = ACh * X_t-1 * (Y_t - Y_t-1) The recv delta is across trials, where the US should activate on trial boundary, to enable sufficient time for gating through to OFC, so BLA initially learns based on US present - US absent. It can also learn based on CS onset if there is a prior CS that predicts that.`, 14: `HipPath is a special pathway for the hippocampus. TODO: fixme.`}

var _PathTypesMap = map[PathTypes]string{0: `ForwardPath`, 1: `BackPath`, 2: `LateralPath`, 3: `InhibPath`, 4: `CTCtxtPath`, 5: `DSPatchPath`, 6: `VSPatchPath`, 7: `VSMatrixPath`, 8: `DSMatrixPath`, 9: `CNIOPath`, 10: `CNeUpPath`, 11: `RWPath`, 12: `TDPredPath`, 13: `BLAPath`, 14: `HipPath`}

// String returns the string representation of this PathTypes value.
func (i PathTypes) String() string { return enums.String(i, _PathTypesMap) }

// SetString sets the PathTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *PathTypes) SetString(s string) error {
	return enums.SetString(i, s, _PathTypesValueMap, "PathTypes")
}

// Int64 returns the PathTypes value as an int64.
func (i PathTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the PathTypes value from an int64.
func (i *PathTypes) SetInt64(in int64) { *i = PathTypes(in) }

// Desc returns the description of the PathTypes value.
func (i PathTypes) Desc() string { return enums.Desc(i, _PathTypesDescMap) }

// PathTypesValues returns all possible values for the type PathTypes.
func PathTypesValues() []PathTypes { return _PathTypesValues }

// Values returns all possible values for the type PathTypes.
func (i PathTypes) Values() []enums.Enum { return enums.Values(_PathTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i PathTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *PathTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "PathTypes")
}

var _GPLayerTypesValues = []GPLayerTypes{0, 1, 2}

// GPLayerTypesN is the highest valid value for type GPLayerTypes, plus one.
//
//gosl:start
const GPLayerTypesN GPLayerTypes = 3

//gosl:end

var _GPLayerTypesValueMap = map[string]GPLayerTypes{`GPePr`: 0, `GPeAk`: 1, `GPi`: 2}

var _GPLayerTypesDescMap = map[GPLayerTypes]string{0: `GPePr is the set of prototypical GPe neurons, mediating classical NoGo`, 1: `GPeAk is arkypallidal layer of GPe neurons, receiving inhibition from GPePr and projecting inhibition to Mtx`, 2: `GPi is the inner globus pallidus, functionally equivalent to SNr, receiving from MtxGo and GPePr, and sending inhibition to VThal`}

var _GPLayerTypesMap = map[GPLayerTypes]string{0: `GPePr`, 1: `GPeAk`, 2: `GPi`}

// String returns the string representation of this GPLayerTypes value.
func (i GPLayerTypes) String() string { return enums.String(i, _GPLayerTypesMap) }

// SetString sets the GPLayerTypes value from its string representation,
// and returns an error if the string is invalid.
func (i *GPLayerTypes) SetString(s string) error {
	return enums.SetString(i, s, _GPLayerTypesValueMap, "GPLayerTypes")
}

// Int64 returns the GPLayerTypes value as an int64.
func (i GPLayerTypes) Int64() int64 { return int64(i) }

// SetInt64 sets the GPLayerTypes value from an int64.
func (i *GPLayerTypes) SetInt64(in int64) { *i = GPLayerTypes(in) }

// Desc returns the description of the GPLayerTypes value.
func (i GPLayerTypes) Desc() string { return enums.Desc(i, _GPLayerTypesDescMap) }

// GPLayerTypesValues returns all possible values for the type GPLayerTypes.
func GPLayerTypesValues() []GPLayerTypes { return _GPLayerTypesValues }

// Values returns all possible values for the type GPLayerTypes.
func (i GPLayerTypes) Values() []enums.Enum { return enums.Values(_GPLayerTypesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i GPLayerTypes) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *GPLayerTypes) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "GPLayerTypes")
}

var _PoolIndexVarsValues = []PoolIndexVars{0, 1, 2, 3}

// PoolIndexVarsN is the highest valid value for type PoolIndexVars, plus one.
//
//gosl:start
const PoolIndexVarsN PoolIndexVars = 4

//gosl:end

var _PoolIndexVarsValueMap = map[string]PoolIndexVars{`PoolNeurSt`: 0, `PoolNeurEd`: 1, `PoolLayerIdx`: 2, `PoolIsLayer`: 3}

var _PoolIndexVarsDescMap = map[PoolIndexVars]string{0: `PoolNeurSt is the starting layer-wise index within the list of neurons in this pool. Add layer starting neuron index (NeurSt) to get index into global network neurons list.`, 1: `PoolNeurEd is the ending (exclusive) layer-wise index within the list of neurons in this pool. Add layer starting neuron index (NeurSt) to get index into global network neurons list.`, 2: `PoolLayerIdx is the layer index for this pool.`, 3: `PoolIsLayer is true (&gt; 0) if this pool represents the entire layer, which is always the first pool in the list of pools for a layer.`}

var _PoolIndexVarsMap = map[PoolIndexVars]string{0: `PoolNeurSt`, 1: `PoolNeurEd`, 2: `PoolLayerIdx`, 3: `PoolIsLayer`}

// String returns the string representation of this PoolIndexVars value.
func (i PoolIndexVars) String() string { return enums.String(i, _PoolIndexVarsMap) }

// SetString sets the PoolIndexVars value from its string representation,
// and returns an error if the string is invalid.
func (i *PoolIndexVars) SetString(s string) error {
	return enums.SetString(i, s, _PoolIndexVarsValueMap, "PoolIndexVars")
}

// Int64 returns the PoolIndexVars value as an int64.
func (i PoolIndexVars) Int64() int64 { return int64(i) }

// SetInt64 sets the PoolIndexVars value from an int64.
func (i *PoolIndexVars) SetInt64(in int64) { *i = PoolIndexVars(in) }

// Desc returns the description of the PoolIndexVars value.
func (i PoolIndexVars) Desc() string { return enums.Desc(i, _PoolIndexVarsDescMap) }

// PoolIndexVarsValues returns all possible values for the type PoolIndexVars.
func PoolIndexVarsValues() []PoolIndexVars { return _PoolIndexVarsValues }

// Values returns all possible values for the type PoolIndexVars.
func (i PoolIndexVars) Values() []enums.Enum { return enums.Values(_PoolIndexVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i PoolIndexVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *PoolIndexVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "PoolIndexVars")
}

var _PoolIntVarsValues = []PoolIntVars{0, 1, 2, 3, 4, 5}

// PoolIntVarsN is the highest valid value for type PoolIntVars, plus one.
//
//gosl:start
const PoolIntVarsN PoolIntVars = 6

//gosl:end

var _PoolIntVarsValueMap = map[string]PoolIntVars{`Clamped`: 0, `PoolGated`: 1, `FFsRawInt`: 2, `FBsRawInt`: 3, `GeExtRawInt`: 4, `PoolIntAvgMaxStart`: 5}

var _PoolIntVarsDescMap = map[PoolIntVars]string{0: `Clamped if true (!=0), this layer is hard-clamped and should use GeExts exclusively for PV.`, 1: `PoolGated is true (&gt; 0) if this pool gated (for [MatrixLayer], [BGThalLayer])`, 2: `FFsRawInt is the int32 atomic add compatible integration of [fsfffb.FFsRaw].`, 3: `FBsRawInt is the int32 atomic add compatible integration of [fsfffb.FBsRaw].`, 4: `GeExtRawInt is the int32 atomic add compatible integration of [fsfffb.GeExtRaw].`, 5: `PoolIntAvgMaxStart is the starting point for int32 AvgMax variables. Use AvgMaxIntVarIndex to get the relevant variable index. There are only values for Cycle phase, for the different variables.`}

var _PoolIntVarsMap = map[PoolIntVars]string{0: `Clamped`, 1: `PoolGated`, 2: `FFsRawInt`, 3: `FBsRawInt`, 4: `GeExtRawInt`, 5: `PoolIntAvgMaxStart`}

// String returns the string representation of this PoolIntVars value.
func (i PoolIntVars) String() string { return enums.String(i, _PoolIntVarsMap) }

// SetString sets the PoolIntVars value from its string representation,
// and returns an error if the string is invalid.
func (i *PoolIntVars) SetString(s string) error {
	return enums.SetString(i, s, _PoolIntVarsValueMap, "PoolIntVars")
}

// Int64 returns the PoolIntVars value as an int64.
func (i PoolIntVars) Int64() int64 { return int64(i) }

// SetInt64 sets the PoolIntVars value from an int64.
func (i *PoolIntVars) SetInt64(in int64) { *i = PoolIntVars(in) }

// Desc returns the description of the PoolIntVars value.
func (i PoolIntVars) Desc() string { return enums.Desc(i, _PoolIntVarsDescMap) }

// PoolIntVarsValues returns all possible values for the type PoolIntVars.
func PoolIntVarsValues() []PoolIntVars { return _PoolIntVarsValues }

// Values returns all possible values for the type PoolIntVars.
func (i PoolIntVars) Values() []enums.Enum { return enums.Values(_PoolIntVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i PoolIntVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *PoolIntVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "PoolIntVars")
}

var _AvgMaxValues = []AvgMax{0, 1}

// AvgMaxN is the highest valid value for type AvgMax, plus one.
//
//gosl:start
const AvgMaxN AvgMax = 2

//gosl:end

var _AvgMaxValueMap = map[string]AvgMax{`Avg`: 0, `Max`: 1}

var _AvgMaxDescMap = map[AvgMax]string{0: ``, 1: ``}

var _AvgMaxMap = map[AvgMax]string{0: `Avg`, 1: `Max`}

// String returns the string representation of this AvgMax value.
func (i AvgMax) String() string { return enums.String(i, _AvgMaxMap) }

// SetString sets the AvgMax value from its string representation,
// and returns an error if the string is invalid.
func (i *AvgMax) SetString(s string) error { return enums.SetString(i, s, _AvgMaxValueMap, "AvgMax") }

// Int64 returns the AvgMax value as an int64.
func (i AvgMax) Int64() int64 { return int64(i) }

// SetInt64 sets the AvgMax value from an int64.
func (i *AvgMax) SetInt64(in int64) { *i = AvgMax(in) }

// Desc returns the description of the AvgMax value.
func (i AvgMax) Desc() string { return enums.Desc(i, _AvgMaxDescMap) }

// AvgMaxValues returns all possible values for the type AvgMax.
func AvgMaxValues() []AvgMax { return _AvgMaxValues }

// Values returns all possible values for the type AvgMax.
func (i AvgMax) Values() []enums.Enum { return enums.Values(_AvgMaxValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i AvgMax) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *AvgMax) UnmarshalText(text []byte) error { return enums.UnmarshalText(i, text, "AvgMax") }

var _AvgMaxPhasesValues = []AvgMaxPhases{0, 1, 2, 3}

// AvgMaxPhasesN is the highest valid value for type AvgMaxPhases, plus one.
//
//gosl:start
const AvgMaxPhasesN AvgMaxPhases = 4

//gosl:end

var _AvgMaxPhasesValueMap = map[string]AvgMaxPhases{`Cycle`: 0, `Minus`: 1, `Plus`: 2, `Prev`: 3}

var _AvgMaxPhasesDescMap = map[AvgMaxPhases]string{0: `Cycle is the current cycle, which is the source for the rest.`, 1: `Minus is at the end of the minus phase.`, 2: `Plus is at the end of the plus phase.`, 3: `Prev is at the end of the previous plus phase.`}

var _AvgMaxPhasesMap = map[AvgMaxPhases]string{0: `Cycle`, 1: `Minus`, 2: `Plus`, 3: `Prev`}

// String returns the string representation of this AvgMaxPhases value.
func (i AvgMaxPhases) String() string { return enums.String(i, _AvgMaxPhasesMap) }

// SetString sets the AvgMaxPhases value from its string representation,
// and returns an error if the string is invalid.
func (i *AvgMaxPhases) SetString(s string) error {
	return enums.SetString(i, s, _AvgMaxPhasesValueMap, "AvgMaxPhases")
}

// Int64 returns the AvgMaxPhases value as an int64.
func (i AvgMaxPhases) Int64() int64 { return int64(i) }

// SetInt64 sets the AvgMaxPhases value from an int64.
func (i *AvgMaxPhases) SetInt64(in int64) { *i = AvgMaxPhases(in) }

// Desc returns the description of the AvgMaxPhases value.
func (i AvgMaxPhases) Desc() string { return enums.Desc(i, _AvgMaxPhasesDescMap) }

// AvgMaxPhasesValues returns all possible values for the type AvgMaxPhases.
func AvgMaxPhasesValues() []AvgMaxPhases { return _AvgMaxPhasesValues }

// Values returns all possible values for the type AvgMaxPhases.
func (i AvgMaxPhases) Values() []enums.Enum { return enums.Values(_AvgMaxPhasesValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i AvgMaxPhases) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *AvgMaxPhases) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "AvgMaxPhases")
}

var _AvgMaxVarsValues = []AvgMaxVars{0, 1, 2, 3, 4, 5, 6}

// AvgMaxVarsN is the highest valid value for type AvgMaxVars, plus one.
//
//gosl:start
const AvgMaxVarsN AvgMaxVars = 7

//gosl:end

var _AvgMaxVarsValueMap = map[string]AvgMaxVars{`CaP`: 0, `CaD`: 1, `CaPMax`: 2, `Act`: 3, `GeInt`: 4, `GiInt`: 5, `AvgDif`: 6}

var _AvgMaxVarsDescMap = map[AvgMaxVars]string{0: `CaP is the primary variable for tracking overall pool activity over a recent timescale, integrated at roughly 40 msec time constant.`, 1: `CaD is a slower moving activation signal, capable of reflecting activity over the entire trial.`, 2: `CaPMax is the maximum CaP over the trial of processing.`, 3: `Act is the computed rate-code equivalent of current spike rate.`, 4: `GeInt is the integrated running-average value of excitatory conductance.`, 5: `GiInt is the integrated running-average value of inhibitory conductance.`, 6: `AvgDif is the integrated AvgDif between ActPct - TrgAvg. Only the Plus phase is used.`}

var _AvgMaxVarsMap = map[AvgMaxVars]string{0: `CaP`, 1: `CaD`, 2: `CaPMax`, 3: `Act`, 4: `GeInt`, 5: `GiInt`, 6: `AvgDif`}

// String returns the string representation of this AvgMaxVars value.
func (i AvgMaxVars) String() string { return enums.String(i, _AvgMaxVarsMap) }

// SetString sets the AvgMaxVars value from its string representation,
// and returns an error if the string is invalid.
func (i *AvgMaxVars) SetString(s string) error {
	return enums.SetString(i, s, _AvgMaxVarsValueMap, "AvgMaxVars")
}

// Int64 returns the AvgMaxVars value as an int64.
func (i AvgMaxVars) Int64() int64 { return int64(i) }

// SetInt64 sets the AvgMaxVars value from an int64.
func (i *AvgMaxVars) SetInt64(in int64) { *i = AvgMaxVars(in) }

// Desc returns the description of the AvgMaxVars value.
func (i AvgMaxVars) Desc() string { return enums.Desc(i, _AvgMaxVarsDescMap) }

// AvgMaxVarsValues returns all possible values for the type AvgMaxVars.
func AvgMaxVarsValues() []AvgMaxVars { return _AvgMaxVarsValues }

// Values returns all possible values for the type AvgMaxVars.
func (i AvgMaxVars) Values() []enums.Enum { return enums.Values(_AvgMaxVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i AvgMaxVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *AvgMaxVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "AvgMaxVars")
}

var _SynapseVarsValues = []SynapseVars{0, 1, 2, 3, 4}

// SynapseVarsN is the highest valid value for type SynapseVars, plus one.
//
//gosl:start
const SynapseVarsN SynapseVars = 5

//gosl:end

var _SynapseVarsValueMap = map[string]SynapseVars{`Wt`: 0, `LWt`: 1, `SWt`: 2, `DWt`: 3, `DSWt`: 4}

var _SynapseVarsDescMap = map[SynapseVars]string{0: `Wt is the effective synaptic weight value, determining how much conductance one presynaptic spike drives into the receiving neuron. Biologically it represents the number of effective AMPA receptors in the synapse. Wt = [SWt] * WtSig([LWt]), where WtSig is the sigmoidal constrast enhancement function that produces values between 0-2 based on LWt, centered on 1.`, 1: `LWt is the rapid, online learning, linear weight value. It learns on every trial according to the learning rate (LRate) parameter. Biologically, this represents the internal biochemical processes that drive the trafficking of AMPA receptors in the synaptic density.`, 2: `SWt is a slowly adapting structural weight value, which acts as a multiplicative scaling factor on net synaptic efficacy [Wt]. Biologically it represents the physical size and efficacy of the dendritic spine. SWt values adapt in a slower outer loop along with synaptic scaling, with constraints to prevent runaway positive feedback loops and maintain variance and further capacity to learn. Initial weight variance is partially or fully captured in the SWt values, with LWt capturing the remainder.`, 3: `DWt is delta (change in) synaptic weight, from learning. This updates [LWt] on every trial. It is reset to 0 after it is applied, but the network view captures this value just prior to application.`, 4: `DSWt is the accumulated change in the [SWt] slow structural weight, computed as the accumulation of [DWt] values over the longer slow weight update window.`}

var _SynapseVarsMap = map[SynapseVars]string{0: `Wt`, 1: `LWt`, 2: `SWt`, 3: `DWt`, 4: `DSWt`}

// String returns the string representation of this SynapseVars value.
func (i SynapseVars) String() string { return enums.String(i, _SynapseVarsMap) }

// SetString sets the SynapseVars value from its string representation,
// and returns an error if the string is invalid.
func (i *SynapseVars) SetString(s string) error {
	return enums.SetString(i, s, _SynapseVarsValueMap, "SynapseVars")
}

// Int64 returns the SynapseVars value as an int64.
func (i SynapseVars) Int64() int64 { return int64(i) }

// SetInt64 sets the SynapseVars value from an int64.
func (i *SynapseVars) SetInt64(in int64) { *i = SynapseVars(in) }

// Desc returns the description of the SynapseVars value.
func (i SynapseVars) Desc() string { return enums.Desc(i, _SynapseVarsDescMap) }

// SynapseVarsValues returns all possible values for the type SynapseVars.
func SynapseVarsValues() []SynapseVars { return _SynapseVarsValues }

// Values returns all possible values for the type SynapseVars.
func (i SynapseVars) Values() []enums.Enum { return enums.Values(_SynapseVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i SynapseVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *SynapseVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "SynapseVars")
}

var _SynapseTraceVarsValues = []SynapseTraceVars{0, 1, 2}

// SynapseTraceVarsN is the highest valid value for type SynapseTraceVars, plus one.
//
//gosl:start
const SynapseTraceVarsN SynapseTraceVars = 3

//gosl:end

var _SynapseTraceVarsValueMap = map[string]SynapseTraceVars{`Tr`: 0, `DTr`: 1, `DiDWt`: 2}

var _SynapseTraceVarsDescMap = map[SynapseTraceVars]string{0: `Tr is trace of synaptic activity over time, which is used for credit assignment in learning. In MatrixPath this is a tag that is then updated later when US occurs.`, 1: `DTr is delta (change in) Tr trace of synaptic activity over time.`, 2: `DiDWt is delta weight for each data parallel index (Di). This is directly computed from the Ca values (in cortical version) and then aggregated into the overall DWt (which may be further integrated across MPI nodes), which then drives changes in Wt values.`}

var _SynapseTraceVarsMap = map[SynapseTraceVars]string{0: `Tr`, 1: `DTr`, 2: `DiDWt`}

// String returns the string representation of this SynapseTraceVars value.
func (i SynapseTraceVars) String() string { return enums.String(i, _SynapseTraceVarsMap) }

// SetString sets the SynapseTraceVars value from its string representation,
// and returns an error if the string is invalid.
func (i *SynapseTraceVars) SetString(s string) error {
	return enums.SetString(i, s, _SynapseTraceVarsValueMap, "SynapseTraceVars")
}

// Int64 returns the SynapseTraceVars value as an int64.
func (i SynapseTraceVars) Int64() int64 { return int64(i) }

// SetInt64 sets the SynapseTraceVars value from an int64.
func (i *SynapseTraceVars) SetInt64(in int64) { *i = SynapseTraceVars(in) }

// Desc returns the description of the SynapseTraceVars value.
func (i SynapseTraceVars) Desc() string { return enums.Desc(i, _SynapseTraceVarsDescMap) }

// SynapseTraceVarsValues returns all possible values for the type SynapseTraceVars.
func SynapseTraceVarsValues() []SynapseTraceVars { return _SynapseTraceVarsValues }

// Values returns all possible values for the type SynapseTraceVars.
func (i SynapseTraceVars) Values() []enums.Enum { return enums.Values(_SynapseTraceVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i SynapseTraceVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *SynapseTraceVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "SynapseTraceVars")
}

var _SynapseIndexVarsValues = []SynapseIndexVars{0, 1, 2}

// SynapseIndexVarsN is the highest valid value for type SynapseIndexVars, plus one.
//
//gosl:start
const SynapseIndexVarsN SynapseIndexVars = 3

//gosl:end

var _SynapseIndexVarsValueMap = map[string]SynapseIndexVars{`SynRecvIndex`: 0, `SynSendIndex`: 1, `SynPathIndex`: 2}

var _SynapseIndexVarsDescMap = map[SynapseIndexVars]string{0: `SynRecvIndex is receiving neuron index in network&#39;s global list of neurons`, 1: `SynSendIndex is sending neuron index in network&#39;s global list of neurons`, 2: `SynPathIndex is pathway index in global list of pathways organized as [Layers][RecvPaths]`}

var _SynapseIndexVarsMap = map[SynapseIndexVars]string{0: `SynRecvIndex`, 1: `SynSendIndex`, 2: `SynPathIndex`}

// String returns the string representation of this SynapseIndexVars value.
func (i SynapseIndexVars) String() string { return enums.String(i, _SynapseIndexVarsMap) }

// SetString sets the SynapseIndexVars value from its string representation,
// and returns an error if the string is invalid.
func (i *SynapseIndexVars) SetString(s string) error {
	return enums.SetString(i, s, _SynapseIndexVarsValueMap, "SynapseIndexVars")
}

// Int64 returns the SynapseIndexVars value as an int64.
func (i SynapseIndexVars) Int64() int64 { return int64(i) }

// SetInt64 sets the SynapseIndexVars value from an int64.
func (i *SynapseIndexVars) SetInt64(in int64) { *i = SynapseIndexVars(in) }

// Desc returns the description of the SynapseIndexVars value.
func (i SynapseIndexVars) Desc() string { return enums.Desc(i, _SynapseIndexVarsDescMap) }

// SynapseIndexVarsValues returns all possible values for the type SynapseIndexVars.
func SynapseIndexVarsValues() []SynapseIndexVars { return _SynapseIndexVarsValues }

// Values returns all possible values for the type SynapseIndexVars.
func (i SynapseIndexVars) Values() []enums.Enum { return enums.Values(_SynapseIndexVarsValues) }

// MarshalText implements the [encoding.TextMarshaler] interface.
func (i SynapseIndexVars) MarshalText() ([]byte, error) { return []byte(i.String()), nil }

// UnmarshalText implements the [encoding.TextUnmarshaler] interface.
func (i *SynapseIndexVars) UnmarshalText(text []byte) error {
	return enums.UnmarshalText(i, text, "SynapseIndexVars")
}
