// Code generated by "goal build"; DO NOT EDIT.
//line layer.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"errors"
	"fmt"
	"io"
	"log"
	"math/rand"
	"strconv"
	"strings"

	"cogentcore.org/core/core"
	"cogentcore.org/core/icons"
	"cogentcore.org/core/math32"
	"cogentcore.org/core/tree"
	"github.com/emer/emergent/v2/emer"
	"github.com/emer/emergent/v2/weights"
)

// index naming:
// lni = layer-based neuron index (0 = first neuron in layer)
// ni  = absolute whole network neuron index

// Layer implements the basic Axon spiking activation function,
// and manages learning in the pathways.
type Layer struct {
	emer.LayerBase

	// Params are layer parameters (pointer to item in Network.LayerParams).
	Params *LayerParams

	// our parent network, in case we need to use it to find
	// other layers etc; set when added by network.
	Network *Network `copier:"-" json:"-" xml:"-" display:"-"`

	// Type is the type of layer, which drives specialized computation as needed.
	Type LayerTypes

	// NNeurons is the number of neurons in the layer.
	NNeurons uint32 `display:"-"`

	// NeurStIndex is the starting index of neurons for this layer within
	// the global Network list.
	NeurStIndex uint32 `display:"-" inactive:"-"`

	// NPools is the number of inhibitory pools based on layer shape,
	// with the first one representing the entire set of neurons in the layer,
	// and 4D shaped layers have sub-pools after that.
	NPools uint32 `display:"-"`

	// MaxData is the maximum amount of input data that can be processed in
	// parallel in one pass of the network (copied from [NetworkIndexes]).
	// Neuron, Pool, Values storage is allocated to hold this amount.
	MaxData uint32 `display:"-"`

	// RecvPaths is the list of receiving pathways into this layer from other layers.
	RecvPaths []*Path

	// SendPaths is the list of sending pathways from this layer to other layers.
	SendPaths []*Path

	// BuildConfig has configuration data set when the network is configured,
	// that is used during the network Build() process via PostBuild method,
	// after all the structure of the network has been fully constructed.
	// In particular, the Params is nil until Build, so setting anything
	// specific in there (e.g., an index to another layer) must be done
	// as a second pass.  Note that Params are all applied after Build
	// and can set user-modifiable params, so this is for more special
	// algorithm structural parameters set during ConfigNet() methods.
	BuildConfig map[string]string `table:"-"`

	// DefaultParams are closures that apply default parameters
	// prior to user-set parameters. These are useful for specific layer
	// functionality in specialized brain areas (e.g., Rubicon, BG etc)
	// not associated with a layer type, which otherwise is used to hard-code
	// initial default parameters.
	DefaultParams []func(ly *LayerParams) `display:"-"`
}

// emer.Layer interface methods

func (ly *Layer) TypeName() string           { return ly.Type.String() }
func (ly *Layer) TypeNumber() int            { return int(ly.Type) }
func (ly *Layer) NumRecvPaths() int          { return len(ly.RecvPaths) }
func (ly *Layer) RecvPath(idx int) emer.Path { return ly.RecvPaths[idx] }
func (ly *Layer) NumSendPaths() int          { return len(ly.SendPaths) }
func (ly *Layer) SendPath(idx int) emer.Path { return ly.SendPaths[idx] }
func (ly *Layer) AddClass(cls ...string) *Layer {
	ly.LayerBase.AddClass(cls...)
	return ly
}

func (ly *Layer) Defaults() { //types:add
	ctx := ly.Network.Context()
	li := ly.Index
	if ly.Params != nil {
		ly.Params.Type = ly.Type
		ly.Params.Defaults()
		for di := uint32(0); di < ly.MaxData; di++ {
			LayerStates.Set(1, int(li), int(di), int(LayerGiMult))
		}
		// ly.Params.Learn.CaLearn.Dt.PDTauForNCycles(int(ctx.ThetaCycles))
		// ly.Params.Learn.CaSpike.Dt.PDTauForNCycles(int(ctx.ThetaCycles))
	}
	for _, pt := range ly.RecvPaths { // must do path defaults first, then custom
		pt.Defaults()
	}
	if ly.Params == nil {
		return
	}
	switch ly.Type {
	case InputLayer:
		ly.Params.Acts.Clamp.Ge = 1.5
		ly.Params.Inhib.Layer.Gi = 0.9
		ly.Params.Inhib.Pool.Gi = 0.9
		ly.Params.Learn.TrgAvgAct.SubMean = 0
	case TargetLayer:
		ly.Params.Acts.Clamp.Ge = 0.8
		ly.Params.Learn.TrgAvgAct.SubMean = 0
	// ly.Params.Learn.RLRate.SigmoidMin = 1

	case CTLayer:
		ly.Params.CTDefaults()
	case PTMaintLayer:
		ly.PTMaintDefaults()
	case PTPredLayer:
		ly.Params.PTPredDefaults()
	case PulvinarLayer:
		ly.Params.PulvDefaults()

	case RewLayer:
		ly.Params.RWDefaults()
	case RWPredLayer:
		ly.Params.RWDefaults()
		ly.Params.RWPredDefaults()
	case RWDaLayer:
		ly.Params.RWDefaults()
	case TDPredLayer:
		ly.Params.TDDefaults()
		ly.Params.TDPredDefaults()
	case TDIntegLayer, TDDaLayer:
		ly.Params.TDDefaults()

	case LDTLayer:
		ly.LDTDefaults()
	case BLALayer:
		ly.BLADefaults()
	case CeMLayer:
		ly.CeMDefaults()
	case VSPatchLayer:
		ly.Params.VSPatchDefaults()
	case DrivesLayer:
		ly.Params.DrivesDefaults()
	case UrgencyLayer:
		ly.Params.UrgencyDefaults()
	case USLayer:
		ly.Params.USDefaults()
	case PVLayer:
		ly.Params.PVDefaults()

	case MatrixLayer:
		ly.MatrixDefaults()
	case GPLayer:
		ly.GPDefaults()
	case STNLayer:
		ly.STNDefaults()
	case BGThalLayer:
		ly.BGThalDefaults()
	case VSGatedLayer:
		ly.Params.VSGatedDefaults()
	}
	ly.Params.CT.DecayForNCycles(int(ctx.ThetaCycles))
	ly.applyDefaultParams()
	ly.UpdateParams()
}

// Update is an interface for generically updating after edits
// this should be used only for the values on the struct itself.
// UpdateParams is used to update all parameters, including Path.
func (ly *Layer) Update() {
	if ly.Params == nil {
		return
	}
	if !ly.Is4D() && ly.Params.Inhib.Pool.On.IsTrue() {
		ly.Params.Inhib.Pool.On.SetBool(false)
	}
	ly.Params.Update()
}

// UpdateParams updates all params given any changes that might
// have been made to individual values including those in the
// receiving pathways of this layer.
// This is not called Update because it is not just about the
// local values in the struct.
func (ly *Layer) UpdateParams() {
	ly.Update()
	for _, pt := range ly.RecvPaths {
		pt.UpdateParams()
	}
}

// todo: not standard:
func (ly *Layer) SetOff(off bool) {
	ly.Off = off
	// a Path is off if either the sending or the receiving layer is off
	// or if the path has been set to Off directly
	for _, pt := range ly.RecvPaths {
		pt.Off = pt.Send.Off || off
	}
	for _, pt := range ly.SendPaths {
		pt.Off = pt.Recv.Off || off
	}
}

// RecipToSendPath finds the reciprocal pathway to
// the given sending pathway within the ly layer.
// i.e., where ly is instead the *receiving* layer from same other layer B
// that is the receiver of the spj pathway we're sending to.
//
//	ly = A,  other layer = B:
//
// spj: S=A -> R=B
// rpj: R=A <- S=B
//
// returns false if not found.
func (ly *Layer) RecipToSendPath(spj *Path) (*Path, bool) {
	for _, rpj := range ly.RecvPaths {
		if rpj.Send == spj.Recv { // B = sender of rpj, recv of spj
			return rpj, true
		}
	}
	return nil, false
}

// RecipToRecvPath finds the reciprocal pathway to
// the given recv pathway within the ly layer.
// i.e., where ly is instead the *sending* layer to same other layer B
// that is the sender of the rpj pathway we're receiving from.
//
//	ly = A, other layer = B:
//
// rpj: R=A <- S=B
// spj: S=A -> R=B
//
// returns false if not found.
func (ly *Layer) RecipToRecvPath(rpj *Path) (*Path, bool) {
	for _, spj := range ly.SendPaths {
		if spj.Recv == rpj.Send { // B = sender of rpj, recv of spj
			return spj, true
		}
	}
	return nil, false
}

// AddDefaultParams adds given default param setting function.
func (ly *Layer) AddDefaultParams(fun func(ly *LayerParams)) {
	ly.DefaultParams = append(ly.DefaultParams, fun)
}

// applyDefaultParams applies DefaultParams default parameters.
// Called by Layer.Defaults()
func (ly *Layer) applyDefaultParams() {
	for _, f := range ly.DefaultParams {
		f(ly.Params)
	}
}

// AllParams returns a listing of all parameters in the Layer
func (ly *Layer) AllParams() string {
	str := "/////////////////////////////////////////////////\nLayer: " + ly.Name + "\n" + ly.Params.AllParams()
	for _, pt := range ly.RecvPaths {
		str += pt.AllParams()
	}
	return str
}

////////  Build

// SetBuildConfig sets named configuration parameter to given string value
// to be used in the PostBuild stage -- mainly for layer names that need to be
// looked up and turned into indexes, after entire network is built.
func (ly *Layer) SetBuildConfig(param, val string) {
	ly.BuildConfig[param] = val
}

// BuildConfigByName looks for given BuildConfig option by name,
// and reports & returns an error if not found.
func (ly *Layer) BuildConfigByName(nm string) (string, error) {
	cfg, ok := ly.BuildConfig[nm]
	if !ok {
		err := fmt.Errorf("Layer: %s does not have BuildConfig: %s set -- error in ConfigNet", ly.Name, nm)
		log.Println(err)
		return cfg, err
	}
	return cfg, nil
}

// BuildConfigFindLayer looks for BuildConfig of given name
// and if found, looks for layer with corresponding name.
// if mustName is true, then an error is logged if the BuildConfig
// name does not exist.  An error is always logged if the layer name
// is not found.  -1 is returned in any case of not found.
func (ly *Layer) BuildConfigFindLayer(nm string, mustName bool) int32 {
	idx := int32(-1)
	if rnm, ok := ly.BuildConfig[nm]; ok {
		dly := ly.Network.LayerByName(rnm)
		if dly != nil {
			idx = int32(dly.Index)
		}
	} else {
		if mustName {
			err := fmt.Errorf("Layer: %s does not have BuildConfig: %s set -- error in ConfigNet", ly.Name, nm)
			log.Println(err)
		}
	}
	return idx
}

// BuildSubPools initializes neuron start / end indexes for sub-pools
func (ly *Layer) BuildSubPools(ctx *Context) {
	if !ly.Is4D() {
		return
	}
	sh := ly.Shape.Sizes
	spy := sh[0]
	spx := sh[1]
	spi := uint32(1)
	for py := 0; py < spy; py++ {
		for px := 0; px < spx; px++ {
			soff := uint32(ly.Shape.IndexTo1D(py, px, 0, 0))
			eoff := uint32(ly.Shape.IndexTo1D(py, px, sh[2]-1, sh[3]-1) + 1)
			pi := ly.Params.PoolIndex(spi)
			PoolIxs.Set(soff, int(pi), int(PoolNeurSt))
			PoolIxs.Set(eoff, int(pi), int(PoolNeurEd))
			for lni := soff; lni < eoff; lni++ {
				ni := ly.NeurStIndex + lni
				NeuronIxs.Set(spi, int(ni), int(NrnSubPool))
			}
			spi++
		}
	}
}

// BuildPools builds the inhibitory pools structures -- nu = number of units in layer
func (ly *Layer) BuildPools(ctx *Context, nn uint32) error {
	np := 1 + ly.NumPools()
	for di := uint32(0); di < ly.MaxData; di++ {
		lpi := ly.Params.PoolIndex(0)
		PoolIxs.Set(0, int(lpi), int(PoolNeurSt))
		PoolIxs.Set(nn, int(lpi), int(PoolNeurEd))
		PoolIxs.Set(1, int(lpi), int(PoolIsLayer))
	}
	if np > 1 {
		ly.BuildSubPools(ctx)
	}
	return nil
}

// BuildPaths builds the pathways, send-side
func (ly *Layer) BuildPaths(ctx *Context) error {
	emsg := ""
	for _, pt := range ly.SendPaths {
		if pt.Off {
			continue
		}
		err := pt.Build()
		if err != nil {
			emsg += err.Error() + "\n"
		}
	}
	if emsg != "" {
		return errors.New(emsg)
	}
	return nil
}

// Build constructs the layer state, including calling Build on the pathways
func (ly *Layer) Build() error {
	ctx := ly.Network.Context()
	nn := uint32(ly.Shape.Len())
	if nn == 0 {
		return fmt.Errorf("Build Layer %v: no units specified in Shape", ly.Name)
	}
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		NeuronIxs.Set(lni, int(ni), int(NrnNeurIndex))
		NeuronIxs.Set(uint32(ly.Index), int(ni), int(NrnLayIndex))
	}
	err := ly.BuildPools(ctx, nn)
	if err != nil {
		return err
	}
	err = ly.BuildPaths(ctx)
	ly.PostBuild()
	return err
}

// PostBuild performs special post-Build() configuration steps for specific algorithms,
// using configuration data set in BuildConfig during the ConfigNet process.
func (ly *Layer) PostBuild() {
	ly.Params.LayInhib.Index1 = ly.BuildConfigFindLayer("LayInhib1Name", false) // optional
	ly.Params.LayInhib.Index2 = ly.BuildConfigFindLayer("LayInhib2Name", false) // optional
	ly.Params.LayInhib.Index3 = ly.BuildConfigFindLayer("LayInhib3Name", false) // optional
	ly.Params.LayInhib.Index4 = ly.BuildConfigFindLayer("LayInhib4Name", false) // optional

	switch ly.Type {
	case PulvinarLayer:
		ly.PulvPostBuild()

	case LDTLayer:
		ly.LDTPostBuild()
	case RWDaLayer:
		ly.RWDaPostBuild()
	case TDIntegLayer:
		ly.TDIntegPostBuild()
	case TDDaLayer:
		ly.TDDaPostBuild()

	case BLALayer, CeMLayer, USLayer, PVLayer, VSPatchLayer:
		ly.RubiconPostBuild()

	case MatrixLayer:
		ly.MatrixPostBuild()
	case GPLayer:
		ly.GPPostBuild()
	}
}

// UnitVarNames returns a list of variable names available on the units in this layer
func (ly *Layer) UnitVarNames() []string {
	return NeuronVarNames
}

// UnitVarProps returns properties for variables
func (ly *Layer) UnitVarProps() map[string]string {
	return NeuronVarProps
}

// UnitVarIndex returns the index of given variable within the Neuron,
// according to *this layer's* UnitVarNames() list (using a map to lookup index),
// or -1 and error message if not found.
func (ly *Layer) UnitVarIndex(varNm string) (int, error) {
	return NeuronVarIndexByName(varNm)
}

// UnitVarNum returns the number of Neuron-level variables
// for this layer.  This is needed for extending indexes in derived types.
func (ly *Layer) UnitVarNum() int {
	return len(NeuronVarNames)
}

// UnitValue1D returns value of given variable index on given unit, using 1-dimensional index.
// returns NaN on invalid index.
// This is the core unit var access method used by other methods.
func (ly *Layer) UnitValue1D(varIndex int, idx, di int) float32 {
	if idx < 0 || idx >= int(ly.NNeurons) {
		return math32.NaN()
	}
	if varIndex < 0 || varIndex >= ly.UnitVarNum() {
		return math32.NaN()
	}
	if di < 0 || di >= int(ly.MaxData) {
		return math32.NaN()
	}
	ni := ly.NeurStIndex + uint32(idx)
	nvars := ly.UnitVarNum()
	neurVars := int(CaBins) + NNeuronCaBins
	layVarSt := nvars - NNeuronLayerVars
	if varIndex >= layVarSt {
		lvi := varIndex - layVarSt
		switch lvi {
		case 0:
			return GlobalScalars.Value(int(GvDA), int(uint32(di)))
		case 1:
			return GlobalScalars.Value(int(GvACh), int(uint32(di)))
		case 2:
			return GlobalScalars.Value(int(GvNE), int(uint32(di)))
		case 3:
			return GlobalScalars.Value(int(GvSer), int(uint32(di)))
		case 4:
			pi := ly.Params.PoolIndex(NeuronIxs.Value(int(ni), int(NrnSubPool)))
			return float32(PoolsInt.Value(int(pi), int(di), int(PoolGated)))
		}
	} else if varIndex >= neurVars {
		return NeuronAvgs.Value(int(ni), int(NeuronVars(varIndex-neurVars)))
	} else if varIndex < int(CaBins) {
		return Neurons.Value(int(ni), int(di), int(varIndex))
	} else {
		sbin := varIndex - int(CaBins)
		if sbin >= int(NetworkIxs[0].NCaBins) {
			return math32.NaN()
		}
		return Neurons.Value(int(ni), int(di), int(varIndex))
	}
	return math32.NaN()
}

// RecvPathValues fills in values of given synapse variable name,
// for pathway into given sending layer and neuron 1D index,
// for all receiving neurons in this layer,
// into given float32 slice (only resized if not big enough).
// pathType is the string representation of the path type -- used if non-empty,
// useful when there are multiple pathways between two layers.
// Returns error on invalid var name.
// If the receiving neuron is not connected to the given sending layer or neuron
// then the value is set to math32.NaN().
// Returns error on invalid var name or lack of recv path (vals always set to nan on path err).
func (ly *Layer) RecvPathValues(vals *[]float32, varNm string, sendLay emer.Layer, sendIndex1D int, pathType string) error {
	var err error
	nn := int(ly.NNeurons)
	if *vals == nil || cap(*vals) < nn {
		*vals = make([]float32, nn)
	} else if len(*vals) < nn {
		*vals = (*vals)[0:nn]
	}
	nan := math32.NaN()
	for i := 0; i < nn; i++ {
		(*vals)[i] = nan
	}
	if sendLay == nil {
		return fmt.Errorf("sending layer is nil")
	}
	slay := sendLay.AsEmer()
	var pt emer.Path
	if pathType != "" {
		pt, err = slay.SendPathByRecvNameType(ly.Name, pathType)
		if pt == nil {
			pt, err = slay.SendPathByRecvName(ly.Name)
		}
	} else {
		pt, err = slay.SendPathByRecvName(ly.Name)
	}
	if pt == nil {
		return err
	}
	if pt.AsEmer().Off {
		return fmt.Errorf("pathway is off")
	}
	for ri := 0; ri < nn; ri++ {
		(*vals)[ri] = pt.AsEmer().SynValue(varNm, sendIndex1D, ri) // this will work with any variable -- slower, but necessary
	}
	return nil
}

// SendPathValues fills in values of given synapse variable name,
// for pathway into given receiving layer and neuron 1D index,
// for all sending neurons in this layer,
// into given float32 slice (only resized if not big enough).
// pathType is the string representation of the path type -- used if non-empty,
// useful when there are multiple pathways between two layers.
// Returns error on invalid var name.
// If the sending neuron is not connected to the given receiving layer or neuron
// then the value is set to math32.NaN().
// Returns error on invalid var name or lack of recv path (vals always set to nan on path err).
func (ly *Layer) SendPathValues(vals *[]float32, varNm string, recvLay emer.Layer, recvIndex1D int, pathType string) error {
	var err error
	nn := int(ly.NNeurons)
	if *vals == nil || cap(*vals) < nn {
		*vals = make([]float32, nn)
	} else if len(*vals) < nn {
		*vals = (*vals)[0:nn]
	}
	nan := math32.NaN()
	for i := 0; i < nn; i++ {
		(*vals)[i] = nan
	}
	if recvLay == nil {
		return fmt.Errorf("receiving layer is nil")
	}
	rlay := recvLay.AsEmer()
	var pt emer.Path
	if pathType != "" {
		pt, err = rlay.RecvPathBySendNameType(ly.Name, pathType)
		if pt == nil {
			pt, err = rlay.RecvPathBySendName(ly.Name)
		}
	} else {
		pt, err = rlay.RecvPathBySendName(ly.Name)
	}
	if pt == nil {
		return err
	}
	if pt.AsEmer().Off {
		return fmt.Errorf("pathway is off")
	}
	for si := 0; si < nn; si++ {
		(*vals)[si] = pt.AsEmer().SynValue(varNm, si, recvIndex1D)
	}
	return nil
}

// VarRange returns the min / max values for given variable
// todo: support r. s. pathway values
// error occurs when variable name is not found.
func (ly *Layer) VarRange(varNm string) (min, max float32, err error) {
	nn := ly.NNeurons
	if nn == 0 {
		return
	}
	vidx, err := ly.UnitVarIndex(varNm)
	if err != nil {
		return
	}
	nvar := vidx

	v0 := Neurons.Value(int(ly.NeurStIndex), int(0), int(nvar))
	min = v0
	max = v0
	for lni := uint32(1); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		vl := Neurons.Value(int(ni), int(0), int(nvar))
		if vl < min {
			min = vl
		}
		if vl > max {
			max = vl
		}
	}
	return
}

////////	Weights

// WriteWeightsJSON writes the weights from this layer from the receiver-side perspective
// in a JSON text format.  We build in the indentation logic to make it much faster and
// more efficient.
func (ly *Layer) WriteWeightsJSON(w io.Writer, depth int) {
	li := ly.Index
	ly.MetaData = make(map[string]string)
	ly.MetaData["ActMAvg"] = fmt.Sprintf("%g", LayerStates.Value(int(li), int(0), int(LayerActMAvg)))
	ly.MetaData["ActPAvg"] = fmt.Sprintf("%g", LayerStates.Value(int(li), int(0), int(LayerActPAvg)))
	ly.MetaData["GiMult"] = fmt.Sprintf("%g", LayerStates.Value(int(li), int(0), int(LayerGiMult)))

	if ly.Params.IsLearnTrgAvg() {
		ly.LayerBase.WriteWeightsJSONBase(w, depth, "ActAvg", "TrgAvg")
	} else {
		ly.LayerBase.WriteWeightsJSONBase(w, depth)
	}
}

// SetWeights sets the weights for this layer from weights.Layer decoded values
func (ly *Layer) SetWeights(lw *weights.Layer) error {
	if ly.Off {
		return nil
	}
	li := ly.Index
	ctx := ly.Network.Context()
	if lw.MetaData != nil {
		for di := uint32(0); di < ly.MaxData; di++ {
			if am, ok := lw.MetaData["ActMAvg"]; ok {
				pv, _ := strconv.ParseFloat(am, 32)
				LayerStates.Set(float32(pv), int(li), int(di), int(LayerActMAvg))
			}
			if ap, ok := lw.MetaData["ActPAvg"]; ok {
				pv, _ := strconv.ParseFloat(ap, 32)
				LayerStates.Set(float32(pv), int(li), int(di), int(LayerActPAvg))
			}
			if gi, ok := lw.MetaData["GiMult"]; ok {
				pv, _ := strconv.ParseFloat(gi, 32)
				LayerStates.Set(float32(pv), int(li), int(di), int(LayerGiMult))
			}
		}
	}
	if lw.Units != nil {
		if ta, ok := lw.Units["ActAvg"]; ok {
			for lni := range ta {
				if lni > int(ly.NNeurons) {
					break
				}
				ni := ly.NeurStIndex + uint32(lni)
				NeuronAvgs.Set(ta[lni], int(ni), int(ActAvg))
			}
		}
		if ta, ok := lw.Units["TrgAvg"]; ok {
			for lni := range ta {
				if lni > int(ly.NNeurons) {
					break
				}
				ni := ly.NeurStIndex + uint32(lni)
				NeuronAvgs.Set(ta[lni], int(ni), int(TrgAvg))
			}
		}
	}
	var err error
	if len(lw.Paths) == ly.NumRecvPaths() { // this is essential if multiple paths from same layer
		for pi := range lw.Paths {
			pw := &lw.Paths[pi]
			pt := ly.RecvPaths[pi]
			er := pt.SetWeights(pw)
			if er != nil {
				err = er
			}
		}
	} else {
		for pi := range lw.Paths {
			pw := &lw.Paths[pi]
			pt, _ := ly.RecvPathBySendName(pw.From)
			if pt != nil {
				er := pt.SetWeights(pw)
				if er != nil {
					err = er
				}
			}
		}
	}
	ly.Params.AvgDifFromTrgAvg(ctx) // update AvgPct based on loaded ActAvg values
	return err
}

// JsonToParams reformates json output to suitable params display output
func JsonToParams(b []byte) string {
	br := strings.Replace(string(b), `"`, ``, -1)
	br = strings.Replace(br, ",\n", "", -1)
	br = strings.Replace(br, "{\n", "{", -1)
	br = strings.Replace(br, "} ", "}\n  ", -1)
	br = strings.Replace(br, "\n }", " }", -1)
	br = strings.Replace(br, "\n  }\n", " }", -1)
	return br[1:] + "\n"
}

// TestValues returns a map of key vals for testing
// ctrKey is a key of counters to contextualize values.
func (ly *Layer) TestValues(ctrKey string, vals map[string]float32) {
	for spi := uint32(0); spi < ly.NPools; spi++ {
		for di := uint32(0); di < ly.MaxData; di++ {
			pi := ly.Params.PoolIndex(spi)
			key := fmt.Sprintf("%s  Lay: %s\tPool: %d\tDi: %d", ctrKey, ly.Name, pi, di)
			PoolTestValues(pi, di, key, vals)
		}
	}
}

////////  Lesion

// UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer
func (ly *Layer) UnLesionNeurons() { //types:add
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		for di := uint32(0); di < ly.MaxData; di++ {
			NeuronClearFlag(NeuronOff, ni, di)
		}
	}
}

// LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer
// returns number of neurons lesioned.  Emits error if prop > 1 as indication that percent
// might have been passed
func (ly *Layer) LesionNeurons(prop float32) int { //types:add
	ly.UnLesionNeurons()
	if prop > 1 {
		log.Printf("LesionNeurons got a proportion > 1 -- must be 0-1 as *proportion* (not percent) of neurons to lesion: %v\n", prop)
		return 0
	}
	nn := ly.NNeurons
	if nn == 0 {
		return 0
	}
	p := rand.Perm(int(nn))
	nl := int(prop * float32(nn))
	for lni := uint32(0); lni < nn; lni++ {
		nip := uint32(p[lni])
		ni := ly.NeurStIndex + nip
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			NeuronSetFlag(NeuronOff, ni, di)
		}
	}
	return nl
}

// MakeToolbar is the standard core GUI toolbar for the layer when edited.
func (ly *Layer) MakeToolbar(p *tree.Plan) {
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.Defaults).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.InitWeights).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.InitActs).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.Separator) {})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.LesionNeurons).SetIcon(icons.Cut)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.UnLesionNeurons).SetIcon(icons.Cut)
	})
}
