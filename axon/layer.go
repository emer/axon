// Code generated by "goal build"; DO NOT EDIT.
//line layer.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"fmt"
	"log"
	"math/rand"
	"strings"

	"cogentcore.org/core/base/randx"
	"cogentcore.org/core/core"
	"cogentcore.org/core/icons"
	"cogentcore.org/core/tensor"
	"cogentcore.org/core/tree"
)

// index naming:
// lni = layer-based neuron index (0 = first neuron in layer)
// ni  = absolute whole network neuron index

func (ly *Layer) Defaults() { //types:add
	ctx := ly.Network.Context()
	li := ly.Index
	if ly.Params != nil {
		ly.Params.Type = ly.Type
		ly.Params.Defaults()
		for di := uint32(0); di < ly.MaxData; di++ {
			LayerStates.Set(1, int(LayerGiMult), int(li), int(di))
		}
		ly.Params.Learn.CaLearn.Dt.PDTauForNCycles(int(ctx.ThetaCycles))
		ly.Params.Learn.CaSpk.Dt.PDTauForNCycles(int(ctx.ThetaCycles))
	}
	for _, pt := range ly.RecvPaths { // must do path defaults first, then custom
		pt.Defaults()
	}
	if ly.Params == nil {
		return
	}
	switch ly.Type {
	case InputLayer:
		ly.Params.Acts.Clamp.Ge = 1.5
		ly.Params.Inhib.Layer.Gi = 0.9
		ly.Params.Inhib.Pool.Gi = 0.9
		ly.Params.Learn.TrgAvgAct.SubMean = 0
	case TargetLayer:
		ly.Params.Acts.Clamp.Ge = 0.8
		ly.Params.Learn.TrgAvgAct.SubMean = 0
	// ly.Params.Learn.RLRate.SigmoidMin = 1

	case CTLayer:
		ly.Params.CTDefaults()
	case PTMaintLayer:
		ly.PTMaintDefaults()
	case PTPredLayer:
		ly.Params.PTPredDefaults()
	case PulvinarLayer:
		ly.Params.PulvDefaults()

	case RewLayer:
		ly.Params.RWDefaults()
	case RWPredLayer:
		ly.Params.RWDefaults()
		ly.Params.RWPredDefaults()
	case RWDaLayer:
		ly.Params.RWDefaults()
	case TDPredLayer:
		ly.Params.TDDefaults()
		ly.Params.TDPredDefaults()
	case TDIntegLayer, TDDaLayer:
		ly.Params.TDDefaults()

	case LDTLayer:
		ly.LDTDefaults()
	case BLALayer:
		ly.BLADefaults()
	case CeMLayer:
		ly.CeMDefaults()
	case VSPatchLayer:
		ly.Params.VSPatchDefaults()
	case DrivesLayer:
		ly.Params.DrivesDefaults()
	case UrgencyLayer:
		ly.Params.UrgencyDefaults()
	case USLayer:
		ly.Params.USDefaults()
	case PVLayer:
		ly.Params.PVDefaults()

	case MatrixLayer:
		ly.MatrixDefaults()
	case GPLayer:
		ly.GPDefaults()
	case STNLayer:
		ly.STNDefaults()
	case BGThalLayer:
		ly.BGThalDefaults()
	case VSGatedLayer:
		ly.Params.VSGatedDefaults()
	}
	ly.Params.CT.DecayForNCycles(int(ctx.ThetaCycles))
	ly.ApplyDefaultParams()
	ly.UpdateParams()
}

// Update is an interface for generically updating after edits
// this should be used only for the values on the struct itself.
// UpdateParams is used to update all parameters, including Path.
func (ly *Layer) Update() {
	if ly.Params == nil {
		return
	}
	if !ly.Is4D() && ly.Params.Inhib.Pool.On.IsTrue() {
		ly.Params.Inhib.Pool.On.SetBool(false)
	}
	ly.Params.Update()
}

// UpdateParams updates all params given any changes that might
// have been made to individual values including those in the
// receiving pathways of this layer.
// This is not called Update because it is not just about the
// local values in the struct.
func (ly *Layer) UpdateParams() {
	ly.Update()
	for _, pt := range ly.RecvPaths {
		pt.UpdateParams()
	}
}

// PostBuild performs special post-Build() configuration steps for specific algorithms,
// using configuration data set in BuildConfig during the ConfigNet process.
func (ly *Layer) PostBuild() {
	ly.Params.LayInhib.Index1 = ly.BuildConfigFindLayer("LayInhib1Name", false) // optional
	ly.Params.LayInhib.Index2 = ly.BuildConfigFindLayer("LayInhib2Name", false) // optional
	ly.Params.LayInhib.Index3 = ly.BuildConfigFindLayer("LayInhib3Name", false) // optional
	ly.Params.LayInhib.Index4 = ly.BuildConfigFindLayer("LayInhib4Name", false) // optional

	switch ly.Type {
	case PulvinarLayer:
		ly.PulvPostBuild()

	case LDTLayer:
		ly.LDTPostBuild()
	case RWDaLayer:
		ly.RWDaPostBuild()
	case TDIntegLayer:
		ly.TDIntegPostBuild()
	case TDDaLayer:
		ly.TDDaPostBuild()

	case BLALayer, CeMLayer, USLayer, PVLayer, VSPatchLayer:
		ly.RubiconPostBuild()

	case MatrixLayer:
		ly.MatrixPostBuild()
	case GPLayer:
		ly.GPPostBuild()
	}
}

// HasPoolInhib returns true if the layer is using pool-level inhibition (implies 4D too).
// This is the proper check for using pool-level target average activations, for example.
func (ly *Layer) HasPoolInhib() bool {
	return ly.Params.Inhib.Pool.On.IsTrue()
}

// AsAxon returns this layer as a axon.Layer -- all derived layers must redefine
// this to return the base Layer type, so that the AxonLayer interface does not
// need to include accessors to all the basic stuff
func (ly *Layer) AsAxon() *Layer {
	return ly
}

// JsonToParams reformates json output to suitable params display output
func JsonToParams(b []byte) string {
	br := strings.Replace(string(b), `"`, ``, -1)
	br = strings.Replace(br, ",\n", "", -1)
	br = strings.Replace(br, "{\n", "{", -1)
	br = strings.Replace(br, "} ", "}\n  ", -1)
	br = strings.Replace(br, "\n }", " }", -1)
	br = strings.Replace(br, "\n  }\n", " }", -1)
	return br[1:] + "\n"
}

// note: all basic computation can be performed on layer-level and path level

//////////////////////////////////////////////////////////////////////////////////////
//  Init methods

// InitWeights initializes the weight values in the network, i.e., resetting learning
// Also calls InitActs
func (ly *Layer) InitWeights(ctx *Context, nt *Network) { //types:add
	ly.UpdateParams()
	ly.Params.Acts.Dend.HasMod.SetBool(false)
	li := ly.Index
	for di := uint32(0); di < ly.MaxData; di++ {
		LayerStates.Set(1, int(LayerAvgMaxGeM), int(li), int(di))
		LayerStates.Set(1, int(LayerAvgMaxGiM), int(li), int(di))
		LayerStates.Set(1, int(LayerGiMult), int(li), int(di))
		LayerStates.Set(ly.Params.Inhib.ActAvg.Nominal, int(LayerActMAvg), int(li), int(di))
		LayerStates.Set(ly.Params.Inhib.ActAvg.Nominal, int(LayerActPAvg), int(li), int(di))
	}
	ly.InitActAvg(ctx)
	ly.InitActs(ctx)
	ly.InitGScale(ctx)
	for _, pt := range ly.SendPaths {
		if pt.Off {
			continue
		}
		pt.InitWeights(ctx, nt)
	}
	for _, pt := range ly.RecvPaths {
		if pt.Off {
			continue
		}
		if pt.Params.Com.GType == ModulatoryG {
			ly.Params.Acts.Dend.HasMod.SetBool(true)
			break
		}
	}

}

// InitActAvg initializes the running-average activation values
// that drive learning and the longer time averaging values.
func (ly *Layer) InitActAvg(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.Learn.InitNeurCa(ctx, ni, di)
		}
	}
	if ly.HasPoolInhib() && ly.Params.Learn.TrgAvgAct.Pool.IsTrue() {
		ly.InitActAvgPools(ctx)
	} else {
		ly.InitActAvgLayer(ctx)
	}
}

// InitActAvgLayer initializes the running-average activation values
// that drive learning and the longer time averaging values.
// version with just overall layer-level inhibition.
func (ly *Layer) InitActAvgLayer(ctx *Context) {
	strg := ly.Params.Learn.TrgAvgAct.TrgRange.Min
	rng := ly.Params.Learn.TrgAvgAct.TrgRange.Range()
	tmax := ly.Params.Learn.TrgAvgAct.TrgRange.Max
	gibinit := ly.Params.Learn.TrgAvgAct.GiBaseInit
	inc := float32(0)
	nn := ly.NNeurons
	if nn > 1 {
		inc = rng / float32(nn-1)
	}
	porder := make([]int, nn)
	for i := range porder {
		porder[i] = i
	}
	if ly.Params.Learn.TrgAvgAct.Permute.IsTrue() {
		randx.PermuteInts(porder, &ly.Network.Rand)
	}
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		vi := porder[lni] // same for all datas
		trg := strg + inc*float32(vi)
		NeuronAvgs.Set(trg, int(TrgAvg), int(ni))
		NeuronAvgs.Set(trg, int(AvgPct), int(ni))
		NeuronAvgs.Set(ly.Params.Inhib.ActAvg.Nominal*trg, int(ActAvg), int(ni))
		NeuronAvgs.Set(0, int(AvgDif), int(ni))
		NeuronAvgs.Set(0, int(DTrgAvg), int(ni))
		NeuronAvgs.Set(ly.Params.Acts.Init.GetGeBase(&ly.Network.Rand), int(GeBase), int(ni))
		NeuronAvgs.Set(ly.Params.Acts.Init.GetGiBase(&ly.Network.Rand), int(GiBase), int(ni))
		if gibinit > 0 {
			gib := gibinit * (tmax - trg)
			NeuronAvgs.Set(gib, int(GiBase), int(ni))
		}
	}
}

// InitActAvgPools initializes the running-average activation values
// that drive learning and the longer time averaging values.
// version with pooled inhibition.
func (ly *Layer) InitActAvgPools(ctx *Context) {
	strg := ly.Params.Learn.TrgAvgAct.TrgRange.Min
	rng := ly.Params.Learn.TrgAvgAct.TrgRange.Range()
	tmax := ly.Params.Learn.TrgAvgAct.TrgRange.Max
	gibinit := ly.Params.Learn.TrgAvgAct.GiBaseInit
	inc := float32(0)
	nNy := ly.Shape.DimSize(2)
	nNx := ly.Shape.DimSize(3)
	nn := nNy * nNx
	if nn > 1 {
		inc = rng / float32(nn-1)
	}
	np := ly.NPools
	porder := make([]int, nn)
	for i := range porder {
		porder[i] = i
	}
	for spi := uint32(1); spi < np; spi++ {
		if ly.Params.Learn.TrgAvgAct.Permute.IsTrue() {
			randx.PermuteInts(porder, &ly.Network.Rand)
		}
		pi := ly.Params.PoolIndex(spi) // only using for idxs
		nsi := PoolsInt.Value(int(PoolNeurSt), int(pi), int(0))
		nei := PoolsInt.Value(int(PoolNeurEd), int(pi), int(0))
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NrnIsOff(ni) {
				continue
			}
			vi := porder[lni-nsi]
			trg := strg + inc*float32(vi)
			NeuronAvgs.Set(trg, int(TrgAvg), int(ni))
			NeuronAvgs.Set(trg, int(AvgPct), int(ni))
			NeuronAvgs.Set(ly.Params.Inhib.ActAvg.Nominal*trg, int(ActAvg), int(ni))
			NeuronAvgs.Set(0, int(AvgDif), int(ni))
			NeuronAvgs.Set(0, int(DTrgAvg), int(ni))
			NeuronAvgs.Set(ly.Params.Acts.Init.GetGeBase(&ly.Network.Rand), int(GeBase), int(ni))
			NeuronAvgs.Set(ly.Params.Acts.Init.GetGiBase(&ly.Network.Rand), int(GiBase), int(ni))
			if gibinit > 0 {
				gib := gibinit * (tmax - trg)
				NeuronAvgs.Set(gib, int(GiBase), int(ni))
			}
		}
	}
}

// InitActs fully initializes activation state -- only called automatically during InitWeights
func (ly *Layer) InitActs(ctx *Context) { //types:add
	ly.Params.Acts.Clamp.IsInput.SetBool(ly.Params.IsInput())
	ly.Params.Acts.Clamp.IsTarget.SetBool(ly.Params.IsTarget())
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.Acts.InitActs(ctx, ni, di)
		}
	}
	np := ly.NPools
	for spi := uint32(0); spi < np; spi++ {
		for di := uint32(0); di < ly.MaxData; di++ {
			pi := ly.Params.PoolIndex(spi)
			PoolInit(pi, di)
			if ly.Params.Acts.Clamp.Add.IsFalse() && ly.Params.Acts.Clamp.IsInput.IsTrue() {
				PoolsInt.Set(1, int(Clamped), int(pi), int(di))
			}
			// Target layers are dynamically updated
		}
	}
	ly.InitPathGBuffs(ctx)
}

// InitPathGBuffs initializes the pathway-level conductance buffers and
// conductance integration values for receiving pathways in this layer.
func (ly *Layer) InitPathGBuffs(ctx *Context) {
	for _, pt := range ly.RecvPaths {
		if pt.Off {
			continue
		}
		pt.Params.InitGBuffs()
	}
}

// InitWeightsSym initializes the weight symmetry -- higher layers copy weights from lower layers
func (ly *Layer) InitWtSym(ctx *Context) {
	for _, pt := range ly.SendPaths {
		if pt.Off {
			continue
		}
		if pt.Params.SWts.Init.Sym.IsFalse() {
			continue
		}
		// key ordering constraint on which way weights are copied
		if pt.Recv.Index < pt.Send.Index {
			continue
		}
		rpj, has := ly.RecipToSendPath(pt)
		if !has {
			continue
		}
		if rpj.Params.SWts.Init.Sym.IsFalse() {
			continue
		}
		pt.InitWtSym(ctx, rpj)
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  ApplyExt

// InitExt initializes external input state.
// Should be called prior to ApplyExt on all layers receiving Ext input.
func (ly *Layer) InitExt(ctx *Context) {
	if !ly.Type.IsExt() {
		return
	}
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.InitExt(ctx, ni, di)
			Exts.Set(-1, int(ly.Params.Indexes.ExtsSt+lni), int(di)) // missing by default
		}
	}
}

// ApplyExt applies external input in the form of an tensor.Float32 or 64.
// Negative values and NaNs are not valid, and will be interpreted as missing inputs.
// The given data index di is the data parallel index (0 < di < MaxData):
// must present inputs separately for each separate data parallel set.
// If dimensionality of tensor matches that of layer, and is 2D or 4D,
// then each dimension is iterated separately, so any mismatch preserves
// dimensional structure.
// Otherwise, the flat 1D view of the tensor is used.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext.
// Also sets the Exts values on layer, which are used for the GPU version,
// which requires calling the network ApplyExts() method -- is a no-op for CPU.
func (ly *Layer) ApplyExt(ctx *Context, di uint32, ext tensor.Tensor) {
	switch {
	case ext.NumDims() == 2 && ly.Shape.NumDims() == 4: // special case
		ly.ApplyExt2Dto4D(ctx, di, ext)
	case ext.NumDims() != ly.Shape.NumDims() || !(ext.NumDims() == 2 || ext.NumDims() == 4):
		ly.ApplyExt1DTsr(ctx, di, ext)
	case ext.NumDims() == 2:
		ly.ApplyExt2D(ctx, di, ext)
	case ext.NumDims() == 4:
		ly.ApplyExt4D(ctx, di, ext)
	}
}

// ApplyExtVal applies given external value to given neuron
// using clearMask, setMask, and toTarg from ApplyExtFlags.
// Also saves Val in Exts for potential use by GPU.
func (ly *Layer) ApplyExtValue(ctx *Context, lni, di uint32, val float32, clearMask, setMask NeuronFlags, toTarg bool) {
	ni := ly.NeurStIndex + lni
	if NrnIsOff(ni) {
		return
	}
	Exts.Set(val, int(ly.Params.Indexes.ExtsSt+lni), int(di))
	if val < 0 {
		return
	}
	if toTarg {
		Neurons.Set(val, int(Target), int(ni), int(di))
	} else {
		Neurons.Set(val, int(Ext), int(ni), int(di))
	}
	NrnClearFlag(ni, di, clearMask)
	NrnSetFlag(ni, di, setMask)
}

// ApplyExtFlags gets the clear mask and set mask for updating neuron flags
// based on layer type, and whether input should be applied to Target (else Ext)
func (ly *Layer) ApplyExtFlags() (clearMask, setMask NeuronFlags, toTarg bool) {
	ly.Params.ApplyExtFlags(&clearMask, &setMask, &toTarg)
	return
}

// ApplyExt2D applies 2D tensor external input
func (ly *Layer) ApplyExt2D(ctx *Context, di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	ymx := min(ext.DimSize(0), ly.Shape.DimSize(0))
	xmx := min(ext.DimSize(1), ly.Shape.DimSize(1))
	for y := 0; y < ymx; y++ {
		for x := 0; x < xmx; x++ {
			idx := []int{y, x}
			val := float32(ext.Float(idx...))
			lni := uint32(ly.Shape.IndexTo1D(idx...))
			ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
		}
	}
}

// ApplyExt2Dto4D applies 2D tensor external input to a 4D layer
func (ly *Layer) ApplyExt2Dto4D(ctx *Context, di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	lNy, lNx, _, _ := tensor.Projection2DShape(&ly.Shape, false)

	ymx := min(ext.DimSize(0), lNy)
	xmx := min(ext.DimSize(1), lNx)
	for y := 0; y < ymx; y++ {
		for x := 0; x < xmx; x++ {
			idx := []int{y, x}
			val := float32(ext.Float(idx...))
			lni := uint32(tensor.Projection2DIndex(&ly.Shape, false, y, x))
			ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
		}
	}
}

// ApplyExt4D applies 4D tensor external input
func (ly *Layer) ApplyExt4D(ctx *Context, di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	ypmx := min(ext.DimSize(0), ly.Shape.DimSize(0))
	xpmx := min(ext.DimSize(1), ly.Shape.DimSize(1))
	ynmx := min(ext.DimSize(2), ly.Shape.DimSize(2))
	xnmx := min(ext.DimSize(3), ly.Shape.DimSize(3))
	for yp := 0; yp < ypmx; yp++ {
		for xp := 0; xp < xpmx; xp++ {
			for yn := 0; yn < ynmx; yn++ {
				for xn := 0; xn < xnmx; xn++ {
					idx := []int{yp, xp, yn, xn}
					val := float32(ext.Float(idx...))
					lni := uint32(ly.Shape.IndexTo1D(idx...))
					ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
				}
			}
		}
	}
}

// ApplyExt1DTsr applies external input using 1D flat interface into tensor.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1DTsr(ctx *Context, di uint32, ext tensor.Tensor) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(ext.Len(), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := float32(ext.Float1D(int(lni)))
		ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
	}
}

// ApplyExt1D applies external input in the form of a flat 1-dimensional slice of floats
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1D(ctx *Context, di uint32, ext []float64) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(len(ext), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := float32(ext[lni])
		ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
	}
}

// ApplyExt1D32 applies external input in the form of a flat 1-dimensional slice of float32s.
// If the layer is a Target or Compare layer type, then it goes in Target
// otherwise it goes in Ext
func (ly *Layer) ApplyExt1D32(ctx *Context, di uint32, ext []float32) {
	clearMask, setMask, toTarg := ly.ApplyExtFlags()
	mx := uint32(min(len(ext), int(ly.NNeurons)))
	for lni := uint32(0); lni < mx; lni++ {
		val := ext[lni]
		ly.ApplyExtValue(ctx, lni, di, val, clearMask, setMask, toTarg)
	}
}

// UpdateExtFlags updates the neuron flags for external input based on current
// layer Type field -- call this if the Type has changed since the last
// ApplyExt* method call.
func (ly *Layer) UpdateExtFlags(ctx *Context) {
	clearMask, setMask, _ := ly.ApplyExtFlags()
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			NrnClearFlag(ni, di, clearMask)
			NrnSetFlag(ni, di, setMask)
		}
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  InitGScale

// InitGScale computes the initial scaling factor for synaptic input conductances G,
// stored in GScale.Scale, based on sending layer initial activation.
func (ly *Layer) InitGScale(ctx *Context) {
	totGeRel := float32(0)
	totGiRel := float32(0)
	totGmRel := float32(0)
	totGmnRel := float32(0)
	for _, pt := range ly.RecvPaths {
		if pt.Off {
			continue
		}
		slay := pt.Send
		savg := slay.Params.Inhib.ActAvg.Nominal
		snu := slay.NNeurons
		ncon := pt.RecvConNAvgMax.Avg
		pt.Params.GScale.Scale = pt.Params.PathScale.FullScale(savg, float32(snu), ncon)
		// reverting this change: if you want to eliminate a path, set the Off flag
		// if you want to negate it but keep the relative factor in the denominator
		// then set the scale to 0.
		//
		//	if pj.Params.GScale == 0 {
		//		continue
		//	}
		switch pt.Params.Com.GType {
		case InhibitoryG:
			totGiRel += pt.Params.PathScale.Rel
		case ModulatoryG:
			totGmRel += pt.Params.PathScale.Rel
		case MaintG:
			totGmnRel += pt.Params.PathScale.Rel
		default:
			totGeRel += pt.Params.PathScale.Rel
		}
	}

	for _, pt := range ly.RecvPaths {
		switch pt.Params.Com.GType {
		case InhibitoryG:
			if totGiRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGiRel
				pt.Params.GScale.Scale /= totGiRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0
			}
		case ModulatoryG:
			if totGmRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGmRel
				pt.Params.GScale.Scale /= totGmRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0

			}
		case MaintG:
			if totGmnRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGmnRel
				pt.Params.GScale.Scale /= totGmnRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0

			}
		default:
			if totGeRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGeRel
				pt.Params.GScale.Scale /= totGeRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0
			}
		}
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  Threading / Reports

// CostEst returns the estimated computational cost associated with this layer,
// separated by neuron-level and synapse-level, in arbitrary units where
// cost per synapse is 1.  Neuron-level computation is more expensive but
// there are typically many fewer neurons, so in larger networks, synaptic
// costs tend to dominate.  Neuron cost is estimated from TimerReport output
// for large networks.
func (ly *Layer) CostEst() (neur, syn, tot int) {
	perNeur := 300 // cost per neuron, relative to synapse which is 1
	neur = int(ly.NNeurons) * perNeur
	syn = 0
	for _, pt := range ly.SendPaths {
		syn += int(pt.NSyns)
	}
	tot = neur + syn
	return
}

//////////////////////////////////////////////////////////////////////////////////////
//  Stats

// note: use float64 for stats as that is best for logging

// PctUnitErr returns the proportion of units where the thresholded value of
// Target (Target or Compare types) or ActP does not match that of ActM.
// If Act > ly.Params.Acts.Clamp.ErrThr, effective activity = 1 else 0
// robust to noisy activations.
// returns one result per data parallel index ([ctx.NData])
func (ly *Layer) PctUnitErr(ctx *Context) []float64 {
	nn := ly.NNeurons
	if nn == 0 {
		return nil
	}
	errs := make([]float64, ctx.NData)
	thr := ly.Params.Acts.Clamp.ErrThr
	for di := uint32(0); di < ctx.NData; di++ {
		wrong := 0
		n := 0
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			if NrnIsOff(ni) {
				continue
			}
			trg := false
			if ly.Type == CompareLayer || ly.Type == TargetLayer {
				if Neurons.Value(int(Target), int(ni), int(di)) > thr {
					trg = true
				}
			} else {
				if Neurons.Value(int(ActP), int(ni), int(di)) > thr {
					trg = true
				}
			}
			if Neurons.Value(int(ActM), int(ni), int(di)) > thr {
				if !trg {
					wrong++
				}
			} else {
				if trg {
					wrong++
				}
			}
			n++
		}
		if n > 0 {
			errs[di] = float64(wrong) / float64(n)
		}
	}
	return errs
}

// LocalistErr2D decodes a 2D layer with Y axis = redundant units, X = localist units
// returning the indexes of the max activated localist value in the minus and plus phase
// activities, and whether these are the same or different (err = different)
// returns one result per data parallel index ([ctx.NData])
func (ly *Layer) LocalistErr2D(ctx *Context) (err []bool, minusIndex, plusIndex []int) {
	err = make([]bool, ctx.NData)
	minusIndex = make([]int, ctx.NData)
	plusIndex = make([]int, ctx.NData)
	ydim := ly.Shape.DimSize(0)
	xdim := ly.Shape.DimSize(1)
	for di := uint32(0); di < ctx.NData; di++ {
		var maxM, maxP float32
		var mIndex, pIndex int
		for xi := 0; xi < xdim; xi++ {
			var sumP, sumM float32
			for yi := 0; yi < ydim; yi++ {
				lni := uint32(yi*xdim + xi)
				ni := ly.NeurStIndex + lni
				sumM += Neurons.Value(int(ActM), int(ni), int(di))
				sumP += Neurons.Value(int(ActP), int(ni), int(di))
			}
			if sumM > maxM {
				mIndex = xi
				maxM = sumM
			}
			if sumP > maxP {
				pIndex = xi
				maxP = sumP
			}
		}
		er := mIndex != pIndex
		err[di] = er
		minusIndex[di] = mIndex
		plusIndex[di] = pIndex
	}
	return
}

// LocalistErr4D decodes a 4D layer with each pool representing a localist value.
// Returns the flat 1D indexes of the max activated localist value in the minus and plus phase
// activities, and whether these are the same or different (err = different)
func (ly *Layer) LocalistErr4D(ctx *Context) (err []bool, minusIndex, plusIndex []int) {
	err = make([]bool, ctx.NData)
	minusIndex = make([]int, ctx.NData)
	plusIndex = make([]int, ctx.NData)
	npool := ly.Shape.DimSize(0) * ly.Shape.DimSize(1)
	nun := ly.Shape.DimSize(2) * ly.Shape.DimSize(3)
	for di := uint32(0); di < ctx.NData; di++ {
		var maxM, maxP float32
		var mIndex, pIndex int
		for xi := 0; xi < npool; xi++ {
			var sumP, sumM float32
			for yi := 0; yi < nun; yi++ {
				lni := uint32(xi*nun + yi)
				ni := ly.NeurStIndex + lni
				sumM += Neurons.Value(int(ActM), int(ni), int(di))
				sumP += Neurons.Value(int(ActP), int(ni), int(di))
			}
			if sumM > maxM {
				mIndex = xi
				maxM = sumM
			}
			if sumP > maxP {
				pIndex = xi
				maxP = sumP
			}
		}
		er := mIndex != pIndex
		err[di] = er
		minusIndex[di] = mIndex
		plusIndex[di] = pIndex
	}
	return
}

// TestValues returns a map of key vals for testing
// ctrKey is a key of counters to contextualize values.
func (ly *Layer) TestValues(ctrKey string, vals map[string]float32) {
	for spi := uint32(0); spi < ly.NPools; spi++ {
		for di := uint32(0); di < ly.MaxData; di++ {
			pi := ly.Params.PoolIndex(spi)
			key := fmt.Sprintf("%s  Lay: %s\tPool: %d\tDi: %d", ctrKey, ly.Name, pi, di)
			PoolTestValues(pi, di, key, vals)
		}
	}
}

//////////////////////////////////////////////////////////////////////////////////////
//  Lesion

// UnLesionNeurons unlesions (clears the Off flag) for all neurons in the layer
func (ly *Layer) UnLesionNeurons() { //types:add
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		for di := uint32(0); di < ly.MaxData; di++ {
			NrnClearFlag(ni, di, NeuronOff)
		}
	}
}

// LesionNeurons lesions (sets the Off flag) for given proportion (0-1) of neurons in layer
// returns number of neurons lesioned.  Emits error if prop > 1 as indication that percent
// might have been passed
func (ly *Layer) LesionNeurons(prop float32) int { //types:add
	ly.UnLesionNeurons()
	if prop > 1 {
		log.Printf("LesionNeurons got a proportion > 1 -- must be 0-1 as *proportion* (not percent) of neurons to lesion: %v\n", prop)
		return 0
	}
	nn := ly.NNeurons
	if nn == 0 {
		return 0
	}
	p := rand.Perm(int(nn))
	nl := int(prop * float32(nn))
	for lni := uint32(0); lni < nn; lni++ {
		nip := uint32(p[lni])
		ni := ly.NeurStIndex + nip
		if NrnIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			NrnSetFlag(ni, di, NeuronOff)
		}
	}
	return nl
}

func (ly *Layer) MakeToolbar(p *tree.Plan) {
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.Defaults).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.InitWeights).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.InitActs).SetIcon(icons.Reset)
	})
	tree.Add(p, func(w *core.Separator) {})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.LesionNeurons).SetIcon(icons.Cut)
	})
	tree.Add(p, func(w *core.FuncButton) {
		w.SetFunc(ly.UnLesionNeurons).SetIcon(icons.Cut)
	})
}
