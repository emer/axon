// Code generated by "goal build"; DO NOT EDIT.
//line init-layer.goal:1
// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package axon

import (
	"cogentcore.org/core/base/randx"
)

// InitWeights initializes the weight values in the network, i.e., resetting learning
// Also calls InitActs
func (ly *Layer) InitWeights(ctx *Context, nt *Network) { //types:add
	ly.UpdateParams()
	ly.Params.Acts.Dend.HasMod.SetBool(false)
	li := ly.Index
	for di := uint32(0); di < ly.MaxData; di++ {
		LayerStates.Set(ly.Params.Inhib.ActAvg.Nominal, int(li), int(di), int(LayerActMAvg))
		LayerStates.Set(ly.Params.Inhib.ActAvg.Nominal, int(li), int(di), int(LayerActPAvg))
		LayerStates.Set(1, int(li), int(di), int(LayerAvgMaxGeM))
		LayerStates.Set(1, int(li), int(di), int(LayerAvgMaxGiM))
		LayerStates.Set(1, int(li), int(di), int(LayerGiMult))
		LayerStates.Set(0, int(li), int(di), int(LayerPhaseDiff))
		LayerStates.Set(0, int(li), int(di), int(LayerPhaseDiffAvg))
		LayerStates.Set(0, int(li), int(di), int(LayerPhaseDiffVar))
		LayerStates.Set(-1, int(li), int(di), int(LayerRT))
		LayerStates.Set(0, int(li), int(di), int(LayerRewPredPos))
		LayerStates.Set(0, int(li), int(di), int(LayerRewPredNeg))
	}
	ly.InitActAvg(ctx)
	ly.InitActs(ctx)
	ly.InitGScale(ctx)
	for _, pt := range ly.SendPaths {
		if pt.Off {
			continue
		}
		pt.InitWeights(ctx, nt)
	}
	for _, pt := range ly.RecvPaths {
		if pt.Off {
			continue
		}
		if pt.Params.Com.GType == ModulatoryG {
			ly.Params.Acts.Dend.HasMod.SetBool(true)
			break
		}
	}

}

// InitActAvg initializes the running-average activation values
// that drive learning and the longer time averaging values.
func (ly *Layer) InitActAvg(ctx *Context) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.Learn.InitNeurCa(ctx, ni, di)
		}
	}
	if ly.Params.HasPoolInhib() && ly.Params.Learn.TrgAvgAct.Pool.IsTrue() {
		ly.InitActAvgPools(ctx)
	} else {
		ly.InitActAvgLayer(ctx)
	}
}

// InitActAvgLayer initializes the running-average activation values
// that drive learning and the longer time averaging values.
// version with just overall layer-level inhibition.
func (ly *Layer) InitActAvgLayer(ctx *Context) {
	strg := ly.Params.Learn.TrgAvgAct.TrgRange.Min
	rng := ly.Params.Learn.TrgAvgAct.TrgRange.Range()
	tmax := ly.Params.Learn.TrgAvgAct.TrgRange.Max
	gibinit := ly.Params.Learn.TrgAvgAct.GiBaseInit
	inc := float32(0)
	nn := ly.NNeurons
	if nn > 1 {
		inc = rng / float32(nn-1)
	}
	porder := make([]int, nn)
	for i := range porder {
		porder[i] = i
	}
	if ly.Params.Learn.TrgAvgAct.Permute.IsTrue() {
		randx.PermuteInts(porder, &ly.Network.Rand)
	}
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		vi := porder[lni] // same for all datas
		trg := strg + inc*float32(vi)
		NeuronAvgs.Set(trg, int(ni), int(TrgAvg))
		NeuronAvgs.Set(trg, int(ni), int(AvgPct))
		NeuronAvgs.Set(ly.Params.Inhib.ActAvg.Nominal*trg, int(ni), int(ActAvg))
		NeuronAvgs.Set(0, int(ni), int(AvgDif))
		NeuronAvgs.Set(0, int(ni), int(DTrgAvg))
		NeuronAvgs.Set(ly.Params.Acts.Init.GetGeBase(&ly.Network.Rand), int(ni), int(GeBase))
		NeuronAvgs.Set(ly.Params.Acts.Init.GetGiBase(&ly.Network.Rand), int(ni), int(GiBase))
		if gibinit > 0 {
			gib := gibinit * (tmax - trg)
			NeuronAvgs.Set(gib, int(ni), int(GiBase))
		}
	}
}

// InitActAvgPools initializes the running-average activation values
// that drive learning and the longer time averaging values.
// version with pooled inhibition.
func (ly *Layer) InitActAvgPools(ctx *Context) {
	strg := ly.Params.Learn.TrgAvgAct.TrgRange.Min
	rng := ly.Params.Learn.TrgAvgAct.TrgRange.Range()
	tmax := ly.Params.Learn.TrgAvgAct.TrgRange.Max
	gibinit := ly.Params.Learn.TrgAvgAct.GiBaseInit
	inc := float32(0)
	nNy := ly.Shape.DimSize(2)
	nNx := ly.Shape.DimSize(3)
	nn := nNy * nNx
	if nn > 1 {
		inc = rng / float32(nn-1)
	}
	np := ly.NPools
	porder := make([]int, nn)
	for i := range porder {
		porder[i] = i
	}
	for spi := uint32(1); spi < np; spi++ {
		if ly.Params.Learn.TrgAvgAct.Permute.IsTrue() {
			randx.PermuteInts(porder, &ly.Network.Rand)
		}
		pi := ly.Params.PoolIndex(spi) // only using for idxs
		nsi := PoolsInt.Value(int(pi), int(0), int(PoolNeurSt))
		nei := PoolsInt.Value(int(pi), int(0), int(PoolNeurEd))
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			vi := porder[lni-nsi]
			trg := strg + inc*float32(vi)
			NeuronAvgs.Set(trg, int(ni), int(TrgAvg))
			NeuronAvgs.Set(trg, int(ni), int(AvgPct))
			NeuronAvgs.Set(ly.Params.Inhib.ActAvg.Nominal*trg, int(ni), int(ActAvg))
			NeuronAvgs.Set(0, int(ni), int(AvgDif))
			NeuronAvgs.Set(0, int(ni), int(DTrgAvg))
			NeuronAvgs.Set(ly.Params.Acts.Init.GetGeBase(&ly.Network.Rand), int(ni), int(GeBase))
			NeuronAvgs.Set(ly.Params.Acts.Init.GetGiBase(&ly.Network.Rand), int(ni), int(GiBase))
			if gibinit > 0 {
				gib := gibinit * (tmax - trg)
				NeuronAvgs.Set(gib, int(ni), int(GiBase))
			}
		}
	}
}

// InitActs fully initializes activation state -- only called automatically during InitWeights
func (ly *Layer) InitActs(ctx *Context) { //types:add
	ly.Params.Acts.Clamp.IsInput.SetBool(ly.Params.IsInput())
	ly.Params.Acts.Clamp.IsTarget.SetBool(ly.Params.IsTarget())
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ly.MaxData; di++ {
			ly.Params.Acts.InitActs(ctx, ni, di)
		}
	}
	np := ly.NPools
	for spi := uint32(0); spi < np; spi++ {
		for di := uint32(0); di < ly.MaxData; di++ {
			pi := ly.Params.PoolIndex(spi)
			PoolInit(pi, di)
			if ly.Params.Acts.Clamp.Add.IsFalse() && ly.Params.Acts.Clamp.IsInput.IsTrue() {
				PoolsInt.Set(1, int(pi), int(di), int(Clamped))
			}
			// Target layers are dynamically updated
		}
	}
	// ly.InitPathGBuffs(ctx)
}

// InitWeightsSym initializes the weight symmetry -- higher layers copy weights from lower layers
func (ly *Layer) InitWtSym(ctx *Context) {
	for _, pt := range ly.SendPaths {
		if pt.Off {
			continue
		}
		if pt.Params.SWts.Init.Sym.IsFalse() {
			continue
		}
		// key ordering constraint on which way weights are copied
		if pt.Recv.Index < pt.Send.Index {
			continue
		}
		rpj, has := ly.RecipToSendPath(pt)
		if !has {
			continue
		}
		if rpj.Params.SWts.Init.Sym.IsFalse() {
			continue
		}
		pt.InitWtSym(ctx, rpj)
	}
}

// InitGScale computes the initial scaling factor for synaptic input conductances G,
// stored in GScale.Scale, based on sending layer initial activation.
func (ly *Layer) InitGScale(ctx *Context) {
	totGeRel := float32(0)
	totGiRel := float32(0)
	totGmRel := float32(0)
	totGmnRel := float32(0)
	for _, pt := range ly.RecvPaths {
		if pt.Off {
			continue
		}
		slay := pt.Send
		savg := slay.Params.Inhib.ActAvg.Nominal
		snu := slay.NNeurons
		ncon := pt.RecvConNAvgMax.Avg
		pt.Params.GScale.Scale = pt.Params.PathScale.FullScale(savg, float32(snu), ncon)
		// reverting this change: if you want to eliminate a path, set the Off flag
		// if you want to negate it but keep the relative factor in the denominator
		// then set the scale to 0.
		//
		//	if pj.Params.GScale == 0 {
		//		continue
		//	}
		switch pt.Params.Com.GType {
		case InhibitoryG:
			totGiRel += pt.Params.PathScale.Rel
		case ModulatoryG:
			totGmRel += pt.Params.PathScale.Rel
		case MaintG:
			totGmnRel += pt.Params.PathScale.Rel
		default:
			totGeRel += pt.Params.PathScale.Rel
		}
	}

	for _, pt := range ly.RecvPaths {
		switch pt.Params.Com.GType {
		case InhibitoryG:
			if totGiRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGiRel
				pt.Params.GScale.Scale /= totGiRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0
			}
		case ModulatoryG:
			if totGmRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGmRel
				pt.Params.GScale.Scale /= totGmRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0

			}
		case MaintG:
			if totGmnRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGmnRel
				pt.Params.GScale.Scale /= totGmnRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0

			}
		default:
			if totGeRel > 0 {
				pt.Params.GScale.Rel = pt.Params.PathScale.Rel / totGeRel
				pt.Params.GScale.Scale /= totGeRel
			} else {
				pt.Params.GScale.Rel = 0
				pt.Params.GScale.Scale = 0
			}
		}
	}
}

// NewStateNeurons only calls the neurons part of new state -- for misbehaving GPU
func (ly *Layer) NewStateNeurons(ctx *Context) {
	nn := ly.NNeurons
	for di := uint32(0); di < ctx.NData; di++ {
		for lni := uint32(0); lni < nn; lni++ {
			ni := ly.NeurStIndex + lni
			// note: this calls the basic neuron-level DecayState
			ly.Params.NewStateNeuron(ctx, ni, di)
		}
	}
}

// DecayState decays activation state by given proportion
// (default decay values are ly.Params.Acts.Decay.Act, Glong)
func (ly *Layer) DecayState(ctx *Context, di uint32, decay, glong, ahp float32) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		// Note: synapse-level Ca decay happens in DWt
		if ahp == 1 {
			lt := ly.Type
			if lt == PTMaintLayer {
				Neurons.Set(0, int(ni), int(di), int(CtxtGe))
				Neurons.Set(0, int(ni), int(di), int(CtxtGeRaw))
				Neurons.Set(0, int(ni), int(di), int(CtxtGeOrig))
			}
		}
	}
	ly.DecayStateLayer(ctx, di, decay, glong, ahp)
}

// DecayStateLayer does layer-level decay, but not neuron level
func (ly *Layer) DecayStateLayer(ctx *Context, di uint32, decay, glong, ahp float32) {
	np := ly.NPools
	for spi := uint32(0); spi < np; spi++ {
		pi := ly.Params.PoolIndex(spi)
		PoolInhibDecay(pi, di, decay)
	}
}

// DecayStatePool decays activation state by given proportion in given sub-pool index (0 based)
func (ly *Layer) DecayStatePool(ctx *Context, pool int, decay, glong, ahp float32) {
	spi := uint32(pool + 1) // 1 based
	for di := uint32(0); di < ctx.NData; di++ {
		pi := ly.Params.PoolIndex(spi)
		nsi := PoolsInt.Value(int(pi), int(di), int(PoolNeurSt))
		nei := PoolsInt.Value(int(pi), int(di), int(PoolNeurEd))
		for lni := nsi; lni < nei; lni++ {
			ni := ly.NeurStIndex + uint32(lni)
			if NeuronIsOff(ni) {
				continue
			}
			ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		}
		PoolInhibDecay(pi, di, decay)
	}
}

// DecayStateNeuronsAll decays neural activation state by given proportion
// (default decay values are ly.Params.Acts.Decay.Act, Glong, AHP)
// for all data parallel indexes. Does not decay pool or layer state.
// This is used for minus phase of Pulvinar layers to clear state in prep
// for driver plus phase.
func (ly *Layer) DecayStateNeuronsAll(ctx *Context, decay, glong, ahp float32) {
	nn := ly.NNeurons
	for lni := uint32(0); lni < nn; lni++ {
		ni := ly.NeurStIndex + lni
		if NeuronIsOff(ni) {
			continue
		}
		for di := uint32(0); di < ctx.NData; di++ {
			ly.Params.Acts.DecayState(ctx, ni, di, decay, glong, ahp)
		}
	}
}
