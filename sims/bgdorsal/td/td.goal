// Copyright (c) 2025, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// td simulates a simple td agent
package main

import (
	"math/rand"

	"cogentcore.org/core/math32"
	"cogentcore.org/lab/base/randx"
	"cogentcore.org/lab/tensor"
)

// Epsilon has parameters for epsilon greedy policy,
// where random exploration happens with probability epsilon.
type Epsilon struct {
	// Init is the initial value: pure exploration
	Init float32 `default:"1"`

	// Min is the minimum value: always some exploration.
	Min float32 `default:"0.01"`

	// Decay per epoch.
	Decay float32 `default:"0.0002"`

	// Current epsilon value, updated every epoch.
	Current float32
}

func (eg *Epsilon) Defaults() {
	eg.Init = 1
	eg.Min = 0.01
	eg.Decay = 0.0002
	eg.Current = eg.Init
}

// Update updates the Current epsilon value for given epoch.
func (eg *Epsilon) Update(epoch int) float32 {
	eg.Current = eg.Min + (eg.Init-eg.Min)*math32.Exp(-eg.Decay*float32(epoch))
	return eg.Current
}

// LRate has parameters for an annealing learning rate schedule.
type LRate struct {
	// Init is the initial lrate value.
	Init float32 `default:"0.7"`

	// Min is the minimum lrate.
	Min float32 `default:"0.01"`

	// Decay per epoch.
	Decay float32 `default:"0.001"`

	// Current lrate value, updated every epoch.
	Current float32
}

func (eg *LRate) Defaults() {
	eg.Init = 0.7
	eg.Min = 0.01
	eg.Decay = 0.001
	eg.Current = eg.Init
}

// Update updates the lrate value for given epoch.
func (eg *LRate) Update(epoch int) float32 {
	eg.Current = eg.Min + (eg.Init-eg.Min)*math32.Exp(-eg.Decay*float32(epoch))
	return eg.Current
}

// TD implements a simple TD Q-learning simulation.
type TD struct {
	// LRate is the learning rate per step for updating Q.
	// It can decay to anneal the rate of change over time.
	LRate LRate `display:"inline"`

	// Epsilon computes the epsilon-greedy exploration value,
	// allowing exploration to decrease over time.
	Epsilon Epsilon `display:"inline"`

	// Gamma is the discount factor.
	Gamma float32 `default:"0.95"`

	// NStates is the number of states.
	NStates int

	// NActions is the number of actions per state.
	NActions int

	// Epoch is the current epoch counter, for driving parameter updates.
	Epoch int `edit:"-"`

	// Q are the state-action values: [States][Actions]
	Q tensor.Float32
}

func (td *TD) Defaults() {
	td.LRate.Defaults()
	td.Epsilon.Defaults()
	td.Gamma = 0.95
}

func NewTD(states, actions int) *TD {
	td := &TD{NStates: states, NActions: actions}
	td.Defaults()
	return td
}

func (td *TD) Config(states, actions int) {
	td.NStates = states
	td.NActions = actions
}

func (td *TD) Init() {
	td.Q.SetShapeSizes(td.NStates, td.NActions)
	tensor.SetAllFloat64(&td.Q, 0)
	td.EpochUpdate(0)
}

// EpochUpdate updates parameters for a new epoch.
func (td *TD) EpochUpdate(epoch int) {
	td.Epoch = epoch
	td.Epsilon.Update(epoch)
	td.LRate.Update(epoch)
}

// MaxQ returns the max Q value for given state.
func (td *TD) MaxQ(state int) (float32, int) {
	mx := float32(0)
	mi := 0
	for i := range td.NActions {
		q := td.Q[state, i]
		if i == 0 || q > mx {
			mx = q
			mi = i
		}
	}
	return mx, mi
}

func (td *TD) EpsilonGreedyAction(state int) int {
	if randx.BoolP(float64(td.Epsilon.Current)) {
		return rand.Intn(td.NActions)
	}
	_, mi := td.MaxQ(state)
	return mi
}

// Action computes the action to take for given state.
func (td *TD) Action(state int) int {
	return td.EpsilonGreedyAction(state)
}

// UpdateQ updates the Q value for state, action taken
// based on next state and reward value for current action.
func (td *TD) UpdateQ(state, action, next int, rew float32) {
	lrate := td.LRate.Current
	nq, _ := td.MaxQ(next)
	nextVal := rew + td.Gamma*nq
	td.Q[state, action] += lrate * (nextVal - td.Q[state, action])
}

// UpdateFinal updates the Q value for state, action taken
// for a final trial in a sequence, with a final reward estimate value.
func (td *TD) UpdateFinal(state, action int, rew float32) {
	lrate := td.LRate.Current
	td.Q[state, action] += lrate * (rew - td.Q[state, action])
}
